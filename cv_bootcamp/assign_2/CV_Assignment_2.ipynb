{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXnzcKFFKs5S"
      },
      "source": [
        "# **Assignment 2**\n",
        "## **Convolutional Neural Networks (CNNS) for Image Classification**\n",
        "\n",
        "In this assignment, we explore the capabilities of a popular deep learning architectures — Convolutional Neural Networks (CNNs) — by training them on various datasets and comparing their performance. While ANNs serve as a versatile model for a range of tasks, CNNs are specifically designed for handling spatial data, making them particularly effective for image classification problems. By evaluating these models, we aim to highlight their respective strengths, limitations, and suitability for different types of data, providing insights into their real-world applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdk-N_2hKs5i"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "### **Step - 1**\n",
        "This block imports essential libraries needed for building and training a Convolutional Neural Network (CNN) with PyTorch, including data loading, transformations, and metrics for evaluation.\n",
        "1) <blue>**torch**</blue>: The core PyTorch library, essential for all operations involving tensors, model creation, and training.\n",
        "2) <blue>**torch.nn**</blue>: Contains modules to define neural network architectures.\n",
        "3) <blue>**torch.optim**</blue>: Provides optimization algorithms like <green>**SGD (Stochastic Gradient Descent)**</green>, used during model training.\n",
        "4) <blue>**torchvision.transforms**</blue>: A module for applying various image transformations, such as <green>**normalization**</green> or <green>**random cropping**</green>.\n",
        "5) <blue>**DataLoader**</blue>: Used to efficiently load data in batches, critical for training deep learning models.\n",
        "6) <blue>**sklearn.metrics**</blue>: Includes metrics to evaluate model performance, such as <green>**accuracy**</green> and <green>**precision**</green>.\n",
        "7) <blue>**seaborn/matplotlib**</blue>: Libraries for <green>**visualization**</green>, typically used to visualize model performance metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x2iuKTjqKs5l"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fA3mUffKs5p"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "### **Step - 2**\n",
        "This block configures the device (CPU or GPU) and applies data transformations to the images (conversion to tensors and normalization).\n",
        "1) <blue>**torch.device**</blue>: Chooses the <green>**device**</green> to run computations on. If a <green>**GPU**</green> is available, the model will run on it; otherwise, it defaults to the CPU.\n",
        "2) <blue>**transforms.Compose**</blue>: Combines multiple transformations to apply <green>**sequentially**</green> to the data.\n",
        "3) <blue>**transforms.ToTensor()**</blue>: Converts the image from a <green>**PIL Image**</green> (or numpy array) to a PyTorch tensor.\n",
        "4) <blue>**transforms.Normalize()**</blue>: Normalizes the pixel values of the image. Each <green>**channel (R, G, B)**</green> is normalized with a <blue>**mean**</blue> of <green>**0.5**</green> and a <blue>**standard deviation**</blue> of <green>**0.5**</green>, scaling the values to the range [-1, 1]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZT-d5ht_Ks5q"
      },
      "outputs": [],
      "source": [
        "# Set device (GPU if available, else CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the dataset\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPu9JOXOKs5t"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "### **Step - 3**\n",
        "This block is meant to load the CIFAR-10 dataset, though the actual dataset loading is marked as a TODO. It sets up data loaders for both the training and testing datasets.\n",
        "1) <blue>**train_dataset/test_dataset**</blue>: These will eventually contain the training and testing datasets. The <blue>**torchvision.datasets.CIFAR10**</blue> class will likely be used here.\n",
        "2) <blue>**train_loader**</blue>: Loads the training dataset in <blue>**batches**</blue> of <green>**64**</green> images and <green>**shuffles**</green> them to ensure randomness during training.\n",
        "3) <blue>**test_loader**</blue>: Loads the test dataset in <blue>**batches**</blue> of <green>**64**</green> images but <green>**does not shuffle**</green> them since testing doesn't require randomization.\n",
        "\n",
        "Note: Replace the placeholders **\"None\"** with the appropriate code to download the training and testing dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VVbR6hIyKs5w"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 4227072/170498071 [00:11<07:34, 365547.20it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load CIFAR-10 dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# TODO: Use torchvision.datasets.CIFAR10 to load the training and test sets\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCIFAR10\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mCIFAR10(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m      6\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset\u001b[38;5;241m=\u001b[39mtrain_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/torchvision/datasets/cifar.py:66\u001b[0m, in \u001b[0;36mCIFAR10.__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;241m=\u001b[39m train  \u001b[38;5;66;03m# training set or test set\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/torchvision/datasets/cifar.py:140\u001b[0m, in \u001b[0;36mCIFAR10.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiles already downloaded and verified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m \u001b[43mdownload_and_extract_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtgz_md5\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/torchvision/datasets/utils.py:395\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[1;32m    393\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[0;32m--> 395\u001b[0m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m archive \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_root, filename)\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchive\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/torchvision/datasets/utils.py:132\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fpath)\n\u001b[0;32m--> 132\u001b[0m     \u001b[43m_urlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m url[:\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/torchvision/datasets/utils.py:30\u001b[0m, in \u001b[0;36m_urlretrieve\u001b[0;34m(url, filename, chunk_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: USER_AGENT})) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh, tqdm(total\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mlength) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m chunk \u001b[38;5;241m:=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     31\u001b[0m             fh\u001b[38;5;241m.\u001b[39mwrite(chunk)\n\u001b[1;32m     32\u001b[0m             pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Load CIFAR-10 dataset\n",
        "# TODO: Use torchvision.datasets.CIFAR10 to load the training and test sets\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD8QOcmOKs5y"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 4**\n",
        "1) This code defines a simple <green>**feedforward artificial neural network (ANN)**</green> for classification.\n",
        "2) The model has three fully connected layers <blue>**(fc1, fc2, fc3)**</blue>, and the final layer outputs predictions for <green>**10 classes** </green>.\n",
        "3) The <green>**forward pass**</green> describes how the input data flows through the network\n",
        "4) The input image is first <green>**flattened**</green>.\n",
        "5) It passes through fully connected layers with <blue>**ReLU activation function**</blue>.\n",
        "\n",
        "Note: Replace the placeholders **\"None\"** with the appropriate numbers of units and activation functions in each layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E54RPwV8Ks50"
      },
      "outputs": [],
      "source": [
        "class ANN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ANN, self).__init__()\n",
        "        # TODO: Define the layers for the ANN (Flatten, Fully Connected, Activation functions)\n",
        "        self.fc1 = nn.Linear(32*32*3, 512)  # Input layer (replace None with the number of units)\n",
        "        self.fc2 = nn.Linear(512, 256)  # Hidden layer (replace None with the number of units)\n",
        "        self.fc3 = nn.Linear(256, 10)  # Output layer for 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Define forward pass\n",
        "        x = x.view(-1, 32*32*3)  # Flatten the input image\n",
        "        x = torch.relu(self.fc1(x))  # First fully connected layer + relu activation\n",
        "        x = torch.relu(self.fc2(x))  # Second fully connected layer + relu activation\n",
        "        x = self.fc3(x)  # Output layer\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR6cFEx8Ks52"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 5**\n",
        "1) The <blue>**ANN model**</blue> is instantiated and moved to the <green>**selected device (CPU or GPU)**</green>.\n",
        "2) The loss function is set to <blue>**CrossEntropyLoss**</blue>, which is suitable for <green>**multi-class classification**</green> problems like in our case of CIFAR-10.\n",
        "3) The optimizer is <blue>**Adam**</blue>, with a <blue>**learning rate**</blue> of <green>**0.001**</green>, used to adjust the model parameters during training based on gradients computed from the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-g1UHuSYKs53"
      },
      "outputs": [],
      "source": [
        "# Initialize the ANN model, loss function, and optimizer\n",
        "model_ann = ANN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_ann.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdHUohymKs55"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 6**\n",
        "This code trains the neural network over a specified number of epochs (num_epochs).\n",
        "For each batch of images and labels, the following steps are performed:\n",
        "1) <blue>**Data Movement**</blue>: Images and labels are moved to the <green>**device (CPU or GPU)**</green>.\n",
        "2) <blue>**Forward Pass**</blue>: Images pass through the network to compute the <green>**output predictions**</green>.\n",
        "3) <blue>**Loss Calculation**</blue>: The loss between the predictions and true labels is computed and added to <blue>**ls_losses**</blue> for tracking.\n",
        "4) <blue>**Backpropagation and Optimization**</blue>: Gradients are calculated using backpropagation, and the optimizer <green>**updates the model parameters**</green> based on these gradients.\n",
        "\n",
        "Every 100 batches, the loss is printed to monitor training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOvLNvZBKs56"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "num_epochs = 5\n",
        "ls_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # TODO: Move images and labels to the device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # TODO: Forward pass\n",
        "        outputs = model_ann(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        ls_losses.append(loss.detach().item())\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oGJSpn5Ks57"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 7**\n",
        "1) This code plots the <blue>**training losses**</blue> that were recorded in <blue>**ls_losses**</blue> during training.\n",
        "2) The <blue>**x-axis**</blue> represents the <green>**number of samples (batches)**</green> seen during training, and the <blue>**y-axis**</blue> shows the <green>**corresponding loss**</green>.\n",
        "3) It provides a visual representation of how the <green>**model's loss decreases over time**</green>, indicating whether the training is progressing well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsCwnhvtKs58"
      },
      "outputs": [],
      "source": [
        "# Plot Losses\n",
        "x_axis = np.arange(0, len(ls_losses), 1)\n",
        "plt.plot(x_axis, ls_losses)\n",
        "plt.xlabel = \"Sample\"\n",
        "plt.ylabel = \"Loss\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A20nOS-6Ks58"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 8**\n",
        "This block evaluates the model on the test dataset.\\\n",
        "<blue>**model.eval()**</blue> sets the model to <green>**evaluation mode**</green>, disabling dropout layers and stopping the computation of gradients to save memory and speed up computations.\n",
        "For each batch of test images:\n",
        "1) <blue>**Data Movement**</blue>: Images and labels are moved to the <green>**device (CPU/GPU)**</green>.\n",
        "2) <blue>**Forward Pass**</blue>: Images pass through the network to compute <green>**predictions**</green>.\n",
        "3) <blue>**Storing Results**</blue>: Predictions and true labels are saved to <green>**calculate metrics**</green> later.\n",
        "\n",
        "This code also calculates key performance metrics to evaluate the model:\n",
        "1) <blue>**Accuracy**</blue>: Percentage of correctly classified samples.\n",
        "2) <blue>**Precision**</blue>: Proportion of true positive predictions out of all positive predictions.\n",
        "3) <blue>**Recall**</blue>: Proportion of true positives out of actual positive samples.\n",
        "4) <blue>**F1-Score**</blue>: <green>**Harmonic mean**</green> of precision and recall.\n",
        "5) The <blue>**confusion matrix**</blue> is also calculated, showing the number of correct and incorrect predictions for each class. It is visualized using a <green>**heatmap**</green>, where the <blue>**rows**</blue> represent <green>**true labels**</green>, and the <blue>**columns**</blue> represent <green>**predicted labels**</green>.\n",
        "\n",
        "Note: Replace the placeholders **\"None\"** with the appropriate code to calculate the metrics using sklearn and compute the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nFG5UohKs5_"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "model_ann.eval()\n",
        "all_preds_ann = []\n",
        "all_labels_ann = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # TODO: Move images and labels to the device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # TODO: Forward pass\n",
        "        outputs = model_ann(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        all_preds_ann.extend(predicted.cpu().numpy())\n",
        "        all_labels_ann.extend(labels.cpu().numpy())\n",
        "\n",
        "# TODO: Calculate accuracy, precision, recall, and F1-score using sklearn\n",
        "accuracy_ann = accuracy_score(all_labels_ann, all_preds_ann)\n",
        "precision_ann = precision_score(all_labels_ann, all_preds_ann, average='macro', zero_division=0)\n",
        "recall_ann = recall_score(all_labels_ann, all_preds_ann, average='macro', zero_division=0)\n",
        "f1_ann = f1_score(all_labels_ann, all_preds_ann, average='macro', zero_division=0)\n",
        "\n",
        "print(f\"ANN Accuracy: {accuracy_ann:.4f}\")\n",
        "print(f\"ANN Precision: {precision_ann:.4f}\")\n",
        "print(f\"ANN Recall: {recall_ann:.4f}\")\n",
        "print(f\"ANN F1-Score: {f1_ann:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le6yLSF6Ks6A"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "### **Step - 9**\n",
        "This block defines a simple Convolutional Neural Network (CNN) architecture using PyTorch. The model will consist of convolutional layers, activation functions, pooling layers, and fully connected layers to classify images from the CIFAR-10 dataset.\n",
        "1) <blue>**class CNN(nn.Module)**</blue>: This defines a custom CNN class that inherits from PyTorch's nn.Module, the base class for all neural networks in PyTorch.\n",
        "2) <blue>**self.conv**</blue>: These define the two convolutional layers. The first takes an <blue>**input**</blue> with <green>**3 channels (RGB)**</green> and produces <green>**32**</green> <blue>**feature maps**</blue>. The second takes <green>**32**</green> <blue>**input channels**</blue> and produces <green>**64**</green> <blue>**feature maps**</blue>.\n",
        "3) <blue>**self.pool**</blue>: A max-pooling layer that reduces the size of the feature maps by <green>**half (downsampling)**</green>. It takes the maximum value over a <green>**2x2**</green> <blue>**grid**</blue> with a <blue>**stride**</blue> of <green>**2**</green>.\n",
        "4) <blue>**self.fc**</blue>: Fully connected layers. The first layer takes the flattened feature maps from the convolutional layers as input and <blue>**outputs**</blue> <green>**512**</green> features. The second layer maps the <green>**512**</green> <blue>**features**</blue> to <green>**10**</green> <blue>**output classes**</blue> (for the 10 CIFAR-10 categories).\n",
        "5) <blue>**forward(self, x)**</blue>: This function defines how data flows through the network. It applies the convolutional layers, pooling, and fully connected layers in sequence.\n",
        "\n",
        "Note: Replace the placeholders **\"None\"** with the appropriate code to calculate the metrics using sklearn and compute the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcQdhK8bKs6C"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        # TODO: Define the CNN layers (Conv2D, MaxPool, Fully Connected)\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X2ZLwpHKs6D"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "### **Step - 10**\n",
        "This block defines the training loop for the CNN model. It will train the model over multiple epochs, compute the loss using cross-entropy, and optimize the model using an optimizer like SGD.\n",
        "1) <blue>**optimizer.zero_grad()**</blue>: <green>**Clears**</green> the <blue>**gradients**</blue> from the previous iteration to prevent accumulation.\n",
        "2) <blue>**loss.backward()**</blue>: Computes the <blue>**gradients**</blue> via <green>**backpropagation**</green>.\n",
        "3) <blue>**optimizer.step()**</blue>: Updates the model's parameters based on the <green>**computed gradients**</green>.\n",
        "4) <blue>**running_loss**</blue>: Keeps track of the <blue>**cumulative loss**</blue> for the epoch, which is divided by the <green>**number of batches**</green> to return the <blue>**average loss**</blue>.\n",
        "\n",
        "Note: Replace the placeholders **\"None\"** with the appropriate code to calculate the metrics using sklearn and compute the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSZcYh_CKs6F"
      },
      "outputs": [],
      "source": [
        "# Initialize the CNN model, loss function, and optimizer\n",
        "model_cnn = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_cnn.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # TODO: Move images and labels to the device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # TODO: Forward pass\n",
        "        outputs = model_cnn(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXgekN2Ks6G"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "### **Step - 11**\n",
        "This code performs evaluation on the test set by moving data to the appropriate device, running the model to get predictions, and then calculating key performance metrics using the predicted and actual labels.\n",
        "1) <blue>**model_cnn.eval()**</blue>: The code begins by setting the model to <green>**evaluation mode**</green> to ensure proper inference behavior.\n",
        "2) <blue>**all_preds_cnn, all_labels_cnn**</blue>: These lists are initialized to store <green>**predicted**</green> and <green>**true labels**</green>, respectively.\n",
        "3) <blue>**torch.no_grad()**</blue>: This is used to <green>**disable gradients**</green>, save memory and speed up computations, the test set is processed in batches from test_loader, where both images and labels are moved to the correct device.\n",
        "6) After processing the test set, evaluation metrics such as <blue>**accuracy**</blue>, <blue>**precision**</blue>, <blue>**recall**</blue>, and <blue>**F1-score**</blue> are calculated.\n",
        "7) Finally, the results are printed to display the performance of the CNN model\n",
        "\n",
        "Note: Replace the placeholders **\"None\"** with the appropriate code to calculate the metrics using sklearn and compute the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HkMOfQbKs6J"
      },
      "outputs": [],
      "source": [
        "# Test the CNN model\n",
        "model_cnn.eval()\n",
        "all_preds_cnn = []\n",
        "all_labels_cnn = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # TODO: Move images and labels to the device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model_cnn(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        all_preds_cnn.extend(predicted.cpu().numpy())\n",
        "        all_labels_cnn.extend(labels.cpu().numpy())\n",
        "\n",
        "# TODO: Calculate accuracy, precision, recall, and F1-score for the CNN\n",
        "accuracy_cnn = accuracy_score(all_labels_cnn, all_preds_cnn)\n",
        "precision_cnn = precision_score(all_labels_cnn, all_preds_cnn, average='macro', zero_division=0)\n",
        "recall_cnn = recall_score(all_labels_cnn, all_preds_cnn, average='macro', zero_division=0)\n",
        "f1_cnn = f1_score(all_labels_cnn, all_preds_cnn, average='macro', zero_division=0)\n",
        "\n",
        "print(f\"CNN Accuracy: {accuracy_cnn:.4f}\")\n",
        "print(f\"CNN Precision: {precision_cnn:.4f}\")\n",
        "print(f\"CNN Recall: {recall_cnn:.4f}\")\n",
        "print(f\"CNN F1-Score: {f1_cnn:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Otr_qj-TKs6M"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "### **Step - 12**\n",
        "This code block compares performance of the trained ANN and CNN models, showing which one performs better.\n",
        "1) <blue>**import pandas as pd**</blue>: The code starts by importing the <green>**Pandas library**</green>, which is used for data manipulation and creation of a comparison table.\n",
        "2) <blue>**data dictionary**</blue>: A dictionary is defined with the keys 'Model', 'Accuracy', 'Precision', 'Recall', and 'F1-Score', containing <green>**placeholders (None)**</green> for both the 'ANN' and 'CNN' models. These placeholders will later hold the actual performance metrics.\n",
        "3) <blue>**pd.DataFrame(data)**</blue>: The dictionary is converted into a <green>**Pandas DataFrame**</green>, which provides a tabular structure for easy comparison of the metrics between the two models.\n",
        "4) <blue>**print(df_comparison)**</blue>: This line prints the <green>**comparison table**</green> of ANN and CNN metrics, enabling a visual representation of their performance.\n",
        "\n",
        "Note: Replace the placeholders **\"None\"** with the appropriate code to calculate the metrics using sklearn and compute the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aF5qJjUcKs6N"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# TODO: Create a comparison table\n",
        "data = {\n",
        "    'Model': ['ANN', 'CNN'],\n",
        "    'Accuracy': [accuracy_ann, accuracy_cnn],\n",
        "    'Precision': [precision_ann, precision_cnn],\n",
        "    'Recall': [recall_ann, recall_cnn],\n",
        "    'F1-Score': [f1_ann, f1_cnn]\n",
        "}\n",
        "\n",
        "df_comparison = pd.DataFrame(data)\n",
        "print(df_comparison)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAW_O7npNUjU"
      },
      "source": [
        "### Advanced Steps (Optional for people targeting basic level)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b504987"
      },
      "source": [
        "### **Step - 13 (Understanding CNN Filters)**\n",
        "This step aims to provide insight into what the convolutional filters in the CNN model have learned. By visualizing the weights of the filters in the first convolutional layer, we can get a sense of the basic features (like edges, corners, or textures) that the network is detecting in the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d21965d"
      },
      "outputs": [],
      "source": [
        "# Visualize filters from the first convolutional layer of the CNN model\n",
        "\n",
        "# TODO: Get the weights of the first convolutional layer\n",
        "first_conv_layer_weights = model_cnn.conv1.weight.data.cpu().numpy()\n",
        "\n",
        "# TODO: Normalize the weights for visualization\n",
        "# Normalizing helps bring all filter values to a common scale between 0 and 1\n",
        "min_w = first_conv_layer_weights.min()\n",
        "max_w = first_conv_layer_weights.max()\n",
        "first_conv_layer_weights = (first_conv_layer_weights - min_w) / (max_w - min_w + 1e-8)\n",
        "\n",
        "# TODO: Plot the filters\n",
        "# We’ll display up to 16 filters for clarity\n",
        "num_filters = first_conv_layer_weights.shape[0]\n",
        "num_plots = min(num_filters, 16)\n",
        "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(10, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in range(num_plots):\n",
        "    # TODO: Select one filter to visualize\n",
        "    filter_img = first_conv_layer_weights[i].transpose((1, 2, 0))\n",
        "    filter_img = filter_img.mean(axis=2)  # Average across RGB channels\n",
        "    axes[i].imshow(filter_img, cmap='gray')\n",
        "    axes[i].set_title(f'Filter {i+1}')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "# Hide unused subplots\n",
        "for j in range(num_plots, len(axes)):\n",
        "    axes[j].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62b0e7ca"
      },
      "source": [
        "### **Step - 14 (Data Augmentation)**\n",
        "Data augmentation is a technique used to increase the diversity of the training dataset by applying random transformations such as rotations, flips, and crops to the images. This helps to prevent overfitting and can improve the generalization ability of the model. We will apply data augmentation to the training data and retrain the CNN model to see if it improves performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU-uJXJVO7hr"
      },
      "outputs": [],
      "source": [
        "# Define data augmentation transformations\n",
        "train_transform_augmented = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset with data augmentation\n",
        "train_dataset_augmented = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                                      download=True, transform=train_transform_augmented)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform) # Use the original transform for testing\n",
        "\n",
        "train_loader_augmented = DataLoader(dataset=train_dataset_augmented, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# TODO: Initialize CNN model, loss function, and optimizer\n",
        "model_cnn_augmented = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_augmented = optim.Adam(model_cnn_augmented.parameters(), lr=0.001)\n",
        "\n",
        "print(\"Training CNN with Data Augmentation...\")\n",
        "\n",
        "# TODO: Train the CNN model\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader_augmented):\n",
        "\n",
        "        # Move images and labels to the device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_cnn_augmented(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer_augmented.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_augmented.step()\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader_augmented)}], Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6d1dfb5"
      },
      "source": [
        "### **Step - 15 (Visualize Incorrect Predictions)**\n",
        "Visualizing some of the images that the CNN model misclassified can provide valuable insights into the model's weaknesses and help identify areas for improvement. This step will display a few examples of images where the model's predicted label does not match the true label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcf50c4c"
      },
      "outputs": [],
      "source": [
        "# Visualize some incorrect predictions from the CNN model (using the augmented model if trained)\n",
        "# TODO: Choose which model to evaluate (use augmented model if available)\n",
        "model_to_evaluate = model_cnn_augmented if 'model_cnn_augmented' in globals() else model_cnn\n",
        "model_to_evaluate.eval()\n",
        "\n",
        "\n",
        "incorrect_preds = []\n",
        "incorrect_labels = []\n",
        "incorrect_images = []\n",
        "\n",
        "# TODO: Run the model on the test dataset\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images_cpu = images.clone()\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_to_evaluate(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        for i in range(len(labels)):\n",
        "            if predicted[i] != labels[i]:\n",
        "                incorrect_preds.append(predicted[i].item())\n",
        "                incorrect_labels.append(labels[i].item())\n",
        "                # Denormalize the image before storing\n",
        "                img = images_cpu[i].numpy().transpose((1, 2, 0))\n",
        "                img = 0.5 * img + 0.5 # Denormalize\n",
        "                img = np.clip(img, 0, 1)\n",
        "                incorrect_images.append(img)\n",
        "\n",
        "# Display up to 10 incorrect predictions, add more visualisations if possible\n",
        "num_to_display = min(len(incorrect_images), 10)\n",
        "fig, axes = plt.subplots(nrows=1, ncols=num_to_display, figsize=(20, 4))\n",
        "\n",
        "if num_to_display > 0:\n",
        "    for i in range(num_to_display):\n",
        "        axes[i].imshow(incorrect_images[i])\n",
        "        axes[i].set_title(f'Pred: {incorrect_preds[i]}, True: {incorrect_labels[i]}')\n",
        "        axes[i].axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No incorrect predictions to display.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CIFAR-10 class names\n",
        "cifar10_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Calculate confusion matrices\n",
        "cm_ann = confusion_matrix(all_labels_ann, all_preds_ann)\n",
        "cm_cnn = confusion_matrix(all_labels_cnn, all_preds_cnn)\n",
        "\n",
        "# Plot confusion matrices\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# ANN confusion matrix\n",
        "sns.heatmap(cm_ann, annot=True, fmt='d', cmap='Blues', ax=axes[0], \n",
        "            xticklabels=cifar10_classes, yticklabels=cifar10_classes)\n",
        "axes[0].set_title('ANN Confusion Matrix')\n",
        "axes[0].set_xlabel('Predicted')\n",
        "axes[0].set_ylabel('Actual')\n",
        "\n",
        "# CNN confusion matrix\n",
        "sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
        "            xticklabels=cifar10_classes, yticklabels=cifar10_classes)\n",
        "axes[1].set_title('CNN Confusion Matrix')\n",
        "axes[1].set_xlabel('Predicted')\n",
        "axes[1].set_ylabel('Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print some statistics\n",
        "print(\"ANN Confusion Matrix:\\n\", cm_ann)\n",
        "print(\"\\nCNN Confusion Matrix:\\n\", cm_cnn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **ROC Curves**\n",
        "Receiver Operating Characteristic (ROC) curves are plotted for each class to show the trade-off between true positive rate and false positive rate at different classification thresholds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "\n",
        "# Get probability scores from both models\n",
        "model_ann.eval()\n",
        "model_cnn.eval()\n",
        "\n",
        "all_probs_ann = []\n",
        "all_probs_cnn = []\n",
        "all_labels_roc = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Get probabilities\n",
        "        outputs_ann = model_ann(images)\n",
        "        outputs_cnn = model_cnn(images)\n",
        "        \n",
        "        probs_ann = torch.softmax(outputs_ann, dim=1)\n",
        "        probs_cnn = torch.softmax(outputs_cnn, dim=1)\n",
        "        \n",
        "        all_probs_ann.extend(probs_ann.cpu().numpy())\n",
        "        all_probs_cnn.extend(probs_cnn.cpu().numpy())\n",
        "        all_labels_roc.extend(labels.cpu().numpy())\n",
        "\n",
        "all_probs_ann = np.array(all_probs_ann)\n",
        "all_probs_cnn = np.array(all_probs_cnn)\n",
        "all_labels_roc = np.array(all_labels_roc)\n",
        "\n",
        "# Binarize the labels for multi-class ROC\n",
        "y_test_bin = label_binarize(all_labels_roc, classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
        "\n",
        "print(\"Computing ROC curves...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot ROC curves for ANN\n",
        "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
        "\n",
        "# ANN ROC curves\n",
        "n_classes = 10\n",
        "for i in range(n_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], all_probs_ann[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    axes[0].plot(fpr, tpr, lw=2, label=f'{cifar10_classes[i]} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "axes[0].plot([0, 1], [0, 1], 'k--', lw=2, label='Random')\n",
        "axes[0].set_xlim([0.0, 1.0])\n",
        "axes[0].set_ylim([0.0, 1.05])\n",
        "axes[0].set_xlabel('False Positive Rate')\n",
        "axes[0].set_ylabel('True Positive Rate')\n",
        "axes[0].set_title('ANN - ROC Curves for Each Class')\n",
        "axes[0].legend(loc=\"lower right\", fontsize=8)\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# CNN ROC curves\n",
        "for i in range(n_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], all_probs_cnn[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    axes[1].plot(fpr, tpr, lw=2, label=f'{cifar10_classes[i]} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "axes[1].plot([0, 1], [0, 1], 'k--', lw=2, label='Random')\n",
        "axes[1].set_xlim([0.0, 1.0])\n",
        "axes[1].set_ylim([0.0, 1.05])\n",
        "axes[1].set_xlabel('False Positive Rate')\n",
        "axes[1].set_ylabel('True Positive Rate')\n",
        "axes[1].set_title('CNN - ROC Curves for Each Class')\n",
        "axes[1].legend(loc=\"lower right\", fontsize=8)\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate macro-averaged AUC\n",
        "roc_auc_ann_macro = roc_auc_score(y_test_bin, all_probs_ann, average='macro')\n",
        "roc_auc_cnn_macro = roc_auc_score(y_test_bin, all_probs_cnn, average='macro')\n",
        "\n",
        "print(f\"\\nANN Macro-averaged AUC: {roc_auc_ann_macro:.4f}\")\n",
        "print(f\"CNN Macro-averaged AUC: {roc_auc_cnn_macro:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Updated comparison table with AUC scores\n",
        "comparison_data = {\n",
        "    'Model': ['ANN', 'CNN'],\n",
        "    'Accuracy': [accuracy_ann, accuracy_cnn],\n",
        "    'Precision': [precision_ann, precision_cnn],\n",
        "    'Recall': [recall_ann, recall_cnn],\n",
        "    'F1-Score': [f1_ann, f1_cnn],\n",
        "    'AUC-Score': [roc_auc_ann_macro, roc_auc_cnn_macro]\n",
        "}\n",
        "\n",
        "df_final_comparison = pd.DataFrame(comparison_data)\n",
        "print(\"\\nFinal Model Comparison:\")\n",
        "print(df_final_comparison)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aiml_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
