{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad1149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Section 1: Setup and Environment\n",
    "# ==============================================================================\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import cv2\n",
    "\n",
    "# Define base paths\n",
    "# IMPORTANT: Please update this path to the root directory of your dataset\n",
    "BASE_PATH = './2025_Karyogram_CV_Camp/'\n",
    "\n",
    "SINGLE_CHROMOSOME_PATH = os.path.join(BASE_PATH, 'single_chromosomes_object')\n",
    "MULTI_CHROMOSOME_PATH = os.path.join(BASE_PATH, '24_chromsomes_object')\n",
    "TRAIN_TXT_PATH = os.path.join(BASE_PATH, 'train.txt')\n",
    "TEST_TXT_PATH = os.path.join(BASE_PATH, 'test.txt')\n",
    "\n",
    "# Check if paths exist\n",
    "if not os.path.exists(BASE_PATH):\n",
    "    print(f\"Error: The base path '{BASE_PATH}' does not exist.\")\n",
    "    print(\"Please download the dataset and update the BASE_PATH variable.\")\n",
    "    # As a fallback for demonstration, we will create dummy directories\n",
    "    os.makedirs(SINGLE_CHROMOSOME_PATH, exist_ok=True)\n",
    "    os.makedirs(MULTI_CHROMOSOME_PATH, exist_ok=True)\n",
    "    # Create dummy train/test files\n",
    "    with open(TRAIN_TXT_PATH, 'w') as f: f.write('dummy_image_1\\n')\n",
    "    with open(TEST_TXT_PATH, 'w') as f: f.write('dummy_image_2\\n')\n",
    "\n",
    "# Setup device\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Using GPU: {gpus[0].name}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Section 2: Data Loading and Preprocessing (Framework Agnostic)\n",
    "# ==============================================================================\n",
    "def parse_xml_annotation(xml_path):\n",
    "    \"\"\"Parses a single XML annotation file to extract bounding box info.\"\"\"\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    objects = []\n",
    "    for obj in root.findall('object'):\n",
    "        name = obj.find('name').text\n",
    "        bndbox = obj.find('bndbox')\n",
    "        xmin = int(bndbox.find('xmin').text)\n",
    "        ymin = int(bndbox.find('ymin').text)\n",
    "        xmax = int(bndbox.find('xmax').text)\n",
    "        ymax = int(bndbox.find('ymax').text)\n",
    "        objects.append({'name': name, 'xmin': xmin, 'ymin': ymin, 'xmax': xmax, 'ymax': ymax})\n",
    "    return objects\n",
    "\n",
    "def create_dataframe(image_ids, data_path):\n",
    "    \"\"\"Creates a pandas DataFrame from a list of image IDs and their annotations.\"\"\"\n",
    "    records = []\n",
    "    for img_id in tqdm(image_ids, desc=f\"Processing {os.path.basename(data_path)}\"):\n",
    "        xml_path = os.path.join(data_path, f\"{img_id}.xml\")\n",
    "        img_path = os.path.join(data_path, f\"{img_id}.jpg\")\n",
    "        if os.path.exists(xml_path) and os.path.exists(img_path):\n",
    "            annotations = parse_xml_annotation(xml_path)\n",
    "            for ann in annotations:\n",
    "                records.append({\n",
    "                    'image_id': img_id,\n",
    "                    'image_path': img_path,\n",
    "                    'class_name': ann['name'],\n",
    "                    'xmin': ann['xmin'],\n",
    "                    'ymin': ann['ymin'],\n",
    "                    'xmax': ann['xmax'],\n",
    "                    'ymax': ann['ymax']\n",
    "                })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Load train/test splits\n",
    "try:\n",
    "    with open(TRAIN_TXT_PATH, 'r') as f:\n",
    "        train_ids = [line.strip() for line in f.readlines()]\n",
    "    with open(TEST_TXT_PATH, 'r') as f:\n",
    "        test_ids = [line.strip() for line in f.readlines()]\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: train.txt or test.txt not found. Using dummy IDs.\")\n",
    "    train_ids, test_ids = [], []\n",
    "\n",
    "# Create DataFrames\n",
    "print(\"Creating DataFrames for Identifier...\")\n",
    "df_identifier_train = create_dataframe(train_ids, SINGLE_CHROMOSOME_PATH)\n",
    "df_identifier_test = create_dataframe(test_ids, SINGLE_CHROMOSOME_PATH)\n",
    "\n",
    "print(\"\\nCreating DataFrames for Classifier...\")\n",
    "df_classifier_train = create_dataframe(train_ids, MULTI_CHROMOSOME_PATH)\n",
    "df_classifier_test = create_dataframe(test_ids, MULTI_CHROMOSOME_PATH)\n",
    "\n",
    "# ==============================================================================\n",
    "# Section 3: Module 1 - Identifier (Segmentation Model)\n",
    "# ==============================================================================\n",
    "\n",
    "# 3.1. Identifier Data Pipeline (tf.data)\n",
    "def load_identifier_data(df_row, img_size=(256, 256)):\n",
    "    \"\"\"Loads a single image and its corresponding segmentation mask.\"\"\"\n",
    "    image_path = df_row['image_path']\n",
    "    # Load image\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_jpeg(image, channels=1) # Grayscale\n",
    "    original_shape = tf.shape(image)\n",
    "    image = tf.image.resize(image, img_size)\n",
    "    image = (tf.cast(image, tf.float32) / 127.5) - 1 # Normalize to [-1, 1]\n",
    "\n",
    "    # Create mask\n",
    "    mask = np.zeros((original_shape[0], original_shape[1]), dtype=np.uint8)\n",
    "    records = df_identifier_train[df_identifier_train['image_path'] == image_path.numpy().decode()]\n",
    "    for _, row in records.iterrows():\n",
    "        cv2.rectangle(mask, (row['xmin'], row['ymin']), (row['xmax'], row['ymax']), color=1, thickness=-1)\n",
    "    \n",
    "    mask = tf.convert_to_tensor(mask, dtype=tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=-1)\n",
    "    mask = tf.image.resize(mask, img_size, method='nearest')\n",
    "    return image, mask\n",
    "\n",
    "def identifier_data_generator(df):\n",
    "    \"\"\"Generator function to yield unique image paths for the identifier dataset.\"\"\"\n",
    "    unique_images = df.drop_duplicates(subset=['image_path']).reset_index(drop=True)\n",
    "    for i in range(len(unique_images)):\n",
    "        yield (unique_images.iloc[i],)\n",
    "\n",
    "def create_tf_dataset_identifier(df, batch_size=8):\n",
    "    \"\"\"Creates a tf.data.Dataset for the identifier model.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: identifier_data_generator(df),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(), dtype=tf.string, name='image_path'),\n",
    "        )\n",
    "    )\n",
    "    # The actual processing function needs to be wrapped in py_function because it uses pandas and cv2\n",
    "    def process_path(path_tensor):\n",
    "        path_str = path_tensor.numpy().decode()\n",
    "        # Create a dummy row dict to pass to the loading function\n",
    "        row = {'image_path': path_str}\n",
    "        return load_identifier_data(row)\n",
    "\n",
    "    dataset = dataset.map(lambda row_tuple: tf.py_function(load_identifier_data, [row_tuple[0]], [tf.float32, tf.float32]),\n",
    "                          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# 3.2. Identifier Model Architecture (U-Net)\n",
    "def build_unet(input_shape=(256, 256, 1)):\n",
    "    \"\"\"Builds a U-Net model using Keras Functional API.\"\"\"\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    c1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    c1 = layers.Conv2D(64, 3, activation='relu', padding='same')(c1)\n",
    "    p1 = layers.MaxPooling2D(2)(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(128, 3, activation='relu', padding='same')(p1)\n",
    "    c2 = layers.Conv2D(128, 3, activation='relu', padding='same')(c2)\n",
    "    p2 = layers.MaxPooling2D(2)(c2)\n",
    "\n",
    "    c3 = layers.Conv2D(256, 3, activation='relu', padding='same')(p2)\n",
    "    c3 = layers.Conv2D(256, 3, activation='relu', padding='same')(c3)\n",
    "    p3 = layers.MaxPooling2D(2)(c3)\n",
    "\n",
    "    # Bottleneck\n",
    "    bn = layers.Conv2D(512, 3, activation='relu', padding='same')(p3)\n",
    "    bn = layers.Conv2D(512, 3, activation='relu', padding='same')(bn)\n",
    "\n",
    "    # Decoder\n",
    "    u3 = layers.Conv2DTranspose(256, 2, strides=2, padding='same')(bn)\n",
    "    u3 = layers.concatenate([u3, c3])\n",
    "    d3 = layers.Conv2D(256, 3, activation='relu', padding='same')(u3)\n",
    "    d3 = layers.Conv2D(256, 3, activation='relu', padding='same')(d3)\n",
    "\n",
    "    u2 = layers.Conv2DTranspose(128, 2, strides=2, padding='same')(d3)\n",
    "    u2 = layers.concatenate([u2, c2])\n",
    "    d2 = layers.Conv2D(128, 3, activation='relu', padding='same')(u2)\n",
    "    d2 = layers.Conv2D(128, 3, activation='relu', padding='same')(d2)\n",
    "\n",
    "    u1 = layers.Conv2DTranspose(64, 2, strides=2, padding='same')(d2)\n",
    "    u1 = layers.concatenate([u1, c1])\n",
    "    d1 = layers.Conv2D(64, 3, activation='relu', padding='same')(u1)\n",
    "    d1 = layers.Conv2D(64, 3, activation='relu', padding='same')(d1)\n",
    "\n",
    "    outputs = layers.Conv2D(1, 1, activation='sigmoid')(d1) # Use sigmoid for binary segmentation\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# 3.3. Identifier Visualization\n",
    "def visualize_segmentation_tf(model, dataset, num_images=3):\n",
    "    \"\"\"Visualizes segmentation results for a TensorFlow model.\"\"\"\n",
    "    plt.figure(figsize=(12, num_images * 4))\n",
    "    for i, (image, mask) in enumerate(dataset.take(num_images)):\n",
    "        pred_mask = model.predict(image)[0]\n",
    "\n",
    "        plt.subplot(num_images, 3, i * 3 + 1)\n",
    "        plt.title(\"Input Image\")\n",
    "        plt.imshow(image[0, :, :, 0] * 0.5 + 0.5, cmap='gray') # Denormalize\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(num_images, 3, i * 3 + 2)\n",
    "        plt.title(\"Ground Truth Mask\")\n",
    "        plt.imshow(mask[0, :, :, 0], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(num_images, 3, i * 3 + 3)\n",
    "        plt.title(\"Predicted Mask\")\n",
    "        plt.imshow(pred_mask[:, :, 0] > 0.5, cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5f7215",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# Section 4: Module 2 - Classifier\n",
    "# ==============================================================================\n",
    "\n",
    "# 4.1. Classifier Data Pipeline (tf.data)\n",
    "def setup_classifier_data():\n",
    "    if not df_classifier_train.empty:\n",
    "        class_names = sorted(df_classifier_train['class_name'].unique())\n",
    "        if 'X' in class_names: class_names.remove('X')\n",
    "        if 'Y' in class_names: class_names.remove('Y')\n",
    "        class_names = sorted(class_names, key=lambda x: int(x) if x.isdigit() else 99)\n",
    "        if 'X' in df_classifier_train['class_name'].unique(): class_names.append('X')\n",
    "        if 'Y' in df_classifier_train['class_name'].unique(): class_names.append('Y')\n",
    "        \n",
    "        class_to_idx = {name: i for i, name in enumerate(class_names)}\n",
    "        idx_to_class = {i: name for i, name in enumerate(class_names)}\n",
    "        num_classes = len(class_names)\n",
    "        print(f\"Number of classes: {num_classes}\")\n",
    "        return num_classes, class_to_idx, idx_to_class\n",
    "    else:\n",
    "        print(\"Classifier DataFrame is empty. Using fallback class count.\")\n",
    "        return 24, {}, {}\n",
    "\n",
    "def load_classifier_data(row, class_map, img_size=(64, 64)):\n",
    "    image = Image.open(row['image_path']).convert('L')\n",
    "    cropped_image = image.crop((row['xmin'], row['ymin'], row['xmax'], row['ymax']))\n",
    "    cropped_image = cropped_image.resize(img_size)\n",
    "    image_np = np.array(cropped_image, dtype=np.float32)\n",
    "    image_np = (image_np / 127.5) - 1 # Normalize to [-1, 1]\n",
    "    image_np = np.expand_dims(image_np, axis=-1)\n",
    "    \n",
    "    label = class_map[row['class_name']]\n",
    "    return image_np, label\n",
    "\n",
    "def classifier_data_generator(df, class_map):\n",
    "    for _, row in df.iterrows():\n",
    "        yield load_classifier_data(row, class_map)\n",
    "\n",
    "def create_tf_dataset_classifier(df, class_map, batch_size=32):\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: classifier_data_generator(df, class_map),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(64, 64, 1), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "        )\n",
    "    )\n",
    "    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 4.2. Classifier Model Architecture (CNN)\n",
    "def build_classifier(num_classes, input_shape=(64, 64, 1)):\n",
    "    \"\"\"Builds a simple CNN for classification.\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes) # Logits output\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# ==============================================================================\n",
    "# Section 5 & 6: Pipeline Integration and Evaluation\n",
    "# ==============================================================================\n",
    "def karyotype_pipeline_tf(image_path, identifier, classifier, class_map):\n",
    "    \"\"\"Runs the full end-to-end pipeline using TensorFlow models.\"\"\"\n",
    "    # 1. Load image and preprocess for identifier\n",
    "    img_orig = Image.open(image_path).convert('L')\n",
    "    img_arr = np.array(img_orig)\n",
    "    img_tensor = tf.convert_to_tensor(img_arr, dtype=tf.float32)\n",
    "    img_tensor = tf.expand_dims(img_tensor, axis=-1)\n",
    "    img_resized = tf.image.resize(img_tensor, (256, 256))\n",
    "    img_normalized = (img_resized / 127.5) - 1\n",
    "    \n",
    "    # 2. Get segmentation mask\n",
    "    mask_pred = identifier.predict(tf.expand_dims(img_normalized, axis=0))[0]\n",
    "    mask_binary = (mask_pred > 0.5).astype(np.uint8)\n",
    "\n",
    "    # 3. Find contours\n",
    "    contours, _ = cv2.findContours(mask_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    results = []\n",
    "    original_h, original_w = img_arr.shape\n",
    "    mask_h, mask_w, _ = mask_binary.shape\n",
    "    \n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        if w * h < 20: continue\n",
    "\n",
    "        # 4. Scale bbox and crop for classifier\n",
    "        orig_x = int(x * original_w / mask_w)\n",
    "        orig_y = int(y * original_h / mask_h)\n",
    "        orig_w = int(w * original_w / mask_w)\n",
    "        orig_h = int(h * original_h / mask_h)\n",
    "        \n",
    "        crop = img_orig.crop((orig_x, orig_y, orig_x + orig_w, orig_y + orig_h))\n",
    "        crop_resized = crop.resize((64, 64))\n",
    "        crop_arr = np.array(crop_resized, dtype=np.float32)\n",
    "        crop_arr = (crop_arr / 127.5) - 1\n",
    "        crop_tensor = tf.expand_dims(tf.expand_dims(crop_arr, axis=-1), axis=0)\n",
    "\n",
    "        # 5. Classify chromosome\n",
    "        logits = classifier.predict(crop_tensor)\n",
    "        probs = tf.nn.softmax(logits, axis=-1).numpy().flatten()\n",
    "        pred_idx = np.argmax(probs)\n",
    "        confidence = probs[pred_idx]\n",
    "        pred_class = class_map[pred_idx]\n",
    "\n",
    "        results.append({\n",
    "            'bbox': [orig_x, orig_y, orig_x + orig_w, orig_y + orig_h],\n",
    "            'class_name': pred_class,\n",
    "            'confidence': confidence,\n",
    "            'all_probs': probs\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def iou(boxA, boxB):\n",
    "    \"\"\"Calculate Intersection over Union (IoU) of two bounding boxes.\"\"\"\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    return interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "def evaluate_auprc_tf(test_df, identifier, classifier, class_to_idx, idx_to_class, iou_thresh=0.5):\n",
    "    \"\"\"Evaluates the full pipeline and computes auPRC.\"\"\"\n",
    "    y_true, y_scores = [], []\n",
    "    num_classes = len(class_to_idx)\n",
    "    \n",
    "    unique_test_images = test_df.drop_duplicates(subset=['image_id'])\n",
    "    for _, row in tqdm(unique_test_images.iterrows(), total=len(unique_test_images), desc=\"Evaluating auPRC\"):\n",
    "        predictions = karyotype_pipeline_tf(row['image_path'], identifier, classifier, idx_to_class)\n",
    "        \n",
    "        gt_rows = test_df[test_df['image_id'] == row['image_id']]\n",
    "        gt_boxes = [list(r[['xmin', 'ymin', 'xmax', 'ymax']]) for _, r in gt_rows.iterrows()]\n",
    "        gt_labels = [class_to_idx[name] for name in gt_rows['class_name']]\n",
    "\n",
    "        for pred in predictions:\n",
    "            best_iou, best_gt_idx = 0, -1\n",
    "            for i, gt_box in enumerate(gt_boxes):\n",
    "                current_iou = iou(pred['bbox'], gt_box)\n",
    "                if current_iou > best_iou:\n",
    "                    best_iou, best_gt_idx = current_iou, i\n",
    "            \n",
    "            pred_label_idx = class_to_idx[pred['class_name']]\n",
    "            \n",
    "            for c in range(num_classes):\n",
    "                score = pred['all_probs'][c]\n",
    "                if c == pred_label_idx:\n",
    "                    match = 1 if best_iou >= iou_thresh and best_gt_idx != -1 and pred_label_idx == gt_labels[best_gt_idx] else 0\n",
    "                    y_true.append(match)\n",
    "                    y_scores.append(score)\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    auprc = auc(recall, precision)\n",
    "    return precision, recall, auprc\n",
    "\n",
    "# ==============================================================================\n",
    "# Main Execution Block\n",
    "# ==============================================================================\n",
    "if __name__ == '__main__':\n",
    "    # --- Identifier Training ---\n",
    "    if not df_identifier_train.empty:\n",
    "        train_ds_id = create_tf_dataset_identifier(df_identifier_train)\n",
    "        test_ds_id = create_tf_dataset_identifier(df_identifier_test)\n",
    "        \n",
    "        identifier_model = build_unet()\n",
    "        identifier_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        print(\"\\n--- Training Identifier Model ---\")\n",
    "        identifier_model.fit(train_ds_id, validation_data=test_ds_id, epochs=1) # Use more epochs for real training\n",
    "        identifier_model.save('identifier_model.keras')\n",
    "        \n",
    "        print(\"\\n--- Visualizing Identifier Results ---\")\n",
    "        visualize_segmentation_tf(identifier_model, test_ds_id)\n",
    "    else:\n",
    "        print(\"Identifier DataFrames are empty. Skipping Identifier training.\")\n",
    "\n",
    "    # --- Classifier Training ---\n",
    "    NUM_CLASSES, CLASS_TO_IDX, IDX_TO_CLASS = setup_classifier_data()\n",
    "    if not df_classifier_train.empty:\n",
    "        train_ds_cl = create_tf_dataset_classifier(df_classifier_train, CLASS_TO_IDX)\n",
    "        test_ds_cl = create_tf_dataset_classifier(df_classifier_test, CLASS_TO_IDX)\n",
    "\n",
    "        classifier_model = build_classifier(NUM_CLASSES)\n",
    "        classifier_model.compile(optimizer='adam',\n",
    "                                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                                 metrics=['accuracy'])\n",
    "        print(\"\\n--- Training Classifier Model ---\")\n",
    "        classifier_model.fit(train_ds_cl, validation_data=test_ds_cl, epochs=1) # Use more epochs for real training\n",
    "        classifier_model.save('classifier_model.keras')\n",
    "    else:\n",
    "        print(\"Classifier DataFrames are empty. Skipping Classifier training.\")\n",
    "\n",
    "    # --- Full Pipeline Execution and Evaluation ---\n",
    "    print(\"\\n--- Running Full Pipeline and Evaluation ---\")\n",
    "    try:\n",
    "        # Load models if not in memory\n",
    "        if 'identifier_model' not in locals():\n",
    "            identifier_model = keras.models.load_model('identifier_model.keras')\n",
    "        if 'classifier_model' not in locals():\n",
    "            classifier_model = keras.models.load_model('classifier_model.keras')\n",
    "\n",
    "        if not df_classifier_test.empty:\n",
    "            precision, recall, auprc = evaluate_auprc_tf(df_classifier_test, identifier_model, classifier_model, CLASS_TO_IDX, IDX_TO_CLASS)\n",
    "            print(f\"Final auPRC: {auprc:.4f}\")\n",
    "\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.plot(recall, precision, marker='.', label=f'auPRC = {auprc:.3f}')\n",
    "            plt.title('Precision-Recall Curve (TensorFlow)')\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "    except (NameError, FileNotFoundError) as e:\n",
    "        print(f\"Could not run evaluation: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
