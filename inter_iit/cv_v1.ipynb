{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "aXbfw4ci5zCN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXbfw4ci5zCN",
        "outputId": "27d84237-a9ca-4cb7-dab2-bdde9f9724f0"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d0f9667",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d0f9667",
        "outputId": "33362c43-6e4f-4bba-8655-6bc210252181"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CELL 1: TRAINING CHROMOSOME IDENTIFIER\n",
            "============================================================\n",
            "\n",
            "TensorFlow version: 2.19.0\n",
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Found 3 images for identifier training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-14 05:38:49.587777: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3\n",
            "2025-10-14 05:38:49.587945: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
            "2025-10-14 05:38:49.587955: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1760400529.588538 3735280 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "I0000 00:00:1760400529.588902 3735280 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting identifier training...\n",
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 278\u001b[0m\n\u001b[1;32m    275\u001b[0m identifier \u001b[38;5;241m=\u001b[39m ChromosomeIdentifier(img_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m, max_detections\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting identifier training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 278\u001b[0m \u001b[43midentifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_identifier_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m identifier\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midentifier_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâœ“ Identifier model saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midentifier_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[2], line 190\u001b[0m, in \u001b[0;36mChromosomeIdentifier.train\u001b[0;34m(self, train_gen, epochs)\u001b[0m\n\u001b[1;32m    187\u001b[0m images \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(images)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m--> 190\u001b[0m     bbox_pred, obj_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     bbox_pred \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(bbox_pred, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_detections, \u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m    193\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/keras/src/layers/layer.py:936\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    934\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[1;32m    940\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/keras/src/ops/operation.py:58\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m     54\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[1;32m     55\u001b[0m         call_fn,\n\u001b[1;32m     56\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     57\u001b[0m     )\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/keras/src/models/functional.py:183\u001b[0m, in \u001b[0;36mFunctional.call\u001b[0;34m(self, inputs, training, mask, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m             backend\u001b[38;5;241m.\u001b[39mset_keras_mask(x, mask)\n\u001b[0;32m--> 183\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_through_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unpack_singleton(outputs)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/keras/src/ops/function.py:177\u001b[0m, in \u001b[0;36mFunction._run_through_graph\u001b[0;34m(self, inputs, operation_fn, call_fn)\u001b[0m\n\u001b[1;32m    175\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m call_fn(op, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(node\u001b[38;5;241m.\u001b[39moutputs, tree\u001b[38;5;241m.\u001b[39mflatten(outputs)):\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/keras/src/models/functional.py:648\u001b[0m, in \u001b[0;36moperation_fn.<locals>.call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    643\u001b[0m         name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(operation, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_call_context_args\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    645\u001b[0m     ):\n\u001b[1;32m    646\u001b[0m         kwargs[name] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m--> 648\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moperation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/keras/src/layers/layer.py:936\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    934\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[1;32m    940\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/keras/src/ops/operation.py:58\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m     54\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[1;32m     55\u001b[0m         call_fn,\n\u001b[1;32m     56\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     57\u001b[0m     )\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/keras/src/layers/pooling/global_average_pooling2d.py:67\u001b[0m, in \u001b[0;36mGlobalAveragePooling2D.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannels_last\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 67\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mmean(inputs, axis\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeepdims)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/keras/src/ops/numpy.py:6596\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(x, axis, keepdims)\u001b[0m\n\u001b[1;32m   6594\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x,)):\n\u001b[1;32m   6595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Mean(axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39mkeepdims)\u001b[38;5;241m.\u001b[39msymbolic_call(x)\n\u001b[0;32m-> 6596\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/numpy.py:686\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(x, axis, keepdims)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    685\u001b[0m     result_dtype \u001b[38;5;241m=\u001b[39m ori_dtype\n\u001b[0;32m--> 686\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_mean\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcast(output, result_dtype)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:88\u001b[0m, in \u001b[0;36mweak_tensor_unary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     87\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py:2593\u001b[0m, in \u001b[0;36mreduce_mean\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   2541\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the mean of elements across dimensions of a tensor.\u001b[39;00m\n\u001b[1;32m   2542\u001b[0m \n\u001b[1;32m   2543\u001b[0m \u001b[38;5;124;03mReduces `input_tensor` along the dimensions given in `axis` by computing the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2588\u001b[0m \u001b[38;5;124;03m@end_compatibility\u001b[39;00m\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2590\u001b[0m keepdims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(keepdims)\n\u001b[1;32m   2591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _may_reduce_to_scalar(\n\u001b[1;32m   2592\u001b[0m     keepdims, axis,\n\u001b[0;32m-> 2593\u001b[0m     \u001b[43mgen_math_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2594\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_ReductionDims\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/tensorflow/python/ops/gen_math_ops.py:6517\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[1;32m   6515\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   6516\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 6517\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmean_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   6518\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6519\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[1;32m   6520\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/tensorflow/python/ops/gen_math_ops.py:6549\u001b[0m, in \u001b[0;36mmean_eager_fallback\u001b[0;34m(input, axis, keep_dims, name, ctx)\u001b[0m\n\u001b[1;32m   6547\u001b[0m _inputs_flat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28minput\u001b[39m, axis]\n\u001b[1;32m   6548\u001b[0m _attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep_dims\u001b[39m\u001b[38;5;124m\"\u001b[39m, keep_dims, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_T, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTidx\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_Tidx)\n\u001b[0;32m-> 6549\u001b[0m _result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_inputs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6550\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n\u001b[1;32m   6552\u001b[0m   _execute\u001b[38;5;241m.\u001b[39mrecord_gradient(\n\u001b[1;32m   6553\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean\u001b[39m\u001b[38;5;124m\"\u001b[39m, _inputs_flat, _attrs, _result)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 1: TRAIN IDENTIFIER MODULE\n",
        "# Uses ALL data from single_chromosomes_object folder\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Dict\n",
        "import albumentations as A\n",
        "\n",
        "# ==================== SHARED UTILITIES ====================\n",
        "\n",
        "class AnnotationParser:\n",
        "    \"\"\"Parse XML annotations for both single and 24-chromosome datasets\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_xml(xml_path: str) -> Dict:\n",
        "        tree = ET.parse(xml_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        annotations = {\n",
        "            'filename': root.find('filename').text if root.find('filename') is not None else '',\n",
        "            'size': {},\n",
        "            'objects': []\n",
        "        }\n",
        "\n",
        "        size = root.find('size')\n",
        "        if size is not None:\n",
        "            annotations['size'] = {\n",
        "                'width': int(size.find('width').text),\n",
        "                'height': int(size.find('height').text),\n",
        "                'depth': int(size.find('depth').text) if size.find('depth') is not None else 3\n",
        "            }\n",
        "\n",
        "        for obj in root.findall('object'):\n",
        "            name = obj.find('name').text\n",
        "            bndbox = obj.find('bndbox')\n",
        "\n",
        "            if bndbox is not None:\n",
        "                bbox = {\n",
        "                    'xmin': int(float(bndbox.find('xmin').text)),\n",
        "                    'ymin': int(float(bndbox.find('ymin').text)),\n",
        "                    'xmax': int(float(bndbox.find('xmax').text)),\n",
        "                    'ymax': int(float(bndbox.find('ymax').text))\n",
        "                }\n",
        "\n",
        "                annotations['objects'].append({\n",
        "                    'name': name,\n",
        "                    'bbox': bbox\n",
        "                })\n",
        "\n",
        "        return annotations\n",
        "\n",
        "\n",
        "# ==================== IDENTIFIER MODULE ====================\n",
        "\n",
        "class ChromosomeIdentifierDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Data generator for training chromosome identifier\"\"\"\n",
        "\n",
        "    def __init__(self, images_dir: str, annotations_dir: str,\n",
        "                 batch_size=2, img_size=800, shuffle=True):\n",
        "        self.images_dir = images_dir\n",
        "        self.annotations_dir = annotations_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.shuffle = shuffle\n",
        "        self.parser = AnnotationParser()\n",
        "\n",
        "        # Get all image files from directory\n",
        "        self.image_list = []\n",
        "        for filename in os.listdir(images_dir):\n",
        "            if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "                base_name = os.path.splitext(filename)[0]\n",
        "                xml_path = os.path.join(annotations_dir, f\"{base_name}.xml\")\n",
        "                if os.path.exists(xml_path):\n",
        "                    self.image_list.append(base_name)\n",
        "\n",
        "        print(f\"Found {len(self.image_list)} images for identifier training\")\n",
        "\n",
        "        self.indexes = np.arange(len(self.image_list))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.image_list) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        batch_images = [self.image_list[k] for k in batch_indexes]\n",
        "\n",
        "        X, y = self._generate_data(batch_images)\n",
        "        return X, y\n",
        "\n",
        "    def _generate_data(self, batch_images):\n",
        "        images = []\n",
        "        targets = []\n",
        "\n",
        "        for img_name in batch_images:\n",
        "            img_path = os.path.join(self.images_dir, f\"{img_name}.jpg\")\n",
        "            xml_path = os.path.join(self.annotations_dir, f\"{img_name}.xml\")\n",
        "\n",
        "            if not os.path.exists(img_path):\n",
        "                img_path = os.path.join(self.images_dir, f\"{img_name}.png\")\n",
        "\n",
        "            if not os.path.exists(img_path) or not os.path.exists(xml_path):\n",
        "                continue\n",
        "\n",
        "            image = cv2.imread(img_path)\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            orig_h, orig_w = image.shape[:2]\n",
        "\n",
        "            image = cv2.resize(image, (self.img_size, self.img_size))\n",
        "            image = image.astype(np.float32) / 255.0\n",
        "\n",
        "            annotations = self.parser.parse_xml(xml_path)\n",
        "\n",
        "            boxes = []\n",
        "            for obj in annotations['objects']:\n",
        "                bbox = obj['bbox']\n",
        "                xmin = bbox['xmin'] / orig_w\n",
        "                ymin = bbox['ymin'] / orig_h\n",
        "                xmax = bbox['xmax'] / orig_w\n",
        "                ymax = bbox['ymax'] / orig_h\n",
        "                boxes.append([ymin, xmin, ymax, xmax])\n",
        "\n",
        "            images.append(image)\n",
        "            targets.append(np.array(boxes) if len(boxes) > 0 else np.zeros((0, 4)))\n",
        "\n",
        "        return images, targets\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "\n",
        "class ChromosomeIdentifier:\n",
        "    \"\"\"Chromosome detection model\"\"\"\n",
        "\n",
        "    def __init__(self, img_size=800, max_detections=50):\n",
        "        self.img_size = img_size\n",
        "        self.max_detections = max_detections\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        inputs = layers.Input(shape=(self.img_size, self.img_size, 3))\n",
        "\n",
        "        base_model = ResNet50(include_top=False, weights='imagenet', input_tensor=inputs)\n",
        "\n",
        "        for layer in base_model.layers[:100]:\n",
        "            layer.trainable = False\n",
        "\n",
        "        x = base_model.output\n",
        "        x = layers.Conv2D(512, 3, padding='same', activation='relu')(x)\n",
        "        x = layers.Conv2D(512, 3, padding='same', activation='relu')(x)\n",
        "        x = layers.Conv2D(256, 3, padding='same', activation='relu')(x)\n",
        "        x = layers.GlobalAveragePooling2D()(x)\n",
        "        x = layers.Dense(1024, activation='relu')(x)\n",
        "        x = layers.Dropout(0.5)(x)\n",
        "\n",
        "        bbox_output = layers.Dense(self.max_detections * 4, name='bbox')(x)\n",
        "        objectness = layers.Dense(self.max_detections, activation='sigmoid', name='objectness')(x)\n",
        "\n",
        "        model = models.Model(inputs=inputs, outputs=[bbox_output, objectness])\n",
        "        return model\n",
        "\n",
        "    def train(self, train_gen, epochs=10):\n",
        "        optimizer = optimizers.Adam(0.001)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "            epoch_losses = []\n",
        "\n",
        "            for batch_idx in range(len(train_gen)):\n",
        "                images, targets = train_gen[batch_idx]\n",
        "\n",
        "                if len(images) == 0:\n",
        "                    continue\n",
        "\n",
        "                images = np.array(images)\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    bbox_pred, obj_pred = self.model(images, training=True)\n",
        "                    bbox_pred = tf.reshape(bbox_pred, (-1, self.max_detections, 4))\n",
        "\n",
        "                    batch_loss = 0\n",
        "                    for i, target_boxes in enumerate(targets):\n",
        "                        if len(target_boxes) > 0:\n",
        "                            num_objs = min(len(target_boxes), self.max_detections)\n",
        "\n",
        "                            target_padded = np.zeros((self.max_detections, 4))\n",
        "                            target_padded[:num_objs] = target_boxes[:num_objs]\n",
        "\n",
        "                            bbox_loss = tf.reduce_mean(tf.square(bbox_pred[i] - target_padded))\n",
        "\n",
        "                            obj_target = np.zeros(self.max_detections)\n",
        "                            obj_target[:num_objs] = 1\n",
        "                            obj_loss = tf.keras.losses.binary_crossentropy(obj_target, obj_pred[i])\n",
        "\n",
        "                            batch_loss += bbox_loss + 0.5 * tf.reduce_mean(obj_loss)\n",
        "\n",
        "                    batch_loss = batch_loss / len(targets)\n",
        "\n",
        "                grads = tape.gradient(batch_loss, self.model.trainable_variables)\n",
        "                optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "\n",
        "                epoch_losses.append(float(batch_loss))\n",
        "\n",
        "                if (batch_idx + 1) % 10 == 0:\n",
        "                    print(f\"  Batch {batch_idx+1}/{len(train_gen)}, Loss: {np.mean(epoch_losses[-10:]):.4f}\")\n",
        "\n",
        "            print(f\"Epoch Loss: {np.mean(epoch_losses):.4f}\")\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def predict_boxes(self, image, confidence_threshold=0.3):\n",
        "        orig_h, orig_w = image.shape[:2]\n",
        "        image_resized = cv2.resize(image, (self.img_size, self.img_size))\n",
        "        image_resized = image_resized.astype(np.float32) / 255.0\n",
        "        image_resized = np.expand_dims(image_resized, axis=0)\n",
        "\n",
        "        bbox_pred, obj_pred = self.model.predict(image_resized, verbose=0)\n",
        "\n",
        "        bbox_pred = bbox_pred.reshape(self.max_detections, 4)\n",
        "        obj_pred = obj_pred.reshape(self.max_detections)\n",
        "\n",
        "        keep = obj_pred > confidence_threshold\n",
        "        boxes = bbox_pred[keep]\n",
        "        scores = obj_pred[keep]\n",
        "\n",
        "        final_boxes = []\n",
        "        for box in boxes:\n",
        "            ymin, xmin, ymax, xmax = box\n",
        "            xmin = int(xmin * orig_w)\n",
        "            ymin = int(ymin * orig_h)\n",
        "            xmax = int(xmax * orig_w)\n",
        "            ymax = int(ymax * orig_h)\n",
        "\n",
        "            xmin = max(0, min(xmin, orig_w))\n",
        "            xmax = max(0, min(xmax, orig_w))\n",
        "            ymin = max(0, min(ymin, orig_h))\n",
        "            ymax = max(0, min(ymax, orig_h))\n",
        "\n",
        "            if xmax > xmin and ymax > ymin:\n",
        "                final_boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        return np.array(final_boxes), scores[:len(final_boxes)]\n",
        "\n",
        "\n",
        "# ==================== TRAIN IDENTIFIER ====================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"CELL 1: TRAINING CHROMOSOME IDENTIFIER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "DATA_ROOT = '/content/drive/MyDrive/ParadoX/inter_iit/2025_Karyogram_CV_Camp'\n",
        "SINGLE_CHR_IMAGES = os.path.join(DATA_ROOT, 'single_chromosomes_object', 'images')\n",
        "SINGLE_CHR_ANNOTATIONS = os.path.join(DATA_ROOT, 'single_chromosomes_object', 'annotations')\n",
        "\n",
        "print(\"\\nTensorFlow version:\", tf.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "train_identifier_gen = ChromosomeIdentifierDataGenerator(\n",
        "    SINGLE_CHR_IMAGES, SINGLE_CHR_ANNOTATIONS,\n",
        "    batch_size=2, img_size=800, shuffle=True\n",
        ")\n",
        "\n",
        "identifier = ChromosomeIdentifier(img_size=800, max_detections=50)\n",
        "\n",
        "print(\"\\nStarting identifier training...\")\n",
        "identifier.train(train_identifier_gen, epochs=10)\n",
        "\n",
        "identifier.model.save('identifier_model.h5')\n",
        "print(\"\\nâœ“ Identifier model saved to 'identifier_model.h5'\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "629ab51b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "629ab51b",
        "outputId": "af58ca7e-7a5a-4f83-a3b5-d27c1af96367"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "CELL 2: TRAINING CHROMOSOME CLASSIFIER\n",
            "============================================================\n",
            "\n",
            "Initializing ChromosomeClassifierDataGenerator:\n",
            "\n",
            "Building classifier dataset...\n",
            "Found 10 images to process...\n",
            "Built dataset with 18 chromosome samples\n",
            "Train samples: 15, Validation samples: 3\n",
            "\n",
            "Initializing ChromosomeClassifierDataGenerator:\n",
            "Validation samples: 3\n",
            "\n",
            "Starting classifier training...\n",
            "Epoch 1/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39s/step - accuracy: 0.0667 - loss: 3.2035\n",
            "Epoch 1: val_accuracy improved from -inf to 1.00000, saving model to best_classifier.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 44s/step - accuracy: 0.0667 - loss: 3.2035 - val_accuracy: 1.0000 - val_loss: 0.7895 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15s/step - accuracy: 0.7333 - loss: 1.3299\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 16s/step - accuracy: 0.7333 - loss: 1.3299 - val_accuracy: 1.0000 - val_loss: 0.0380 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.8000 - loss: 0.5234\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - accuracy: 0.8000 - loss: 0.5234 - val_accuracy: 1.0000 - val_loss: 9.8262e-05 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.8667 - loss: 0.1926\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - accuracy: 0.8667 - loss: 0.1926 - val_accuracy: 1.0000 - val_loss: 5.0862e-06 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.8667 - loss: 0.3965\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.8667 - loss: 0.3965 - val_accuracy: 1.0000 - val_loss: 1.5895e-07 - learning_rate: 5.0000e-04\n",
            "Epoch 6/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.9333 - loss: 0.2879\n",
            "Epoch 6: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.9333 - loss: 0.2879 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 5.0000e-04\n",
            "Epoch 7/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 1.0000 - loss: 0.0627\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\n",
            "Epoch 7: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - accuracy: 1.0000 - loss: 0.0627 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 5.0000e-04\n",
            "Epoch 8/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.9333 - loss: 0.1894\n",
            "Epoch 8: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.9333 - loss: 0.1894 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 2.5000e-04\n",
            "Epoch 8: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\n",
            "âœ“ Classifier model saved to 'best_classifier.h5'\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 2: TRAIN CLASSIFIER MODULE\n",
        "# Uses ALL data from 24_chromosomes_object folder\n",
        "# ==============================================================================\n",
        "\n",
        "class ChromosomeClassifierDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Data generator for training chromosome classifier\"\"\"\n",
        "\n",
        "    CHROMOSOME_CLASSES = {\n",
        "        '1': 0, '2': 1, '3': 2, '4': 3, '5': 4, '6': 5,\n",
        "        '7': 6, '8': 7, '9': 8, '10': 9, '11': 10, '12': 11,\n",
        "        '13': 12, '14': 13, '15': 14, '16': 15, '17': 16, '18': 17,\n",
        "        '19': 18, '20': 19, '21': 20, '22': 21, 'X': 22, 'Y': 23\n",
        "    }\n",
        "\n",
        "    def __init__(self, images_dir: str, annotations_dir: str, is_validation=False,\n",
        "                 batch_size=32, img_size=224, augment=False, validation_split=0.2,\n",
        "                 data=None, indexes=None): # Add data and indexes parameters\n",
        "        print(f\"\\nInitializing ChromosomeClassifierDataGenerator:\")\n",
        "        self.images_dir = images_dir\n",
        "        self.annotations_dir = annotations_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.augment = augment\n",
        "        self.validation_split = validation_split\n",
        "        self.is_validation = is_validation\n",
        "        self.parser = AnnotationParser()\n",
        "\n",
        "        # Initialize data storage\n",
        "        if data is None or indexes is None:\n",
        "            self.chromosome_data = []\n",
        "            self._build_dataset()\n",
        "\n",
        "            # Split dataset only for the training generator\n",
        "            if not self.is_validation:\n",
        "                total_samples = len(self.chromosome_data)\n",
        "                val_size = int(total_samples * validation_split)\n",
        "\n",
        "                # Create train/val indexes\n",
        "                np.random.seed(42)\n",
        "                all_indexes = np.arange(total_samples)\n",
        "                np.random.shuffle(all_indexes)\n",
        "\n",
        "                self.indexes = all_indexes[val_size:]\n",
        "                self.val_indexes = all_indexes[:val_size]\n",
        "\n",
        "                print(f\"Train samples: {len(self.indexes)}, Validation samples: {len(self.val_indexes)}\")\n",
        "            else:\n",
        "                 # For validation generator, use all data passed to it\n",
        "                 self.chromosome_data = data\n",
        "                 self.indexes = indexes\n",
        "                 self.val_indexes = [] # Not needed for validation generator\n",
        "                 print(f\"Validation samples: {len(self.indexes)}\")\n",
        "        else:\n",
        "            # Use provided data and indexes for validation generator\n",
        "            self.chromosome_data = data\n",
        "            self.indexes = indexes\n",
        "            self.val_indexes = [] # Not needed for validation generator\n",
        "            print(f\"Validation samples: {len(self.indexes)}\")\n",
        "\n",
        "\n",
        "        # Setup augmentation if required\n",
        "        if augment and not self.is_validation: # Apply augmentation only to training data\n",
        "            self.aug = A.Compose([\n",
        "                A.HorizontalFlip(p=0.5),\n",
        "                A.VerticalFlip(p=0.5),\n",
        "                A.Rotate(limit=15, p=0.5),\n",
        "                A.RandomBrightnessContrast(p=0.3),\n",
        "                A.GaussNoise(p=0.2),\n",
        "            ])\n",
        "        else:\n",
        "            self.aug = None\n",
        "\n",
        "\n",
        "    def _build_dataset(self):\n",
        "        \"\"\"Build dataset from images and annotations\"\"\"\n",
        "        print(\"\\nBuilding classifier dataset...\")\n",
        "\n",
        "        # Get list of valid image files\n",
        "        image_files = []\n",
        "        for filename in os.listdir(self.images_dir):\n",
        "            if filename.endswith(('.jpg', '.png')):\n",
        "                base_name = os.path.splitext(filename)[0]\n",
        "                xml_path = os.path.join(self.annotations_dir, f\"{base_name}.xml\")\n",
        "                if os.path.exists(xml_path):\n",
        "                    image_files.append(base_name)\n",
        "\n",
        "        print(f\"Found {len(image_files)} images to process...\")\n",
        "\n",
        "        # Process each image and its annotations\n",
        "        for img_name in image_files:\n",
        "            img_path = os.path.join(self.images_dir, f\"{img_name}.jpg\")\n",
        "            xml_path = os.path.join(self.annotations_dir, f\"{img_name}.xml\")\n",
        "\n",
        "            # Try PNG if JPG doesn't exist\n",
        "            if not os.path.exists(img_path):\n",
        "                img_path = os.path.join(self.images_dir, f\"{img_name}.png\")\n",
        "\n",
        "            if not os.path.exists(img_path) or not os.path.exists(xml_path):\n",
        "                print(f\"Warning: Missing files for {img_name}\")\n",
        "                continue\n",
        "\n",
        "            # Load and process image\n",
        "            image = cv2.imread(img_path)\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Parse annotations\n",
        "            annotations = self.parser.parse_xml(xml_path)\n",
        "\n",
        "            # Extract individual chromosomes\n",
        "            for obj in annotations['objects']:\n",
        "                bbox = obj['bbox']\n",
        "                label = obj['name']\n",
        "\n",
        "                if label in self.CHROMOSOME_CLASSES:\n",
        "                    chromosome_img = image[bbox['ymin']:bbox['ymax'],\n",
        "                                        bbox['xmin']:bbox['xmax']]\n",
        "\n",
        "                    if chromosome_img.size > 0:\n",
        "                        self.chromosome_data.append({\n",
        "                            'image': chromosome_img,\n",
        "                            'label': self.CHROMOSOME_CLASSES[label]\n",
        "                        })\n",
        "\n",
        "        print(f\"Built dataset with {len(self.chromosome_data)} chromosome samples\")\n",
        "\n",
        "\n",
        "    def get_validation_generator(self):\n",
        "        \"\"\"Create validation generator using validation split\"\"\"\n",
        "        # Create a new generator instance specifically for validation\n",
        "        val_gen = ChromosomeClassifierDataGenerator(\n",
        "            images_dir=self.images_dir, # Pass directories (not strictly needed but keeps signature)\n",
        "            annotations_dir=self.annotations_dir,\n",
        "            batch_size=self.batch_size,\n",
        "            img_size=self.img_size,\n",
        "            augment=False, # No augmentation for validation\n",
        "            validation_split=0.0, # Not used for validation generator\n",
        "            is_validation=True, # Mark as validation generator\n",
        "            data=[self.chromosome_data[i] for i in self.val_indexes], # Pass the already built data\n",
        "            indexes=np.arange(len(self.val_indexes)) # Pass the validation indexes\n",
        "        )\n",
        "        return val_gen\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return number of batches per epoch\"\"\"\n",
        "        return int(np.ceil(len(self.indexes) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Get batch of data\"\"\"\n",
        "        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        batch_images = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for idx in batch_indexes:\n",
        "            data = self.chromosome_data[idx]\n",
        "            image = data['image'].copy()\n",
        "            label = data['label']\n",
        "\n",
        "            # Apply augmentation if enabled\n",
        "            if self.aug is not None:\n",
        "                augmented = self.aug(image=image)\n",
        "                image = augmented['image']\n",
        "\n",
        "            # Preprocess image\n",
        "            image = cv2.resize(image, (self.img_size, self.img_size))\n",
        "            image = image.astype(np.float32) / 255.0\n",
        "\n",
        "            # Normalize using ImageNet stats\n",
        "            mean = np.array([0.485, 0.456, 0.406])\n",
        "            std = np.array([0.229, 0.224, 0.225])\n",
        "            image = (image - mean) / std\n",
        "\n",
        "            batch_images.append(image)\n",
        "            batch_labels.append(label)\n",
        "\n",
        "        return np.array(batch_images), np.array(batch_labels)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Called at the end of every epoch\"\"\"\n",
        "        if not self.is_validation: # Only shuffle training data\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "\n",
        "class ChromosomeClassifier:\n",
        "    \"\"\"Chromosome classification model (24 classes)\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=24, img_size=224):\n",
        "        self.num_classes = num_classes\n",
        "        self.img_size = img_size\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        base_model = ResNet50(include_top=False, weights='imagenet',\n",
        "                             input_shape=(self.img_size, self.img_size, 3))\n",
        "\n",
        "        for layer in base_model.layers[:140]:\n",
        "            layer.trainable = False\n",
        "\n",
        "        x = base_model.output\n",
        "        x = layers.GlobalAveragePooling2D()(x)\n",
        "        x = layers.Dense(512, activation='relu')(x)\n",
        "        x = layers.Dropout(0.5)(x)\n",
        "        x = layers.Dense(256, activation='relu')(x)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
        "\n",
        "        model = models.Model(inputs=base_model.input, outputs=outputs)\n",
        "        return model\n",
        "\n",
        "    def compile_model(self, lr=0.001):\n",
        "        self.model.compile(\n",
        "            optimizer=optimizers.Adam(lr),\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "    def train(self, train_gen, val_gen, epochs=30):\n",
        "        callbacks = [\n",
        "            keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_accuracy', factor=0.5, patience=3,\n",
        "                min_lr=1e-7, verbose=1\n",
        "            ),\n",
        "            keras.callbacks.ModelCheckpoint(\n",
        "                'best_classifier.h5', monitor='val_accuracy',\n",
        "                save_best_only=True, verbose=1\n",
        "            ),\n",
        "            keras.callbacks.EarlyStopping(\n",
        "                monitor='val_accuracy', patience=7,\n",
        "                restore_best_weights=True, verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        history = self.model.fit(\n",
        "            train_gen,\n",
        "            validation_data=val_gen,\n",
        "            epochs=epochs,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        return history\n",
        "\n",
        "\n",
        "# ==================== TRAIN CLASSIFIER ====================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CELL 2: TRAINING CHROMOSOME CLASSIFIER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "DATA_ROOT = '/content/drive/MyDrive/ParadoX/inter_iit/2025_Karyogram_CV_Camp'\n",
        "MULTI_CHR_IMAGES = os.path.join(DATA_ROOT, '24_chromosomes_object', 'images')\n",
        "MULTI_CHR_ANNOTATIONS = os.path.join(DATA_ROOT, '24_chromosomes_object', 'annotations')\n",
        "\n",
        "train_classifier_gen = ChromosomeClassifierDataGenerator(\n",
        "    MULTI_CHR_IMAGES, MULTI_CHR_ANNOTATIONS,\n",
        "    batch_size=32, img_size=224, augment=True, validation_split=0.2\n",
        ")\n",
        "\n",
        "val_classifier_gen = train_classifier_gen.get_validation_generator()\n",
        "\n",
        "classifier = ChromosomeClassifier(num_classes=24)\n",
        "classifier.compile_model(lr=0.001)\n",
        "\n",
        "print(\"\\nStarting classifier training...\")\n",
        "history = classifier.train(train_classifier_gen, val_classifier_gen, epochs=30)\n",
        "\n",
        "print(\"\\nâœ“ Classifier model saved to 'best_classifier.h5'\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8148cd6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "8148cd6d",
        "outputId": "b9c76346-f6dd-4f8f-a56e-45fa7e059c3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "CELL 3: INTEGRATION AND EVALUATION\n",
            "============================================================\n",
            "\n",
            "Loading train/test splits...\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/ParadoX/inter_iit/2025_Karyogram_CV_Camp/train.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3271622843.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;31m# Load train/test splits from txt files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nLoading train/test splits...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_ROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0mtrain_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/ParadoX/inter_iit/2025_Karyogram_CV_Camp/train.txt'"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 3: INTEGRATION AND EVALUATION\n",
        "# Uses train.txt and test.txt to select specific images from 24_chromosomes_object\n",
        "# ==============================================================================\n",
        "\n",
        "class KaryotypePipeline:\n",
        "    \"\"\"End-to-end pipeline integrating identifier and classifier\"\"\"\n",
        "\n",
        "    def __init__(self, identifier: ChromosomeIdentifier,\n",
        "                 classifier: ChromosomeClassifier):\n",
        "        self.identifier = identifier\n",
        "        self.classifier = classifier\n",
        "        self.img_size = 224\n",
        "\n",
        "    def process_image(self, image_path: str, confidence_threshold=0.3):\n",
        "        image = cv2.imread(image_path)\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        boxes, scores = self.identifier.predict_boxes(image_rgb, confidence_threshold=confidence_threshold)\n",
        "\n",
        "        if len(boxes) == 0:\n",
        "            return []\n",
        "\n",
        "        predictions = []\n",
        "\n",
        "        for i, box in enumerate(boxes):\n",
        "            xmin, ymin, xmax, ymax = map(int, box)\n",
        "\n",
        "            chromosome_img = image_rgb[ymin:ymax, xmin:xmax]\n",
        "\n",
        "            if chromosome_img.size == 0:\n",
        "                continue\n",
        "\n",
        "            chromosome_img = cv2.resize(chromosome_img, (self.img_size, self.img_size))\n",
        "            chromosome_img = chromosome_img.astype(np.float32) / 255.0\n",
        "\n",
        "            mean = np.array([0.485, 0.456, 0.406])\n",
        "            std = np.array([0.229, 0.224, 0.225])\n",
        "            chromosome_img = (chromosome_img - mean) / std\n",
        "\n",
        "            chromosome_img = np.expand_dims(chromosome_img, axis=0)\n",
        "\n",
        "            probs = self.classifier.model.predict(chromosome_img, verbose=0)[0]\n",
        "            class_id = np.argmax(probs)\n",
        "            confidence = probs[class_id]\n",
        "\n",
        "            predictions.append({\n",
        "                'bbox': box,\n",
        "                'class_id': int(class_id),\n",
        "                'class_name': self._get_class_name(class_id),\n",
        "                'confidence': float(confidence),\n",
        "                'score': float(scores[i]) if i < len(scores) else 1.0\n",
        "            })\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_class_name(class_id: int) -> str:\n",
        "        if class_id < 22:\n",
        "            return str(class_id + 1)\n",
        "        elif class_id == 22:\n",
        "            return 'X'\n",
        "        else:\n",
        "            return 'Y'\n",
        "\n",
        "\n",
        "def evaluate_pipeline(pipeline: KaryotypePipeline, images_dir: str,\n",
        "                     annotations_dir: str, test_list: List[str],\n",
        "                     output_dir='results'):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    parser = AnnotationParser()\n",
        "\n",
        "    all_true_labels = []\n",
        "    all_pred_probs = []\n",
        "\n",
        "    print(\"Evaluating pipeline on test set...\")\n",
        "    for idx, img_name in enumerate(test_list):\n",
        "        img_name = img_name.strip()\n",
        "        img_path = os.path.join(images_dir, f\"{img_name}.jpg\")\n",
        "        xml_path = os.path.join(annotations_dir, f\"{img_name}.xml\")\n",
        "\n",
        "        if not os.path.exists(img_path):\n",
        "            img_path = os.path.join(images_dir, f\"{img_name}.png\")\n",
        "\n",
        "        if not os.path.exists(img_path) or not os.path.exists(xml_path):\n",
        "            continue\n",
        "\n",
        "        if (idx + 1) % 10 == 0:\n",
        "            print(f\"Processed {idx+1}/{len(test_list)} images...\")\n",
        "\n",
        "        annotations = parser.parse_xml(xml_path)\n",
        "        gt_chromosomes = [obj['name'] for obj in annotations['objects']]\n",
        "\n",
        "        predictions = pipeline.process_image(img_path)\n",
        "\n",
        "        for gt_chr in gt_chromosomes:\n",
        "            if gt_chr in ChromosomeClassifierDataGenerator.CHROMOSOME_CLASSES:\n",
        "                gt_class = ChromosomeClassifierDataGenerator.CHROMOSOME_CLASSES[gt_chr]\n",
        "\n",
        "                true_labels = np.zeros(24)\n",
        "                true_labels[gt_class] = 1\n",
        "\n",
        "                pred_probs = np.zeros(24)\n",
        "                for pred in predictions:\n",
        "                    pred_probs[pred['class_id']] = max(\n",
        "                        pred_probs[pred['class_id']],\n",
        "                        pred['confidence']\n",
        "                    )\n",
        "\n",
        "                all_true_labels.append(true_labels)\n",
        "                all_pred_probs.append(pred_probs)\n",
        "\n",
        "    all_true_labels = np.array(all_true_labels)\n",
        "    all_pred_probs = np.array(all_pred_probs)\n",
        "\n",
        "    auprc_scores = []\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    for i in range(24):\n",
        "        if np.sum(all_true_labels[:, i]) > 0:\n",
        "            precision, recall, _ = precision_recall_curve(\n",
        "                all_true_labels[:, i],\n",
        "                all_pred_probs[:, i]\n",
        "            )\n",
        "            auprc = auc(recall, precision)\n",
        "            auprc_scores.append(auprc)\n",
        "\n",
        "            plt.plot(recall, precision,\n",
        "                    label=f'Class {pipeline._get_class_name(i)} (auPRC={auprc:.3f})')\n",
        "\n",
        "    plt.xlabel('Recall', fontsize=12)\n",
        "    plt.ylabel('Precision', fontsize=12)\n",
        "    plt.title('Precision-Recall Curves for All Chromosome Classes', fontsize=14)\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'auprc_curves.png'),\n",
        "                dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    mean_auprc = np.mean(auprc_scores)\n",
        "    print(f\"\\nMean auPRC: {mean_auprc:.4f}\")\n",
        "\n",
        "    with open(os.path.join(output_dir, 'auprc_scores.txt'), 'w') as f:\n",
        "        f.write(f\"Mean auPRC: {mean_auprc:.4f}\\n\\n\")\n",
        "        f.write(\"Per-class auPRC scores:\\n\")\n",
        "        for i, score in enumerate(auprc_scores):\n",
        "            f.write(f\"Class {pipeline._get_class_name(i)}: {score:.4f}\\n\")\n",
        "\n",
        "    return mean_auprc, auprc_scores\n",
        "\n",
        "\n",
        "# ==================== INTEGRATION AND EVALUATION ====================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CELL 3: INTEGRATION AND EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load train/test splits from txt files\n",
        "print(\"\\nLoading train/test splits...\")\n",
        "with open(os.path.join(DATA_ROOT, 'train.txt'), 'r') as f:\n",
        "    train_list = f.readlines()\n",
        "\n",
        "with open(os.path.join(DATA_ROOT, 'test.txt'), 'r') as f:\n",
        "    test_list = f.readlines()\n",
        "\n",
        "print(f\"Train samples from train.txt: {len(train_list)}\")\n",
        "print(f\"Test samples from test.txt: {len(test_list)}\")\n",
        "\n",
        "# Load trained models\n",
        "print(\"\\nLoading trained models...\")\n",
        "identifier_loaded = ChromosomeIdentifier(img_size=800, max_detections=50)\n",
        "identifier_loaded.model = keras.models.load_model('identifier_model.h5')\n",
        "print(\"âœ“ Identifier model loaded\")\n",
        "\n",
        "classifier_loaded = ChromosomeClassifier(num_classes=24)\n",
        "classifier_loaded.model = keras.models.load_model('best_classifier.h5')\n",
        "print(\"âœ“ Classifier model loaded\")\n",
        "\n",
        "# Create pipeline\n",
        "pipeline = KaryotypePipeline(identifier_loaded, classifier_loaded)\n",
        "print(\"\\nâœ“ Pipeline created successfully\")\n",
        "\n",
        "# Evaluate on test set (using images specified in test.txt)\n",
        "print(\"\\nEvaluating pipeline on test set from test.txt...\")\n",
        "mean_auprc, class_auprcs = evaluate_pipeline(\n",
        "    pipeline, MULTI_CHR_IMAGES, MULTI_CHR_ANNOTATIONS,\n",
        "    test_list, output_dir='results'\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Mean auPRC: {mean_auprc:.4f}\")\n",
        "print(f\"\\nResults saved to 'results/' directory:\")\n",
        "print(f\"  - auPRC curve: results/auprc_curves.png\")\n",
        "print(f\"  - Detailed scores: results/auprc_scores.txt\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aiml_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
