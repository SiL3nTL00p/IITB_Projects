{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXbfw4ci5zCN",
        "outputId": "27d84237-a9ca-4cb7-dab2-bdde9f9724f0"
      },
      "id": "aXbfw4ci5zCN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d0f9667",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d0f9667",
        "outputId": "33362c43-6e4f-4bba-8655-6bc210252181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "CELL 1: TRAINING CHROMOSOME IDENTIFIER\n",
            "============================================================\n",
            "\n",
            "TensorFlow version: 2.19.0\n",
            "GPU Available: []\n",
            "Found 3 images for identifier training\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "\n",
            "Starting identifier training...\n",
            "\n",
            "Epoch 1/10\n",
            "Epoch Loss: 1.4368\n",
            "\n",
            "Epoch 2/10\n",
            "Epoch Loss: 0.5887\n",
            "\n",
            "Epoch 3/10\n",
            "Epoch Loss: 0.5151\n",
            "\n",
            "Epoch 4/10\n",
            "Epoch Loss: 0.3998\n",
            "\n",
            "Epoch 5/10\n",
            "Epoch Loss: 0.3864\n",
            "\n",
            "Epoch 6/10\n",
            "Epoch Loss: 0.3188\n",
            "\n",
            "Epoch 7/10\n",
            "Epoch Loss: 0.2629\n",
            "\n",
            "Epoch 8/10\n",
            "Epoch Loss: 0.2358\n",
            "\n",
            "Epoch 9/10\n",
            "Epoch Loss: 0.2139\n",
            "\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch Loss: 0.2037\n",
            "\n",
            "✓ Identifier model saved to 'identifier_model.h5'\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 1: TRAIN IDENTIFIER MODULE\n",
        "# Uses ALL data from single_chromosomes_object folder\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Dict\n",
        "import albumentations as A\n",
        "\n",
        "# ==================== SHARED UTILITIES ====================\n",
        "\n",
        "class AnnotationParser:\n",
        "    \"\"\"Parse XML annotations for both single and 24-chromosome datasets\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_xml(xml_path: str) -> Dict:\n",
        "        tree = ET.parse(xml_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        annotations = {\n",
        "            'filename': root.find('filename').text if root.find('filename') is not None else '',\n",
        "            'size': {},\n",
        "            'objects': []\n",
        "        }\n",
        "\n",
        "        size = root.find('size')\n",
        "        if size is not None:\n",
        "            annotations['size'] = {\n",
        "                'width': int(size.find('width').text),\n",
        "                'height': int(size.find('height').text),\n",
        "                'depth': int(size.find('depth').text) if size.find('depth') is not None else 3\n",
        "            }\n",
        "\n",
        "        for obj in root.findall('object'):\n",
        "            name = obj.find('name').text\n",
        "            bndbox = obj.find('bndbox')\n",
        "\n",
        "            if bndbox is not None:\n",
        "                bbox = {\n",
        "                    'xmin': int(float(bndbox.find('xmin').text)),\n",
        "                    'ymin': int(float(bndbox.find('ymin').text)),\n",
        "                    'xmax': int(float(bndbox.find('xmax').text)),\n",
        "                    'ymax': int(float(bndbox.find('ymax').text))\n",
        "                }\n",
        "\n",
        "                annotations['objects'].append({\n",
        "                    'name': name,\n",
        "                    'bbox': bbox\n",
        "                })\n",
        "\n",
        "        return annotations\n",
        "\n",
        "\n",
        "# ==================== IDENTIFIER MODULE ====================\n",
        "\n",
        "class ChromosomeIdentifierDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Data generator for training chromosome identifier\"\"\"\n",
        "\n",
        "    def __init__(self, images_dir: str, annotations_dir: str,\n",
        "                 batch_size=2, img_size=800, shuffle=True):\n",
        "        self.images_dir = images_dir\n",
        "        self.annotations_dir = annotations_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.shuffle = shuffle\n",
        "        self.parser = AnnotationParser()\n",
        "\n",
        "        # Get all image files from directory\n",
        "        self.image_list = []\n",
        "        for filename in os.listdir(images_dir):\n",
        "            if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "                base_name = os.path.splitext(filename)[0]\n",
        "                xml_path = os.path.join(annotations_dir, f\"{base_name}.xml\")\n",
        "                if os.path.exists(xml_path):\n",
        "                    self.image_list.append(base_name)\n",
        "\n",
        "        print(f\"Found {len(self.image_list)} images for identifier training\")\n",
        "\n",
        "        self.indexes = np.arange(len(self.image_list))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.image_list) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        batch_images = [self.image_list[k] for k in batch_indexes]\n",
        "\n",
        "        X, y = self._generate_data(batch_images)\n",
        "        return X, y\n",
        "\n",
        "    def _generate_data(self, batch_images):\n",
        "        images = []\n",
        "        targets = []\n",
        "\n",
        "        for img_name in batch_images:\n",
        "            img_path = os.path.join(self.images_dir, f\"{img_name}.jpg\")\n",
        "            xml_path = os.path.join(self.annotations_dir, f\"{img_name}.xml\")\n",
        "\n",
        "            if not os.path.exists(img_path):\n",
        "                img_path = os.path.join(self.images_dir, f\"{img_name}.png\")\n",
        "\n",
        "            if not os.path.exists(img_path) or not os.path.exists(xml_path):\n",
        "                continue\n",
        "\n",
        "            image = cv2.imread(img_path)\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            orig_h, orig_w = image.shape[:2]\n",
        "\n",
        "            image = cv2.resize(image, (self.img_size, self.img_size))\n",
        "            image = image.astype(np.float32) / 255.0\n",
        "\n",
        "            annotations = self.parser.parse_xml(xml_path)\n",
        "\n",
        "            boxes = []\n",
        "            for obj in annotations['objects']:\n",
        "                bbox = obj['bbox']\n",
        "                xmin = bbox['xmin'] / orig_w\n",
        "                ymin = bbox['ymin'] / orig_h\n",
        "                xmax = bbox['xmax'] / orig_w\n",
        "                ymax = bbox['ymax'] / orig_h\n",
        "                boxes.append([ymin, xmin, ymax, xmax])\n",
        "\n",
        "            images.append(image)\n",
        "            targets.append(np.array(boxes) if len(boxes) > 0 else np.zeros((0, 4)))\n",
        "\n",
        "        return images, targets\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "\n",
        "class ChromosomeIdentifier:\n",
        "    \"\"\"Chromosome detection model\"\"\"\n",
        "\n",
        "    def __init__(self, img_size=800, max_detections=50):\n",
        "        self.img_size = img_size\n",
        "        self.max_detections = max_detections\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        inputs = layers.Input(shape=(self.img_size, self.img_size, 3))\n",
        "\n",
        "        base_model = ResNet50(include_top=False, weights='imagenet', input_tensor=inputs)\n",
        "\n",
        "        for layer in base_model.layers[:100]:\n",
        "            layer.trainable = False\n",
        "\n",
        "        x = base_model.output\n",
        "        x = layers.Conv2D(512, 3, padding='same', activation='relu')(x)\n",
        "        x = layers.Conv2D(512, 3, padding='same', activation='relu')(x)\n",
        "        x = layers.Conv2D(256, 3, padding='same', activation='relu')(x)\n",
        "        x = layers.GlobalAveragePooling2D()(x)\n",
        "        x = layers.Dense(1024, activation='relu')(x)\n",
        "        x = layers.Dropout(0.5)(x)\n",
        "\n",
        "        bbox_output = layers.Dense(self.max_detections * 4, name='bbox')(x)\n",
        "        objectness = layers.Dense(self.max_detections, activation='sigmoid', name='objectness')(x)\n",
        "\n",
        "        model = models.Model(inputs=inputs, outputs=[bbox_output, objectness])\n",
        "        return model\n",
        "\n",
        "    def train(self, train_gen, epochs=10):\n",
        "        optimizer = optimizers.Adam(0.001)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "            epoch_losses = []\n",
        "\n",
        "            for batch_idx in range(len(train_gen)):\n",
        "                images, targets = train_gen[batch_idx]\n",
        "\n",
        "                if len(images) == 0:\n",
        "                    continue\n",
        "\n",
        "                images = np.array(images)\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    bbox_pred, obj_pred = self.model(images, training=True)\n",
        "                    bbox_pred = tf.reshape(bbox_pred, (-1, self.max_detections, 4))\n",
        "\n",
        "                    batch_loss = 0\n",
        "                    for i, target_boxes in enumerate(targets):\n",
        "                        if len(target_boxes) > 0:\n",
        "                            num_objs = min(len(target_boxes), self.max_detections)\n",
        "\n",
        "                            target_padded = np.zeros((self.max_detections, 4))\n",
        "                            target_padded[:num_objs] = target_boxes[:num_objs]\n",
        "\n",
        "                            bbox_loss = tf.reduce_mean(tf.square(bbox_pred[i] - target_padded))\n",
        "\n",
        "                            obj_target = np.zeros(self.max_detections)\n",
        "                            obj_target[:num_objs] = 1\n",
        "                            obj_loss = tf.keras.losses.binary_crossentropy(obj_target, obj_pred[i])\n",
        "\n",
        "                            batch_loss += bbox_loss + 0.5 * tf.reduce_mean(obj_loss)\n",
        "\n",
        "                    batch_loss = batch_loss / len(targets)\n",
        "\n",
        "                grads = tape.gradient(batch_loss, self.model.trainable_variables)\n",
        "                optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "\n",
        "                epoch_losses.append(float(batch_loss))\n",
        "\n",
        "                if (batch_idx + 1) % 10 == 0:\n",
        "                    print(f\"  Batch {batch_idx+1}/{len(train_gen)}, Loss: {np.mean(epoch_losses[-10:]):.4f}\")\n",
        "\n",
        "            print(f\"Epoch Loss: {np.mean(epoch_losses):.4f}\")\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def predict_boxes(self, image, confidence_threshold=0.3):\n",
        "        orig_h, orig_w = image.shape[:2]\n",
        "        image_resized = cv2.resize(image, (self.img_size, self.img_size))\n",
        "        image_resized = image_resized.astype(np.float32) / 255.0\n",
        "        image_resized = np.expand_dims(image_resized, axis=0)\n",
        "\n",
        "        bbox_pred, obj_pred = self.model.predict(image_resized, verbose=0)\n",
        "\n",
        "        bbox_pred = bbox_pred.reshape(self.max_detections, 4)\n",
        "        obj_pred = obj_pred.reshape(self.max_detections)\n",
        "\n",
        "        keep = obj_pred > confidence_threshold\n",
        "        boxes = bbox_pred[keep]\n",
        "        scores = obj_pred[keep]\n",
        "\n",
        "        final_boxes = []\n",
        "        for box in boxes:\n",
        "            ymin, xmin, ymax, xmax = box\n",
        "            xmin = int(xmin * orig_w)\n",
        "            ymin = int(ymin * orig_h)\n",
        "            xmax = int(xmax * orig_w)\n",
        "            ymax = int(ymax * orig_h)\n",
        "\n",
        "            xmin = max(0, min(xmin, orig_w))\n",
        "            xmax = max(0, min(xmax, orig_w))\n",
        "            ymin = max(0, min(ymin, orig_h))\n",
        "            ymax = max(0, min(ymax, orig_h))\n",
        "\n",
        "            if xmax > xmin and ymax > ymin:\n",
        "                final_boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        return np.array(final_boxes), scores[:len(final_boxes)]\n",
        "\n",
        "\n",
        "# ==================== TRAIN IDENTIFIER ====================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"CELL 1: TRAINING CHROMOSOME IDENTIFIER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "DATA_ROOT = '/content/drive/MyDrive/ParadoX/inter_iit/2025_Karyogram_CV_Camp'\n",
        "SINGLE_CHR_IMAGES = os.path.join(DATA_ROOT, 'single_chromosomes_object', 'images')\n",
        "SINGLE_CHR_ANNOTATIONS = os.path.join(DATA_ROOT, 'single_chromosomes_object', 'annotations')\n",
        "\n",
        "print(\"\\nTensorFlow version:\", tf.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "train_identifier_gen = ChromosomeIdentifierDataGenerator(\n",
        "    SINGLE_CHR_IMAGES, SINGLE_CHR_ANNOTATIONS,\n",
        "    batch_size=2, img_size=800, shuffle=True\n",
        ")\n",
        "\n",
        "identifier = ChromosomeIdentifier(img_size=800, max_detections=50)\n",
        "\n",
        "print(\"\\nStarting identifier training...\")\n",
        "identifier.train(train_identifier_gen, epochs=10)\n",
        "\n",
        "identifier.model.save('identifier_model.h5')\n",
        "print(\"\\n✓ Identifier model saved to 'identifier_model.h5'\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "629ab51b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "629ab51b",
        "outputId": "af58ca7e-7a5a-4f83-a3b5-d27c1af96367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "CELL 2: TRAINING CHROMOSOME CLASSIFIER\n",
            "============================================================\n",
            "\n",
            "Initializing ChromosomeClassifierDataGenerator:\n",
            "\n",
            "Building classifier dataset...\n",
            "Found 10 images to process...\n",
            "Built dataset with 18 chromosome samples\n",
            "Train samples: 15, Validation samples: 3\n",
            "\n",
            "Initializing ChromosomeClassifierDataGenerator:\n",
            "Validation samples: 3\n",
            "\n",
            "Starting classifier training...\n",
            "Epoch 1/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39s/step - accuracy: 0.0667 - loss: 3.2035\n",
            "Epoch 1: val_accuracy improved from -inf to 1.00000, saving model to best_classifier.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 44s/step - accuracy: 0.0667 - loss: 3.2035 - val_accuracy: 1.0000 - val_loss: 0.7895 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15s/step - accuracy: 0.7333 - loss: 1.3299\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 16s/step - accuracy: 0.7333 - loss: 1.3299 - val_accuracy: 1.0000 - val_loss: 0.0380 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.8000 - loss: 0.5234\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - accuracy: 0.8000 - loss: 0.5234 - val_accuracy: 1.0000 - val_loss: 9.8262e-05 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.8667 - loss: 0.1926\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - accuracy: 0.8667 - loss: 0.1926 - val_accuracy: 1.0000 - val_loss: 5.0862e-06 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.8667 - loss: 0.3965\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.8667 - loss: 0.3965 - val_accuracy: 1.0000 - val_loss: 1.5895e-07 - learning_rate: 5.0000e-04\n",
            "Epoch 6/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.9333 - loss: 0.2879\n",
            "Epoch 6: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.9333 - loss: 0.2879 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 5.0000e-04\n",
            "Epoch 7/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 1.0000 - loss: 0.0627\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\n",
            "Epoch 7: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - accuracy: 1.0000 - loss: 0.0627 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 5.0000e-04\n",
            "Epoch 8/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.9333 - loss: 0.1894\n",
            "Epoch 8: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.9333 - loss: 0.1894 - val_accuracy: 1.0000 - val_loss: 0.0000e+00 - learning_rate: 2.5000e-04\n",
            "Epoch 8: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\n",
            "✓ Classifier model saved to 'best_classifier.h5'\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 2: TRAIN CLASSIFIER MODULE\n",
        "# Uses ALL data from 24_chromosomes_object folder\n",
        "# ==============================================================================\n",
        "\n",
        "class ChromosomeClassifierDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Data generator for training chromosome classifier\"\"\"\n",
        "\n",
        "    CHROMOSOME_CLASSES = {\n",
        "        '1': 0, '2': 1, '3': 2, '4': 3, '5': 4, '6': 5,\n",
        "        '7': 6, '8': 7, '9': 8, '10': 9, '11': 10, '12': 11,\n",
        "        '13': 12, '14': 13, '15': 14, '16': 15, '17': 16, '18': 17,\n",
        "        '19': 18, '20': 19, '21': 20, '22': 21, 'X': 22, 'Y': 23\n",
        "    }\n",
        "\n",
        "    def __init__(self, images_dir: str, annotations_dir: str, is_validation=False,\n",
        "                 batch_size=32, img_size=224, augment=False, validation_split=0.2,\n",
        "                 data=None, indexes=None): # Add data and indexes parameters\n",
        "        print(f\"\\nInitializing ChromosomeClassifierDataGenerator:\")\n",
        "        self.images_dir = images_dir\n",
        "        self.annotations_dir = annotations_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.augment = augment\n",
        "        self.validation_split = validation_split\n",
        "        self.is_validation = is_validation\n",
        "        self.parser = AnnotationParser()\n",
        "\n",
        "        # Initialize data storage\n",
        "        if data is None or indexes is None:\n",
        "            self.chromosome_data = []\n",
        "            self._build_dataset()\n",
        "\n",
        "            # Split dataset only for the training generator\n",
        "            if not self.is_validation:\n",
        "                total_samples = len(self.chromosome_data)\n",
        "                val_size = int(total_samples * validation_split)\n",
        "\n",
        "                # Create train/val indexes\n",
        "                np.random.seed(42)\n",
        "                all_indexes = np.arange(total_samples)\n",
        "                np.random.shuffle(all_indexes)\n",
        "\n",
        "                self.indexes = all_indexes[val_size:]\n",
        "                self.val_indexes = all_indexes[:val_size]\n",
        "\n",
        "                print(f\"Train samples: {len(self.indexes)}, Validation samples: {len(self.val_indexes)}\")\n",
        "            else:\n",
        "                 # For validation generator, use all data passed to it\n",
        "                 self.chromosome_data = data\n",
        "                 self.indexes = indexes\n",
        "                 self.val_indexes = [] # Not needed for validation generator\n",
        "                 print(f\"Validation samples: {len(self.indexes)}\")\n",
        "        else:\n",
        "            # Use provided data and indexes for validation generator\n",
        "            self.chromosome_data = data\n",
        "            self.indexes = indexes\n",
        "            self.val_indexes = [] # Not needed for validation generator\n",
        "            print(f\"Validation samples: {len(self.indexes)}\")\n",
        "\n",
        "\n",
        "        # Setup augmentation if required\n",
        "        if augment and not self.is_validation: # Apply augmentation only to training data\n",
        "            self.aug = A.Compose([\n",
        "                A.HorizontalFlip(p=0.5),\n",
        "                A.VerticalFlip(p=0.5),\n",
        "                A.Rotate(limit=15, p=0.5),\n",
        "                A.RandomBrightnessContrast(p=0.3),\n",
        "                A.GaussNoise(p=0.2),\n",
        "            ])\n",
        "        else:\n",
        "            self.aug = None\n",
        "\n",
        "\n",
        "    def _build_dataset(self):\n",
        "        \"\"\"Build dataset from images and annotations\"\"\"\n",
        "        print(\"\\nBuilding classifier dataset...\")\n",
        "\n",
        "        # Get list of valid image files\n",
        "        image_files = []\n",
        "        for filename in os.listdir(self.images_dir):\n",
        "            if filename.endswith(('.jpg', '.png')):\n",
        "                base_name = os.path.splitext(filename)[0]\n",
        "                xml_path = os.path.join(self.annotations_dir, f\"{base_name}.xml\")\n",
        "                if os.path.exists(xml_path):\n",
        "                    image_files.append(base_name)\n",
        "\n",
        "        print(f\"Found {len(image_files)} images to process...\")\n",
        "\n",
        "        # Process each image and its annotations\n",
        "        for img_name in image_files:\n",
        "            img_path = os.path.join(self.images_dir, f\"{img_name}.jpg\")\n",
        "            xml_path = os.path.join(self.annotations_dir, f\"{img_name}.xml\")\n",
        "\n",
        "            # Try PNG if JPG doesn't exist\n",
        "            if not os.path.exists(img_path):\n",
        "                img_path = os.path.join(self.images_dir, f\"{img_name}.png\")\n",
        "\n",
        "            if not os.path.exists(img_path) or not os.path.exists(xml_path):\n",
        "                print(f\"Warning: Missing files for {img_name}\")\n",
        "                continue\n",
        "\n",
        "            # Load and process image\n",
        "            image = cv2.imread(img_path)\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Parse annotations\n",
        "            annotations = self.parser.parse_xml(xml_path)\n",
        "\n",
        "            # Extract individual chromosomes\n",
        "            for obj in annotations['objects']:\n",
        "                bbox = obj['bbox']\n",
        "                label = obj['name']\n",
        "\n",
        "                if label in self.CHROMOSOME_CLASSES:\n",
        "                    chromosome_img = image[bbox['ymin']:bbox['ymax'],\n",
        "                                        bbox['xmin']:bbox['xmax']]\n",
        "\n",
        "                    if chromosome_img.size > 0:\n",
        "                        self.chromosome_data.append({\n",
        "                            'image': chromosome_img,\n",
        "                            'label': self.CHROMOSOME_CLASSES[label]\n",
        "                        })\n",
        "\n",
        "        print(f\"Built dataset with {len(self.chromosome_data)} chromosome samples\")\n",
        "\n",
        "\n",
        "    def get_validation_generator(self):\n",
        "        \"\"\"Create validation generator using validation split\"\"\"\n",
        "        # Create a new generator instance specifically for validation\n",
        "        val_gen = ChromosomeClassifierDataGenerator(\n",
        "            images_dir=self.images_dir, # Pass directories (not strictly needed but keeps signature)\n",
        "            annotations_dir=self.annotations_dir,\n",
        "            batch_size=self.batch_size,\n",
        "            img_size=self.img_size,\n",
        "            augment=False, # No augmentation for validation\n",
        "            validation_split=0.0, # Not used for validation generator\n",
        "            is_validation=True, # Mark as validation generator\n",
        "            data=[self.chromosome_data[i] for i in self.val_indexes], # Pass the already built data\n",
        "            indexes=np.arange(len(self.val_indexes)) # Pass the validation indexes\n",
        "        )\n",
        "        return val_gen\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return number of batches per epoch\"\"\"\n",
        "        return int(np.ceil(len(self.indexes) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Get batch of data\"\"\"\n",
        "        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        batch_images = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for idx in batch_indexes:\n",
        "            data = self.chromosome_data[idx]\n",
        "            image = data['image'].copy()\n",
        "            label = data['label']\n",
        "\n",
        "            # Apply augmentation if enabled\n",
        "            if self.aug is not None:\n",
        "                augmented = self.aug(image=image)\n",
        "                image = augmented['image']\n",
        "\n",
        "            # Preprocess image\n",
        "            image = cv2.resize(image, (self.img_size, self.img_size))\n",
        "            image = image.astype(np.float32) / 255.0\n",
        "\n",
        "            # Normalize using ImageNet stats\n",
        "            mean = np.array([0.485, 0.456, 0.406])\n",
        "            std = np.array([0.229, 0.224, 0.225])\n",
        "            image = (image - mean) / std\n",
        "\n",
        "            batch_images.append(image)\n",
        "            batch_labels.append(label)\n",
        "\n",
        "        return np.array(batch_images), np.array(batch_labels)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Called at the end of every epoch\"\"\"\n",
        "        if not self.is_validation: # Only shuffle training data\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "\n",
        "class ChromosomeClassifier:\n",
        "    \"\"\"Chromosome classification model (24 classes)\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=24, img_size=224):\n",
        "        self.num_classes = num_classes\n",
        "        self.img_size = img_size\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        base_model = ResNet50(include_top=False, weights='imagenet',\n",
        "                             input_shape=(self.img_size, self.img_size, 3))\n",
        "\n",
        "        for layer in base_model.layers[:140]:\n",
        "            layer.trainable = False\n",
        "\n",
        "        x = base_model.output\n",
        "        x = layers.GlobalAveragePooling2D()(x)\n",
        "        x = layers.Dense(512, activation='relu')(x)\n",
        "        x = layers.Dropout(0.5)(x)\n",
        "        x = layers.Dense(256, activation='relu')(x)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
        "\n",
        "        model = models.Model(inputs=base_model.input, outputs=outputs)\n",
        "        return model\n",
        "\n",
        "    def compile_model(self, lr=0.001):\n",
        "        self.model.compile(\n",
        "            optimizer=optimizers.Adam(lr),\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "    def train(self, train_gen, val_gen, epochs=30):\n",
        "        callbacks = [\n",
        "            keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_accuracy', factor=0.5, patience=3,\n",
        "                min_lr=1e-7, verbose=1\n",
        "            ),\n",
        "            keras.callbacks.ModelCheckpoint(\n",
        "                'best_classifier.h5', monitor='val_accuracy',\n",
        "                save_best_only=True, verbose=1\n",
        "            ),\n",
        "            keras.callbacks.EarlyStopping(\n",
        "                monitor='val_accuracy', patience=7,\n",
        "                restore_best_weights=True, verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        history = self.model.fit(\n",
        "            train_gen,\n",
        "            validation_data=val_gen,\n",
        "            epochs=epochs,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        return history\n",
        "\n",
        "\n",
        "# ==================== TRAIN CLASSIFIER ====================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CELL 2: TRAINING CHROMOSOME CLASSIFIER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "DATA_ROOT = '/content/drive/MyDrive/ParadoX/inter_iit/2025_Karyogram_CV_Camp'\n",
        "MULTI_CHR_IMAGES = os.path.join(DATA_ROOT, '24_chromosomes_object', 'images')\n",
        "MULTI_CHR_ANNOTATIONS = os.path.join(DATA_ROOT, '24_chromosomes_object', 'annotations')\n",
        "\n",
        "train_classifier_gen = ChromosomeClassifierDataGenerator(\n",
        "    MULTI_CHR_IMAGES, MULTI_CHR_ANNOTATIONS,\n",
        "    batch_size=32, img_size=224, augment=True, validation_split=0.2\n",
        ")\n",
        "\n",
        "val_classifier_gen = train_classifier_gen.get_validation_generator()\n",
        "\n",
        "classifier = ChromosomeClassifier(num_classes=24)\n",
        "classifier.compile_model(lr=0.001)\n",
        "\n",
        "print(\"\\nStarting classifier training...\")\n",
        "history = classifier.train(train_classifier_gen, val_classifier_gen, epochs=30)\n",
        "\n",
        "print(\"\\n✓ Classifier model saved to 'best_classifier.h5'\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8148cd6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "8148cd6d",
        "outputId": "b9c76346-f6dd-4f8f-a56e-45fa7e059c3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "CELL 3: INTEGRATION AND EVALUATION\n",
            "============================================================\n",
            "\n",
            "Loading train/test splits...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/ParadoX/inter_iit/2025_Karyogram_CV_Camp/train.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3271622843.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;31m# Load train/test splits from txt files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nLoading train/test splits...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_ROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0mtrain_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/ParadoX/inter_iit/2025_Karyogram_CV_Camp/train.txt'"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 3: INTEGRATION AND EVALUATION\n",
        "# Uses train.txt and test.txt to select specific images from 24_chromosomes_object\n",
        "# ==============================================================================\n",
        "\n",
        "class KaryotypePipeline:\n",
        "    \"\"\"End-to-end pipeline integrating identifier and classifier\"\"\"\n",
        "\n",
        "    def __init__(self, identifier: ChromosomeIdentifier,\n",
        "                 classifier: ChromosomeClassifier):\n",
        "        self.identifier = identifier\n",
        "        self.classifier = classifier\n",
        "        self.img_size = 224\n",
        "\n",
        "    def process_image(self, image_path: str, confidence_threshold=0.3):\n",
        "        image = cv2.imread(image_path)\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        boxes, scores = self.identifier.predict_boxes(image_rgb, confidence_threshold=confidence_threshold)\n",
        "\n",
        "        if len(boxes) == 0:\n",
        "            return []\n",
        "\n",
        "        predictions = []\n",
        "\n",
        "        for i, box in enumerate(boxes):\n",
        "            xmin, ymin, xmax, ymax = map(int, box)\n",
        "\n",
        "            chromosome_img = image_rgb[ymin:ymax, xmin:xmax]\n",
        "\n",
        "            if chromosome_img.size == 0:\n",
        "                continue\n",
        "\n",
        "            chromosome_img = cv2.resize(chromosome_img, (self.img_size, self.img_size))\n",
        "            chromosome_img = chromosome_img.astype(np.float32) / 255.0\n",
        "\n",
        "            mean = np.array([0.485, 0.456, 0.406])\n",
        "            std = np.array([0.229, 0.224, 0.225])\n",
        "            chromosome_img = (chromosome_img - mean) / std\n",
        "\n",
        "            chromosome_img = np.expand_dims(chromosome_img, axis=0)\n",
        "\n",
        "            probs = self.classifier.model.predict(chromosome_img, verbose=0)[0]\n",
        "            class_id = np.argmax(probs)\n",
        "            confidence = probs[class_id]\n",
        "\n",
        "            predictions.append({\n",
        "                'bbox': box,\n",
        "                'class_id': int(class_id),\n",
        "                'class_name': self._get_class_name(class_id),\n",
        "                'confidence': float(confidence),\n",
        "                'score': float(scores[i]) if i < len(scores) else 1.0\n",
        "            })\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_class_name(class_id: int) -> str:\n",
        "        if class_id < 22:\n",
        "            return str(class_id + 1)\n",
        "        elif class_id == 22:\n",
        "            return 'X'\n",
        "        else:\n",
        "            return 'Y'\n",
        "\n",
        "\n",
        "def evaluate_pipeline(pipeline: KaryotypePipeline, images_dir: str,\n",
        "                     annotations_dir: str, test_list: List[str],\n",
        "                     output_dir='results'):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    parser = AnnotationParser()\n",
        "\n",
        "    all_true_labels = []\n",
        "    all_pred_probs = []\n",
        "\n",
        "    print(\"Evaluating pipeline on test set...\")\n",
        "    for idx, img_name in enumerate(test_list):\n",
        "        img_name = img_name.strip()\n",
        "        img_path = os.path.join(images_dir, f\"{img_name}.jpg\")\n",
        "        xml_path = os.path.join(annotations_dir, f\"{img_name}.xml\")\n",
        "\n",
        "        if not os.path.exists(img_path):\n",
        "            img_path = os.path.join(images_dir, f\"{img_name}.png\")\n",
        "\n",
        "        if not os.path.exists(img_path) or not os.path.exists(xml_path):\n",
        "            continue\n",
        "\n",
        "        if (idx + 1) % 10 == 0:\n",
        "            print(f\"Processed {idx+1}/{len(test_list)} images...\")\n",
        "\n",
        "        annotations = parser.parse_xml(xml_path)\n",
        "        gt_chromosomes = [obj['name'] for obj in annotations['objects']]\n",
        "\n",
        "        predictions = pipeline.process_image(img_path)\n",
        "\n",
        "        for gt_chr in gt_chromosomes:\n",
        "            if gt_chr in ChromosomeClassifierDataGenerator.CHROMOSOME_CLASSES:\n",
        "                gt_class = ChromosomeClassifierDataGenerator.CHROMOSOME_CLASSES[gt_chr]\n",
        "\n",
        "                true_labels = np.zeros(24)\n",
        "                true_labels[gt_class] = 1\n",
        "\n",
        "                pred_probs = np.zeros(24)\n",
        "                for pred in predictions:\n",
        "                    pred_probs[pred['class_id']] = max(\n",
        "                        pred_probs[pred['class_id']],\n",
        "                        pred['confidence']\n",
        "                    )\n",
        "\n",
        "                all_true_labels.append(true_labels)\n",
        "                all_pred_probs.append(pred_probs)\n",
        "\n",
        "    all_true_labels = np.array(all_true_labels)\n",
        "    all_pred_probs = np.array(all_pred_probs)\n",
        "\n",
        "    auprc_scores = []\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    for i in range(24):\n",
        "        if np.sum(all_true_labels[:, i]) > 0:\n",
        "            precision, recall, _ = precision_recall_curve(\n",
        "                all_true_labels[:, i],\n",
        "                all_pred_probs[:, i]\n",
        "            )\n",
        "            auprc = auc(recall, precision)\n",
        "            auprc_scores.append(auprc)\n",
        "\n",
        "            plt.plot(recall, precision,\n",
        "                    label=f'Class {pipeline._get_class_name(i)} (auPRC={auprc:.3f})')\n",
        "\n",
        "    plt.xlabel('Recall', fontsize=12)\n",
        "    plt.ylabel('Precision', fontsize=12)\n",
        "    plt.title('Precision-Recall Curves for All Chromosome Classes', fontsize=14)\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'auprc_curves.png'),\n",
        "                dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    mean_auprc = np.mean(auprc_scores)\n",
        "    print(f\"\\nMean auPRC: {mean_auprc:.4f}\")\n",
        "\n",
        "    with open(os.path.join(output_dir, 'auprc_scores.txt'), 'w') as f:\n",
        "        f.write(f\"Mean auPRC: {mean_auprc:.4f}\\n\\n\")\n",
        "        f.write(\"Per-class auPRC scores:\\n\")\n",
        "        for i, score in enumerate(auprc_scores):\n",
        "            f.write(f\"Class {pipeline._get_class_name(i)}: {score:.4f}\\n\")\n",
        "\n",
        "    return mean_auprc, auprc_scores\n",
        "\n",
        "\n",
        "# ==================== INTEGRATION AND EVALUATION ====================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CELL 3: INTEGRATION AND EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load train/test splits from txt files\n",
        "print(\"\\nLoading train/test splits...\")\n",
        "with open(os.path.join(DATA_ROOT, 'train.txt'), 'r') as f:\n",
        "    train_list = f.readlines()\n",
        "\n",
        "with open(os.path.join(DATA_ROOT, 'test.txt'), 'r') as f:\n",
        "    test_list = f.readlines()\n",
        "\n",
        "print(f\"Train samples from train.txt: {len(train_list)}\")\n",
        "print(f\"Test samples from test.txt: {len(test_list)}\")\n",
        "\n",
        "# Load trained models\n",
        "print(\"\\nLoading trained models...\")\n",
        "identifier_loaded = ChromosomeIdentifier(img_size=800, max_detections=50)\n",
        "identifier_loaded.model = keras.models.load_model('identifier_model.h5')\n",
        "print(\"✓ Identifier model loaded\")\n",
        "\n",
        "classifier_loaded = ChromosomeClassifier(num_classes=24)\n",
        "classifier_loaded.model = keras.models.load_model('best_classifier.h5')\n",
        "print(\"✓ Classifier model loaded\")\n",
        "\n",
        "# Create pipeline\n",
        "pipeline = KaryotypePipeline(identifier_loaded, classifier_loaded)\n",
        "print(\"\\n✓ Pipeline created successfully\")\n",
        "\n",
        "# Evaluate on test set (using images specified in test.txt)\n",
        "print(\"\\nEvaluating pipeline on test set from test.txt...\")\n",
        "mean_auprc, class_auprcs = evaluate_pipeline(\n",
        "    pipeline, MULTI_CHR_IMAGES, MULTI_CHR_ANNOTATIONS,\n",
        "    test_list, output_dir='results'\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Mean auPRC: {mean_auprc:.4f}\")\n",
        "print(f\"\\nResults saved to 'results/' directory:\")\n",
        "print(f\"  - auPRC curve: results/auprc_curves.png\")\n",
        "print(f\"  - Detailed scores: results/auprc_scores.txt\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "aiml_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}