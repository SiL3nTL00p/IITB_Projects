{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXbfw4ci5zCN",
        "outputId": "27d84237-a9ca-4cb7-dab2-bdde9f9724f0"
      },
      "id": "aXbfw4ci5zCN",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8d0f9667",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d0f9667",
        "outputId": "33362c43-6e4f-4bba-8655-6bc210252181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "CELL 1: TRAINING CHROMOSOME IDENTIFIER\n",
            "============================================================\n",
            "\n",
            "TensorFlow version: 2.19.0\n",
            "GPU Available: []\n",
            "Found 3 images for identifier training\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "\n",
            "Starting identifier training...\n",
            "\n",
            "Epoch 1/10\n",
            "Epoch Loss: 1.4368\n",
            "\n",
            "Epoch 2/10\n",
            "Epoch Loss: 0.5887\n",
            "\n",
            "Epoch 3/10\n",
            "Epoch Loss: 0.5151\n",
            "\n",
            "Epoch 4/10\n",
            "Epoch Loss: 0.3998\n",
            "\n",
            "Epoch 5/10\n",
            "Epoch Loss: 0.3864\n",
            "\n",
            "Epoch 6/10\n",
            "Epoch Loss: 0.3188\n",
            "\n",
            "Epoch 7/10\n",
            "Epoch Loss: 0.2629\n",
            "\n",
            "Epoch 8/10\n",
            "Epoch Loss: 0.2358\n",
            "\n",
            "Epoch 9/10\n",
            "Epoch Loss: 0.2139\n",
            "\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch Loss: 0.2037\n",
            "\n",
            "✓ Identifier model saved to 'identifier_model.h5'\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 1: TRAIN IDENTIFIER MODULE\n",
        "# Uses ALL data from single_chromosomes_object folder\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Dict\n",
        "import albumentations as A\n",
        "\n",
        "# ==================== SHARED UTILITIES ====================\n",
        "\n",
        "class AnnotationParser:\n",
        "    \"\"\"Parse XML annotations for both single and 24-chromosome datasets\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_xml(xml_path: str) -> Dict:\n",
        "        tree = ET.parse(xml_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        annotations = {\n",
        "            'filename': root.find('filename').text if root.find('filename') is not None else '',\n",
        "            'size': {},\n",
        "            'objects': []\n",
        "        }\n",
        "\n",
        "        size = root.find('size')\n",
        "        if size is not None:\n",
        "            annotations['size'] = {\n",
        "                'width': int(size.find('width').text),\n",
        "                'height': int(size.find('height').text),\n",
        "                'depth': int(size.find('depth').text) if size.find('depth') is not None else 3\n",
        "            }\n",
        "\n",
        "        for obj in root.findall('object'):\n",
        "            name = obj.find('name').text\n",
        "            bndbox = obj.find('bndbox')\n",
        "\n",
        "            if bndbox is not None:\n",
        "                bbox = {\n",
        "                    'xmin': int(float(bndbox.find('xmin').text)),\n",
        "                    'ymin': int(float(bndbox.find('ymin').text)),\n",
        "                    'xmax': int(float(bndbox.find('xmax').text)),\n",
        "                    'ymax': int(float(bndbox.find('ymax').text))\n",
        "                }\n",
        "\n",
        "                annotations['objects'].append({\n",
        "                    'name': name,\n",
        "                    'bbox': bbox\n",
        "                })\n",
        "\n",
        "        return annotations\n",
        "\n",
        "\n",
        "# ==================== IDENTIFIER MODULE ====================\n",
        "\n",
        "class ChromosomeIdentifierDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Data generator for training chromosome identifier\"\"\"\n",
        "\n",
        "    def __init__(self, images_dir: str, annotations_dir: str,\n",
        "                 batch_size=2, img_size=800, shuffle=True):\n",
        "        self.images_dir = images_dir\n",
        "        self.annotations_dir = annotations_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.shuffle = shuffle\n",
        "        self.parser = AnnotationParser()\n",
        "\n",
        "        # Get all image files from directory\n",
        "        self.image_list = []\n",
        "        for filename in os.listdir(images_dir):\n",
        "            if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "                base_name = os.path.splitext(filename)[0]\n",
        "                xml_path = os.path.join(annotations_dir, f\"{base_name}.xml\")\n",
        "                if os.path.exists(xml_path):\n",
        "                    self.image_list.append(base_name)\n",
        "\n",
        "        print(f\"Found {len(self.image_list)} images for identifier training\")\n",
        "\n",
        "        self.indexes = np.arange(len(self.image_list))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.image_list) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        batch_images = [self.image_list[k] for k in batch_indexes]\n",
        "\n",
        "        X, y = self._generate_data(batch_images)\n",
        "        return X, y\n",
        "\n",
        "    def _generate_data(self, batch_images):\n",
        "        images = []\n",
        "        targets = []\n",
        "\n",
        "        for img_name in batch_images:\n",
        "            img_path = os.path.join(self.images_dir, f\"{img_name}.jpg\")\n",
        "            xml_path = os.path.join(self.annotations_dir, f\"{img_name}.xml\")\n",
        "\n",
        "            if not os.path.exists(img_path):\n",
        "                img_path = os.path.join(self.images_dir, f\"{img_name}.png\")\n",
        "\n",
        "            if not os.path.exists(img_path) or not os.path.exists(xml_path):\n",
        "                continue\n",
        "\n",
        "            image = cv2.imread(img_path)\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            orig_h, orig_w = image.shape[:2]\n",
        "\n",
        "            image = cv2.resize(image, (self.img_size, self.img_size))\n",
        "            image = image.astype(np.float32) / 255.0\n",
        "\n",
        "            annotations = self.parser.parse_xml(xml_path)\n",
        "\n",
        "            boxes = []\n",
        "            for obj in annotations['objects']:\n",
        "                bbox = obj['bbox']\n",
        "                xmin = bbox['xmin'] / orig_w\n",
        "                ymin = bbox['ymin'] / orig_h\n",
        "                xmax = bbox['xmax'] / orig_w\n",
        "                ymax = bbox['ymax'] / orig_h\n",
        "                boxes.append([ymin, xmin, ymax, xmax])\n",
        "\n",
        "            images.append(image)\n",
        "            targets.append(np.array(boxes) if len(boxes) > 0 else np.zeros((0, 4)))\n",
        "\n",
        "        return images, targets\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "\n",
        "class ChromosomeIdentifier:\n",
        "    \"\"\"Chromosome detection model\"\"\"\n",
        "\n",
        "    def __init__(self, img_size=800, max_detections=50):\n",
        "        self.img_size = img_size\n",
        "        self.max_detections = max_detections\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        inputs = layers.Input(shape=(self.img_size, self.img_size, 3))\n",
        "\n",
        "        base_model = ResNet50(include_top=False, weights='imagenet', input_tensor=inputs)\n",
        "\n",
        "        for layer in base_model.layers[:100]:\n",
        "            layer.trainable = False\n",
        "\n",
        "        x = base_model.output\n",
        "        x = layers.Conv2D(512, 3, padding='same', activation='relu')(x)\n",
        "        x = layers.Conv2D(512, 3, padding='same', activation='relu')(x)\n",
        "        x = layers.Conv2D(256, 3, padding='same', activation='relu')(x)\n",
        "        x = layers.GlobalAveragePooling2D()(x)\n",
        "        x = layers.Dense(1024, activation='relu')(x)\n",
        "        x = layers.Dropout(0.5)(x)\n",
        "\n",
        "        bbox_output = layers.Dense(self.max_detections * 4, name='bbox')(x)\n",
        "        objectness = layers.Dense(self.max_detections, activation='sigmoid', name='objectness')(x)\n",
        "\n",
        "        model = models.Model(inputs=inputs, outputs=[bbox_output, objectness])\n",
        "        return model\n",
        "\n",
        "    def train(self, train_gen, epochs=10):\n",
        "        optimizer = optimizers.Adam(0.001)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "            epoch_losses = []\n",
        "\n",
        "            for batch_idx in range(len(train_gen)):\n",
        "                images, targets = train_gen[batch_idx]\n",
        "\n",
        "                if len(images) == 0:\n",
        "                    continue\n",
        "\n",
        "                images = np.array(images)\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    bbox_pred, obj_pred = self.model(images, training=True)\n",
        "                    bbox_pred = tf.reshape(bbox_pred, (-1, self.max_detections, 4))\n",
        "\n",
        "                    batch_loss = 0\n",
        "                    for i, target_boxes in enumerate(targets):\n",
        "                        if len(target_boxes) > 0:\n",
        "                            num_objs = min(len(target_boxes), self.max_detections)\n",
        "\n",
        "                            target_padded = np.zeros((self.max_detections, 4))\n",
        "                            target_padded[:num_objs] = target_boxes[:num_objs]\n",
        "\n",
        "                            bbox_loss = tf.reduce_mean(tf.square(bbox_pred[i] - target_padded))\n",
        "\n",
        "                            obj_target = np.zeros(self.max_detections)\n",
        "                            obj_target[:num_objs] = 1\n",
        "                            obj_loss = tf.keras.losses.binary_crossentropy(obj_target, obj_pred[i])\n",
        "\n",
        "                            batch_loss += bbox_loss + 0.5 * tf.reduce_mean(obj_loss)\n",
        "\n",
        "                    batch_loss = batch_loss / len(targets)\n",
        "\n",
        "                grads = tape.gradient(batch_loss, self.model.trainable_variables)\n",
        "                optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "\n",
        "                epoch_losses.append(float(batch_loss))\n",
        "\n",
        "                if (batch_idx + 1) % 10 == 0:\n",
        "                    print(f\"  Batch {batch_idx+1}/{len(train_gen)}, Loss: {np.mean(epoch_losses[-10:]):.4f}\")\n",
        "\n",
        "            print(f\"Epoch Loss: {np.mean(epoch_losses):.4f}\")\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def predict_boxes(self, image, confidence_threshold=0.3):\n",
        "        orig_h, orig_w = image.shape[:2]\n",
        "        image_resized = cv2.resize(image, (self.img_size, self.img_size))\n",
        "        image_resized = image_resized.astype(np.float32) / 255.0\n",
        "        image_resized = np.expand_dims(image_resized, axis=0)\n",
        "\n",
        "        bbox_pred, obj_pred = self.model.predict(image_resized, verbose=0)\n",
        "\n",
        "        bbox_pred = bbox_pred.reshape(self.max_detections, 4)\n",
        "        obj_pred = obj_pred.reshape(self.max_detections)\n",
        "\n",
        "        keep = obj_pred > confidence_threshold\n",
        "        boxes = bbox_pred[keep]\n",
        "        scores = obj_pred[keep]\n",
        "\n",
        "        final_boxes = []\n",
        "        for box in boxes:\n",
        "            ymin, xmin, ymax, xmax = box\n",
        "            xmin = int(xmin * orig_w)\n",
        "            ymin = int(ymin * orig_h)\n",
        "            xmax = int(xmax * orig_w)\n",
        "            ymax = int(ymax * orig_h)\n",
        "\n",
        "            xmin = max(0, min(xmin, orig_w))\n",
        "            xmax = max(0, min(xmax, orig_w))\n",
        "            ymin = max(0, min(ymin, orig_h))\n",
        "            ymax = max(0, min(ymax, orig_h))\n",
        "\n",
        "            if xmax > xmin and ymax > ymin:\n",
        "                final_boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        return np.array(final_boxes), scores[:len(final_boxes)]\n",
        "\n",
        "\n",
        "# ==================== TRAIN IDENTIFIER ====================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"CELL 1: TRAINING CHROMOSOME IDENTIFIER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "DATA_ROOT = '/content/drive/MyDrive/ParadoX/inter_iit/2025_Karyogram_CV_Camp'\n",
        "SINGLE_CHR_IMAGES = os.path.join(DATA_ROOT, 'single_chromosomes_object', 'images')\n",
        "SINGLE_CHR_ANNOTATIONS = os.path.join(DATA_ROOT, 'single_chromosomes_object', 'annotations')\n",
        "\n",
        "print(\"\\nTensorFlow version:\", tf.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "train_identifier_gen = ChromosomeIdentifierDataGenerator(\n",
        "    SINGLE_CHR_IMAGES, SINGLE_CHR_ANNOTATIONS,\n",
        "    batch_size=2, img_size=800, shuffle=True\n",
        ")\n",
        "\n",
        "identifier = ChromosomeIdentifier(img_size=800, max_detections=50)\n",
        "\n",
        "print(\"\\nStarting identifier training...\")\n",
        "identifier.train(train_identifier_gen, epochs=10)\n",
        "\n",
        "identifier.model.save('identifier_model.h5')\n",
        "print(\"\\n✓ Identifier model saved to 'identifier_model.h5'\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "629ab51b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        },
        "id": "629ab51b",
        "outputId": "f0085904-7bf4-4d64-8110-e538f0050bb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "CELL 2: TRAINING CHROMOSOME CLASSIFIER\n",
            "============================================================\n",
            "\n",
            "Initializing ChromosomeClassifierDataGenerator:\n",
            "\n",
            "Building classifier dataset...\n",
            "Found 3 images to process...\n",
            "Built dataset with 2 chromosome samples\n",
            "Train samples: 2, Validation samples: 0\n",
            "\n",
            "Initializing ChromosomeClassifierDataGenerator:\n",
            "Validation samples: 0\n",
            "\n",
            "Starting classifier training...\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer._make_function.<locals>.multi_step_on_iterator at 0x7809d30b4f40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20s/step - accuracy: 0.0000e+00 - loss: 3.7588"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The PyDataset has length 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2877218295.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStarting classifier training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_classifier_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_classifier_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n✓ Classifier model saved to 'best_classifier.h5'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2877218295.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_gen, val_gen, epochs)\u001b[0m\n\u001b[1;32m    233\u001b[0m         ]\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         history = self.model.fit(\n\u001b[0m\u001b[1;32m    236\u001b[0m             \u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py\u001b[0m in \u001b[0;36mget_tf_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    293\u001b[0m             ]\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The PyDataset has length 0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_adapter_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The PyDataset has length 0"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 2: TRAIN CLASSIFIER MODULE\n",
        "# Uses ALL data from 24_chromosomes_object folder\n",
        "# ==============================================================================\n",
        "\n",
        "class ChromosomeClassifierDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Data generator for training chromosome classifier\"\"\"\n",
        "\n",
        "    CHROMOSOME_CLASSES = {\n",
        "        '1': 0, '2': 1, '3': 2, '4': 3, '5': 4, '6': 5,\n",
        "        '7': 6, '8': 7, '9': 8, '10': 9, '11': 10, '12': 11,\n",
        "        '13': 12, '14': 13, '15': 14, '16': 15, '17': 16, '18': 17,\n",
        "        '19': 18, '20': 19, '21': 20, '22': 21, 'X': 22, 'Y': 23\n",
        "    }\n",
        "\n",
        "    def __init__(self, images_dir: str, annotations_dir: str, is_validation=False,\n",
        "                 batch_size=32, img_size=224, augment=False, validation_split=0.2,\n",
        "                 data=None, indexes=None): # Add data and indexes parameters\n",
        "        print(f\"\\nInitializing ChromosomeClassifierDataGenerator:\")\n",
        "        self.images_dir = images_dir\n",
        "        self.annotations_dir = annotations_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.augment = augment\n",
        "        self.validation_split = validation_split\n",
        "        self.is_validation = is_validation\n",
        "        self.parser = AnnotationParser()\n",
        "\n",
        "        # Initialize data storage\n",
        "        if data is None or indexes is None:\n",
        "            self.chromosome_data = []\n",
        "            self._build_dataset()\n",
        "\n",
        "            # Split dataset only for the training generator\n",
        "            if not self.is_validation:\n",
        "                total_samples = len(self.chromosome_data)\n",
        "                val_size = int(total_samples * validation_split)\n",
        "\n",
        "                # Create train/val indexes\n",
        "                np.random.seed(42)\n",
        "                all_indexes = np.arange(total_samples)\n",
        "                np.random.shuffle(all_indexes)\n",
        "\n",
        "                self.indexes = all_indexes[val_size:]\n",
        "                self.val_indexes = all_indexes[:val_size]\n",
        "\n",
        "                print(f\"Train samples: {len(self.indexes)}, Validation samples: {len(self.val_indexes)}\")\n",
        "            else:\n",
        "                 # For validation generator, use all data passed to it\n",
        "                 self.chromosome_data = data\n",
        "                 self.indexes = indexes\n",
        "                 self.val_indexes = [] # Not needed for validation generator\n",
        "                 print(f\"Validation samples: {len(self.indexes)}\")\n",
        "        else:\n",
        "            # Use provided data and indexes for validation generator\n",
        "            self.chromosome_data = data\n",
        "            self.indexes = indexes\n",
        "            self.val_indexes = [] # Not needed for validation generator\n",
        "            print(f\"Validation samples: {len(self.indexes)}\")\n",
        "\n",
        "\n",
        "        # Setup augmentation if required\n",
        "        if augment and not self.is_validation: # Apply augmentation only to training data\n",
        "            self.aug = A.Compose([\n",
        "                A.HorizontalFlip(p=0.5),\n",
        "                A.VerticalFlip(p=0.5),\n",
        "                A.Rotate(limit=15, p=0.5),\n",
        "                A.RandomBrightnessContrast(p=0.3),\n",
        "                A.GaussNoise(p=0.2),\n",
        "            ])\n",
        "        else:\n",
        "            self.aug = None\n",
        "\n",
        "\n",
        "    def _build_dataset(self):\n",
        "        \"\"\"Build dataset from images and annotations\"\"\"\n",
        "        print(\"\\nBuilding classifier dataset...\")\n",
        "\n",
        "        # Get list of valid image files\n",
        "        image_files = []\n",
        "        for filename in os.listdir(self.images_dir):\n",
        "            if filename.endswith(('.jpg', '.png')):\n",
        "                base_name = os.path.splitext(filename)[0]\n",
        "                xml_path = os.path.join(self.annotations_dir, f\"{base_name}.xml\")\n",
        "                if os.path.exists(xml_path):\n",
        "                    image_files.append(base_name)\n",
        "\n",
        "        print(f\"Found {len(image_files)} images to process...\")\n",
        "\n",
        "        # Process each image and its annotations\n",
        "        for img_name in image_files:\n",
        "            img_path = os.path.join(self.images_dir, f\"{img_name}.jpg\")\n",
        "            xml_path = os.path.join(self.annotations_dir, f\"{img_name}.xml\")\n",
        "\n",
        "            # Try PNG if JPG doesn't exist\n",
        "            if not os.path.exists(img_path):\n",
        "                img_path = os.path.join(self.images_dir, f\"{img_name}.png\")\n",
        "\n",
        "            if not os.path.exists(img_path) or not os.path.exists(xml_path):\n",
        "                print(f\"Warning: Missing files for {img_name}\")\n",
        "                continue\n",
        "\n",
        "            # Load and process image\n",
        "            image = cv2.imread(img_path)\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Parse annotations\n",
        "            annotations = self.parser.parse_xml(xml_path)\n",
        "\n",
        "            # Extract individual chromosomes\n",
        "            for obj in annotations['objects']:\n",
        "                bbox = obj['bbox']\n",
        "                label = obj['name']\n",
        "\n",
        "                if label in self.CHROMOSOME_CLASSES:\n",
        "                    chromosome_img = image[bbox['ymin']:bbox['ymax'],\n",
        "                                        bbox['xmin']:bbox['xmax']]\n",
        "\n",
        "                    if chromosome_img.size > 0:\n",
        "                        self.chromosome_data.append({\n",
        "                            'image': chromosome_img,\n",
        "                            'label': self.CHROMOSOME_CLASSES[label]\n",
        "                        })\n",
        "\n",
        "        print(f\"Built dataset with {len(self.chromosome_data)} chromosome samples\")\n",
        "\n",
        "\n",
        "    def get_validation_generator(self):\n",
        "        \"\"\"Create validation generator using validation split\"\"\"\n",
        "        # Create a new generator instance specifically for validation\n",
        "        val_gen = ChromosomeClassifierDataGenerator(\n",
        "            images_dir=self.images_dir, # Pass directories (not strictly needed but keeps signature)\n",
        "            annotations_dir=self.annotations_dir,\n",
        "            batch_size=self.batch_size,\n",
        "            img_size=self.img_size,\n",
        "            augment=False, # No augmentation for validation\n",
        "            validation_split=0.0, # Not used for validation generator\n",
        "            is_validation=True, # Mark as validation generator\n",
        "            data=[self.chromosome_data[i] for i in self.val_indexes], # Pass the already built data\n",
        "            indexes=np.arange(len(self.val_indexes)) # Pass the validation indexes\n",
        "        )\n",
        "        return val_gen\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return number of batches per epoch\"\"\"\n",
        "        return int(np.ceil(len(self.indexes) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Get batch of data\"\"\"\n",
        "        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        batch_images = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for idx in batch_indexes:\n",
        "            data = self.chromosome_data[idx]\n",
        "            image = data['image'].copy()\n",
        "            label = data['label']\n",
        "\n",
        "            # Apply augmentation if enabled\n",
        "            if self.aug is not None:\n",
        "                augmented = self.aug(image=image)\n",
        "                image = augmented['image']\n",
        "\n",
        "            # Preprocess image\n",
        "            image = cv2.resize(image, (self.img_size, self.img_size))\n",
        "            image = image.astype(np.float32) / 255.0\n",
        "\n",
        "            # Normalize using ImageNet stats\n",
        "            mean = np.array([0.485, 0.456, 0.406])\n",
        "            std = np.array([0.229, 0.224, 0.225])\n",
        "            image = (image - mean) / std\n",
        "\n",
        "            batch_images.append(image)\n",
        "            batch_labels.append(label)\n",
        "\n",
        "        return np.array(batch_images), np.array(batch_labels)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Called at the end of every epoch\"\"\"\n",
        "        if not self.is_validation: # Only shuffle training data\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "\n",
        "class ChromosomeClassifier:\n",
        "    \"\"\"Chromosome classification model (24 classes)\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=24, img_size=224):\n",
        "        self.num_classes = num_classes\n",
        "        self.img_size = img_size\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        base_model = ResNet50(include_top=False, weights='imagenet',\n",
        "                             input_shape=(self.img_size, self.img_size, 3))\n",
        "\n",
        "        for layer in base_model.layers[:140]:\n",
        "            layer.trainable = False\n",
        "\n",
        "        x = base_model.output\n",
        "        x = layers.GlobalAveragePooling2D()(x)\n",
        "        x = layers.Dense(512, activation='relu')(x)\n",
        "        x = layers.Dropout(0.5)(x)\n",
        "        x = layers.Dense(256, activation='relu')(x)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
        "\n",
        "        model = models.Model(inputs=base_model.input, outputs=outputs)\n",
        "        return model\n",
        "\n",
        "    def compile_model(self, lr=0.001):\n",
        "        self.model.compile(\n",
        "            optimizer=optimizers.Adam(lr),\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "    def train(self, train_gen, val_gen, epochs=30):\n",
        "        callbacks = [\n",
        "            keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_accuracy', factor=0.5, patience=3,\n",
        "                min_lr=1e-7, verbose=1\n",
        "            ),\n",
        "            keras.callbacks.ModelCheckpoint(\n",
        "                'best_classifier.h5', monitor='val_accuracy',\n",
        "                save_best_only=True, verbose=1\n",
        "            ),\n",
        "            keras.callbacks.EarlyStopping(\n",
        "                monitor='val_accuracy', patience=7,\n",
        "                restore_best_weights=True, verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        history = self.model.fit(\n",
        "            train_gen,\n",
        "            validation_data=val_gen,\n",
        "            epochs=epochs,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        return history\n",
        "\n",
        "\n",
        "# ==================== TRAIN CLASSIFIER ====================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CELL 2: TRAINING CHROMOSOME CLASSIFIER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "DATA_ROOT = '/content/drive/MyDrive/ParadoX/inter_iit/2025_Karyogram_CV_Camp'\n",
        "MULTI_CHR_IMAGES = os.path.join(DATA_ROOT, '24_chromosomes_object', 'images')\n",
        "MULTI_CHR_ANNOTATIONS = os.path.join(DATA_ROOT, '24_chromosomes_object', 'annotations')\n",
        "\n",
        "train_classifier_gen = ChromosomeClassifierDataGenerator(\n",
        "    MULTI_CHR_IMAGES, MULTI_CHR_ANNOTATIONS,\n",
        "    batch_size=32, img_size=224, augment=True, validation_split=0.2\n",
        ")\n",
        "\n",
        "val_classifier_gen = train_classifier_gen.get_validation_generator()\n",
        "\n",
        "classifier = ChromosomeClassifier(num_classes=24)\n",
        "classifier.compile_model(lr=0.001)\n",
        "\n",
        "print(\"\\nStarting classifier training...\")\n",
        "history = classifier.train(train_classifier_gen, val_classifier_gen, epochs=30)\n",
        "\n",
        "print(\"\\n✓ Classifier model saved to 'best_classifier.h5'\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8148cd6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        },
        "id": "8148cd6d",
        "outputId": "5dba4899-0418-4e09-d1e0-871fe1174a2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "CELL 3: INTEGRATION AND EVALUATION\n",
            "============================================================\n",
            "\n",
            "Loading train/test splits...\n",
            "Train samples from train.txt: 4\n",
            "Test samples from test.txt: 2\n",
            "\n",
            "Loading trained models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Identifier model loaded\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'best_classifier.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-695018059.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0mclassifier_loaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChromosomeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m \u001b[0mclassifier_loaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_classifier.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✓ Classifier model loaded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    194\u001b[0m         )\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         return legacy_h5_format.load_model_from_hdf5(\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 564\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'best_classifier.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 3: INTEGRATION AND EVALUATION\n",
        "# Uses train.txt and test.txt to select specific images from 24_chromosomes_object\n",
        "# ==============================================================================\n",
        "\n",
        "class KaryotypePipeline:\n",
        "    \"\"\"End-to-end pipeline integrating identifier and classifier\"\"\"\n",
        "\n",
        "    def __init__(self, identifier: ChromosomeIdentifier,\n",
        "                 classifier: ChromosomeClassifier):\n",
        "        self.identifier = identifier\n",
        "        self.classifier = classifier\n",
        "        self.img_size = 224\n",
        "\n",
        "    def process_image(self, image_path: str, confidence_threshold=0.3):\n",
        "        image = cv2.imread(image_path)\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        boxes, scores = self.identifier.predict_boxes(image_rgb, confidence_threshold=confidence_threshold)\n",
        "\n",
        "        if len(boxes) == 0:\n",
        "            return []\n",
        "\n",
        "        predictions = []\n",
        "\n",
        "        for i, box in enumerate(boxes):\n",
        "            xmin, ymin, xmax, ymax = map(int, box)\n",
        "\n",
        "            chromosome_img = image_rgb[ymin:ymax, xmin:xmax]\n",
        "\n",
        "            if chromosome_img.size == 0:\n",
        "                continue\n",
        "\n",
        "            chromosome_img = cv2.resize(chromosome_img, (self.img_size, self.img_size))\n",
        "            chromosome_img = chromosome_img.astype(np.float32) / 255.0\n",
        "\n",
        "            mean = np.array([0.485, 0.456, 0.406])\n",
        "            std = np.array([0.229, 0.224, 0.225])\n",
        "            chromosome_img = (chromosome_img - mean) / std\n",
        "\n",
        "            chromosome_img = np.expand_dims(chromosome_img, axis=0)\n",
        "\n",
        "            probs = self.classifier.model.predict(chromosome_img, verbose=0)[0]\n",
        "            class_id = np.argmax(probs)\n",
        "            confidence = probs[class_id]\n",
        "\n",
        "            predictions.append({\n",
        "                'bbox': box,\n",
        "                'class_id': int(class_id),\n",
        "                'class_name': self._get_class_name(class_id),\n",
        "                'confidence': float(confidence),\n",
        "                'score': float(scores[i]) if i < len(scores) else 1.0\n",
        "            })\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_class_name(class_id: int) -> str:\n",
        "        if class_id < 22:\n",
        "            return str(class_id + 1)\n",
        "        elif class_id == 22:\n",
        "            return 'X'\n",
        "        else:\n",
        "            return 'Y'\n",
        "\n",
        "\n",
        "def evaluate_pipeline(pipeline: KaryotypePipeline, images_dir: str,\n",
        "                     annotations_dir: str, test_list: List[str],\n",
        "                     output_dir='results'):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    parser = AnnotationParser()\n",
        "\n",
        "    all_true_labels = []\n",
        "    all_pred_probs = []\n",
        "\n",
        "    print(\"Evaluating pipeline on test set...\")\n",
        "    for idx, img_name in enumerate(test_list):\n",
        "        img_name = img_name.strip()\n",
        "        img_path = os.path.join(images_dir, f\"{img_name}.jpg\")\n",
        "        xml_path = os.path.join(annotations_dir, f\"{img_name}.xml\")\n",
        "\n",
        "        if not os.path.exists(img_path):\n",
        "            img_path = os.path.join(images_dir, f\"{img_name}.png\")\n",
        "\n",
        "        if not os.path.exists(img_path) or not os.path.exists(xml_path):\n",
        "            continue\n",
        "\n",
        "        if (idx + 1) % 10 == 0:\n",
        "            print(f\"Processed {idx+1}/{len(test_list)} images...\")\n",
        "\n",
        "        annotations = parser.parse_xml(xml_path)\n",
        "        gt_chromosomes = [obj['name'] for obj in annotations['objects']]\n",
        "\n",
        "        predictions = pipeline.process_image(img_path)\n",
        "\n",
        "        for gt_chr in gt_chromosomes:\n",
        "            if gt_chr in ChromosomeClassifierDataGenerator.CHROMOSOME_CLASSES:\n",
        "                gt_class = ChromosomeClassifierDataGenerator.CHROMOSOME_CLASSES[gt_chr]\n",
        "\n",
        "                true_labels = np.zeros(24)\n",
        "                true_labels[gt_class] = 1\n",
        "\n",
        "                pred_probs = np.zeros(24)\n",
        "                for pred in predictions:\n",
        "                    pred_probs[pred['class_id']] = max(\n",
        "                        pred_probs[pred['class_id']],\n",
        "                        pred['confidence']\n",
        "                    )\n",
        "\n",
        "                all_true_labels.append(true_labels)\n",
        "                all_pred_probs.append(pred_probs)\n",
        "\n",
        "    all_true_labels = np.array(all_true_labels)\n",
        "    all_pred_probs = np.array(all_pred_probs)\n",
        "\n",
        "    auprc_scores = []\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    for i in range(24):\n",
        "        if np.sum(all_true_labels[:, i]) > 0:\n",
        "            precision, recall, _ = precision_recall_curve(\n",
        "                all_true_labels[:, i],\n",
        "                all_pred_probs[:, i]\n",
        "            )\n",
        "            auprc = auc(recall, precision)\n",
        "            auprc_scores.append(auprc)\n",
        "\n",
        "            plt.plot(recall, precision,\n",
        "                    label=f'Class {pipeline._get_class_name(i)} (auPRC={auprc:.3f})')\n",
        "\n",
        "    plt.xlabel('Recall', fontsize=12)\n",
        "    plt.ylabel('Precision', fontsize=12)\n",
        "    plt.title('Precision-Recall Curves for All Chromosome Classes', fontsize=14)\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'auprc_curves.png'),\n",
        "                dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    mean_auprc = np.mean(auprc_scores)\n",
        "    print(f\"\\nMean auPRC: {mean_auprc:.4f}\")\n",
        "\n",
        "    with open(os.path.join(output_dir, 'auprc_scores.txt'), 'w') as f:\n",
        "        f.write(f\"Mean auPRC: {mean_auprc:.4f}\\n\\n\")\n",
        "        f.write(\"Per-class auPRC scores:\\n\")\n",
        "        for i, score in enumerate(auprc_scores):\n",
        "            f.write(f\"Class {pipeline._get_class_name(i)}: {score:.4f}\\n\")\n",
        "\n",
        "    return mean_auprc, auprc_scores\n",
        "\n",
        "\n",
        "# ==================== INTEGRATION AND EVALUATION ====================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CELL 3: INTEGRATION AND EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load train/test splits from txt files\n",
        "print(\"\\nLoading train/test splits...\")\n",
        "with open(os.path.join(DATA_ROOT, 'train.txt'), 'r') as f:\n",
        "    train_list = f.readlines()\n",
        "\n",
        "with open(os.path.join(DATA_ROOT, 'test.txt'), 'r') as f:\n",
        "    test_list = f.readlines()\n",
        "\n",
        "print(f\"Train samples from train.txt: {len(train_list)}\")\n",
        "print(f\"Test samples from test.txt: {len(test_list)}\")\n",
        "\n",
        "# Load trained models\n",
        "print(\"\\nLoading trained models...\")\n",
        "identifier_loaded = ChromosomeIdentifier(img_size=800, max_detections=50)\n",
        "identifier_loaded.model = keras.models.load_model('identifier_model.h5')\n",
        "print(\"✓ Identifier model loaded\")\n",
        "\n",
        "classifier_loaded = ChromosomeClassifier(num_classes=24)\n",
        "classifier_loaded.model = keras.models.load_model('best_classifier.h5')\n",
        "print(\"✓ Classifier model loaded\")\n",
        "\n",
        "# Create pipeline\n",
        "pipeline = KaryotypePipeline(identifier_loaded, classifier_loaded)\n",
        "print(\"\\n✓ Pipeline created successfully\")\n",
        "\n",
        "# Evaluate on test set (using images specified in test.txt)\n",
        "print(\"\\nEvaluating pipeline on test set from test.txt...\")\n",
        "mean_auprc, class_auprcs = evaluate_pipeline(\n",
        "    pipeline, MULTI_CHR_IMAGES, MULTI_CHR_ANNOTATIONS,\n",
        "    test_list, output_dir='results'\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Mean auPRC: {mean_auprc:.4f}\")\n",
        "print(f\"\\nResults saved to 'results/' directory:\")\n",
        "print(f\"  - auPRC curve: results/auprc_curves.png\")\n",
        "print(f\"  - Detailed scores: results/auprc_scores.txt\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "aiml_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}