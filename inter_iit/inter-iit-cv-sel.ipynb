{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Setting up environment with required libraries...\n",
      "This may take a few minutes...\n",
      "\u001b[33mWARNING: Skipping numpy as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Collecting numpy==1.26.3\n",
      "  Downloading https://download.pytorch.org/whl/numpy-1.26.3-cp310-cp310-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement pandas==2.2.2 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for pandas==2.2.2\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "✅ Environment setup complete!\n",
      "⚠️  IMPORTANT: Please RESTART the runtime/kernel now before running the next cells.\n",
      "   Go to: Runtime → Restart runtime (or Kernel → Restart)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# ENVIRONMENT SETUP - RUN THIS CELL FIRST\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"🔧 Setting up environment with required libraries...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# Uninstall existing packages to avoid conflicts\n",
    "%pip uninstall -y numpy tensorflow torch\n",
    "\n",
    "# Install all required packages with specific versions\n",
    "%pip install \\\n",
    "    numpy==1.26.3 \\\n",
    "    pandas==2.2.2 \\\n",
    "    scikit-learn==1.4.2 \\\n",
    "    matplotlib==3.8.4 \\\n",
    "    seaborn==0.13.2 \\\n",
    "    tensorflow==2.15.0 \\\n",
    "    torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121 \\\n",
    "    ultralytics==8.1.47 \\\n",
    "    albumentations==1.4.0 \\\n",
    "    pyyaml==6.0.1\n",
    "\n",
    "print(\"\\n✅ Environment setup complete!\")\n",
    "print(\"⚠️  IMPORTANT: Please RESTART the runtime/kernel now before running the next cells.\")\n",
    "print(\"   Go to: Runtime → Restart runtime (or Kernel → Restart)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Configuration Guide\n",
    "\n",
    "### Configuration Flags (Set at the top of script)\n",
    "\n",
    "```py\n",
    "    TRAIN_IDENTIFIER: bool = True\n",
    "    TRAIN_CLASSIFIER: bool = True\n",
    "    RUN_EVALUATION: bool = True\n",
    "    VISUALIZE_PREDICTIONS: bool = True # Set to True to save example images\n",
    "    NUM_VISUALIZATION_IMAGES: int = 10 # Number of test images to visualize\n",
    "\n",
    "    # --- Model Paths ---\n",
    "    IDENTIFIER_MODEL_PATH: str = 'identifier_model.h5'\n",
    "    CLASSIFIER_MODEL_PATH: str = 'best_classifier.h5'\n",
    "    OUTPUT_DIR: str = 'pipeline_output' # Directory for plots and images\n",
    "\n",
    "    # --- Training Parameters ---\n",
    "    IDENTIFIER_EPOCHS: int = 5\n",
    "    CLASSIFIER_EPOCHS: int = 15\n",
    "    IDENTIFIER_IMG_SIZE: int = 800\n",
    "    CLASSIFIER_IMG_SIZE: int = 224\n",
    "    VALIDATION_SPLIT: float = 0.2\n",
    "\n",
    "    # --- Pipeline Parameters ---\n",
    "    DETECTION_THRESHOLD: float = 0.3      # Minimum confidence for a detected object\n",
    "    EVAL_IOU_THRESHOLD: float = 0.5       # IoU threshold to match pred/GT boxes\n",
    "\n",
    "    # --- Dataset Path ---\n",
    "    # Ensure this points to the correct directory\n",
    "    DATA_ROOT: str = '/kaggle/input/2025-karyogram-cv-camp/2025_Karyogram_CV_Camp'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-14T17:43:02.278662Z",
     "iopub.status.busy": "2025-10-14T17:43:02.277956Z",
     "iopub.status.idle": "2025-10-14T17:43:02.291188Z",
     "shell.execute_reply": "2025-10-14T17:43:02.290347Z",
     "shell.execute_reply.started": "2025-10-14T17:43:02.278633Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Tuple, Dict, Any\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mitertools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cycle\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Suppress TensorFlow logs\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/sklearn/__init__.py:73\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     __check_build,\n\u001b[1;32m     71\u001b[0m     _distributor_init,\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     76\u001b[0m _submodules \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompose\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    115\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aiml_env/lib/python3.10/site-packages/sklearn/base.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# ⚙️ CELL 1: ENVIRONMENT SETUP, IMPORTS & CONFIGURATION\n",
    "# ==============================================================================\n",
    "# This cell pins the versions of key libraries to ensure full compatibility.\n",
    "# Run this cell once, then RESTART the kernel before running the rest.\n",
    "# After restarting, run the imports below\n",
    "\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "import warnings\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import cycle\n",
    "\n",
    "# Suppress TensorFlow logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    balanced_accuracy_score, roc_curve, auc, precision_recall_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from ultralytics import YOLO\n",
    "import albumentations as A\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.config.optimizer.set_jit(False)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Holds all configuration parameters for the pipeline.\"\"\"\n",
    "    # --- Execution Control ---\n",
    "    # Set these to False to skip re-training and use saved models\n",
    "    TRAIN_IDENTIFIER: bool = True\n",
    "    TRAIN_CLASSIFIER: bool = True\n",
    "    \n",
    "    # --- Model & Training Paths ---\n",
    "    OUTPUT_DIR: str = 'pipeline_output'\n",
    "    CLASSIFIER_MODEL_PATH: str = 'best_classifier.h5'\n",
    "\n",
    "    # --- YOLO Identifier Parameters ---\n",
    "    YOLO_MODEL_NAME: str = 'yolov8n.pt'\n",
    "    YOLO_DATA_DIR: str = 'yolo_dataset'\n",
    "    YOLO_DATA_CONFIG: str = 'yolo_dataset/data.yaml'\n",
    "    YOLO_PROJECT_NAME: str = 'chromosome_detector'\n",
    "    YOLO_RUN_NAME: str = 'train_run'\n",
    "    YOLO_MODEL_PATH: str = f'{YOLO_PROJECT_NAME}/{YOLO_RUN_NAME}/weights/best.pt'\n",
    "\n",
    "    # --- Training Parameters ---\n",
    "    IDENTIFIER_EPOCHS: int = 25\n",
    "    CLASSIFIER_EPOCHS: int = 15\n",
    "    IDENTIFIER_IMG_SIZE: int = 640\n",
    "    CLASSIFIER_IMG_SIZE: int = 224\n",
    "    VALIDATION_SPLIT: float = 0.2\n",
    "\n",
    "    # --- Pipeline Parameters ---\n",
    "    DETECTION_THRESHOLD: float = 0.3\n",
    "    EVAL_IOU_THRESHOLD: float = 0.5\n",
    "\n",
    "    # --- Dataset Path ---\n",
    "    DATA_ROOT: str = '/kaggle/input/inter-iit-small-set/2025_Karyogram_CV_Camp_small'\n",
    "\n",
    "    # --- Class Mapping (for Classifier) ---\n",
    "    CLASS_MAP: Dict[str, int] = {\n",
    "        '1': 0, '2': 1, '3': 2, '4': 3, '5': 4, '6': 5, '7': 6, '8': 7,\n",
    "        '9': 8, '10': 9, '11': 10, '12': 11, '13': 12, '14': 13, '15': 14,\n",
    "        '16': 15, '17': 16, '18': 17, '19': 18, '20': 19, '21': 20, '22': 21,\n",
    "        'X': 22, 'Y': 23\n",
    "    }\n",
    "    CLASS_NAMES: List[str] = [name for name, _ in sorted(CLASS_MAP.items(), key=lambda item: item[1])]\n",
    "    NUM_CLASSES = len(CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 🛠️ CELL 2: UTILITIES & DATA VISUALIZATION\n",
    "# ==============================================================================\n",
    "\n",
    "class AnnotationParser:\n",
    "    \"\"\"Parses XML annotation files.\"\"\"\n",
    "    @staticmethod\n",
    "    def parse_xml(xml_path: Path) -> Dict[str, Any]:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        annotations = {'objects': []}\n",
    "        size_node = root.find('size')\n",
    "        if size_node:\n",
    "            annotations['width'] = int(size_node.find('width').text)\n",
    "            annotations['height'] = int(size_node.find('height').text)\n",
    "        \n",
    "        for obj in root.findall('object'):\n",
    "            bndbox = obj.find('bndbox')\n",
    "            if bndbox:\n",
    "                annotations['objects'].append({\n",
    "                    'name': obj.find('name').text,\n",
    "                    'bbox': [\n",
    "                        int(float(bndbox.find('xmin').text)), int(float(bndbox.find('ymin').text)),\n",
    "                        int(float(bndbox.find('xmax').text)), int(float(bndbox.find('ymax').text))\n",
    "                    ]\n",
    "                })\n",
    "        return annotations\n",
    "\n",
    "def compute_iou(boxA: List[int], boxB: List[int]) -> float:\n",
    "    xA = max(boxA[0], boxB[0]); yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2]); yB = min(boxA[3], boxB[3])\n",
    "    inter_area = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxA_area = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxB_area = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    union_area = float(boxA_area + boxB_area - inter_area)\n",
    "    return inter_area / union_area if union_area > 0 else 0.0\n",
    "\n",
    "def visualize_ground_truth_examples(image_dir: Path, annotation_dir: Path, num_examples: int = 4):\n",
    "    \"\"\"Plots a few images with their ground truth bounding boxes.\"\"\"\n",
    "    print(\"Displaying ground truth examples...\")\n",
    "    image_files = list(image_dir.glob('*.jpg')) + list(image_dir.glob('*.png'))\n",
    "    plt.figure(figsize=(15, 5 * num_examples // 2))\n",
    "    \n",
    "    for i, img_path in enumerate(np.random.choice(image_files, num_examples, replace=False)):\n",
    "        xml_path = annotation_dir / (img_path.stem + \".xml\")\n",
    "        if not xml_path.exists(): continue\n",
    "            \n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        annotations = AnnotationParser.parse_xml(xml_path)\n",
    "        \n",
    "        ax = plt.subplot(num_examples // 2, 2, i + 1)\n",
    "        ax.imshow(image)\n",
    "        ax.set_title(img_path.name)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        for obj in annotations['objects']:\n",
    "            xmin, ymin, xmax, ymax = obj['bbox']\n",
    "            rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=2, edgecolor='lime', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_history(history: keras.callbacks.History, save_path: Path):\n",
    "    pd.DataFrame(history.history).plot(figsize=(12, 8))\n",
    "    plt.grid(True); plt.gca().set_ylim(0, 1)\n",
    "    plt.title(\"Classifier Training History\"); plt.xlabel(\"Epoch\")\n",
    "    plt.savefig(save_path); plt.close()\n",
    "    print(f\"✅ Classifier training history saved to '{save_path}'\")\n",
    "\n",
    "def visualize_predictions(image_path: Path, gt_boxes: List, predictions: List, save_path: Path):\n",
    "    image = cv2.imread(str(image_path)); image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    fig, ax = plt.subplots(1, figsize=(15, 15)); ax.imshow(image); ax.axis('off')\n",
    "    for gt in gt_boxes:\n",
    "        xmin, ymin, xmax, ymax = gt['bbox']\n",
    "        rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=2, edgecolor='g', facecolor='none')\n",
    "        ax.add_patch(rect); ax.text(xmin, ymin - 10, f\"GT: {gt['name']}\", color='g', fontsize=12, weight='bold')\n",
    "    for pred in predictions:\n",
    "        xmin, ymin, xmax, ymax = map(int, pred['bbox'])\n",
    "        rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect); label = f\"Pred: {pred['class_name']} ({pred['combined_score']:.2f})\"\n",
    "        ax.text(xmin, ymax + 20, label, color='r', fontsize=12, weight='bold')\n",
    "    plt.tight_layout(); plt.savefig(save_path, bbox_inches='tight', pad_inches=0.1); plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 🎯 CELL 3: STEP 1 - TRAIN THE YOLO IDENTIFIER (DETECTOR)\n",
    "# ==============================================================================\n",
    "\n",
    "def prepare_yolo_data(image_dir: Path, annotation_dir: Path, output_dir: Path, val_split: float):\n",
    "    \"\"\"Converts XML annotations to YOLO format and creates data.yaml.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n📦 PREPARING YOLO DATASET\\n\" + \"=\"*60)\n",
    "    if output_dir.exists():\n",
    "        shutil.rmtree(output_dir)\n",
    "\n",
    "    dir_paths = { 'train_images': output_dir / 'images' / 'train', 'val_images': output_dir / 'images' / 'val',\n",
    "                  'train_labels': output_dir / 'labels' / 'train', 'val_labels': output_dir / 'labels' / 'val' }\n",
    "    for path in dir_paths.values(): path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    image_stems = sorted([p.stem for p in image_dir.glob('*.jpg')] + [p.stem for p in image_dir.glob('*.png')])\n",
    "    train_stems, val_stems = train_test_split(image_stems, test_size=val_split, random_state=42)\n",
    "    stem_map = {'train': train_stems, 'val': val_stems}\n",
    "\n",
    "    for split, stems in stem_map.items():\n",
    "        for stem in stems:\n",
    "            xml_path = annotation_dir / f\"{stem}.xml\"\n",
    "            img_path = (image_dir / f\"{stem}.jpg\") or (image_dir / f\"{stem}.png\")\n",
    "            if not xml_path.exists() or not img_path.exists(): continue\n",
    "            \n",
    "            shutil.copy(img_path, dir_paths[f'{split}_images'])\n",
    "            ann = AnnotationParser.parse_xml(xml_path)\n",
    "            img_w, img_h = ann['width'], ann['height']\n",
    "            yolo_lines = []\n",
    "            for obj in ann['objects']:\n",
    "                class_id = 0 # Single class \"chromosome\" for detector\n",
    "                xmin, ymin, xmax, ymax = obj['bbox']\n",
    "                x_center, width = ((xmin + xmax) / 2 / img_w, (xmax - xmin) / img_w)\n",
    "                y_center, height = ((ymin + ymax) / 2 / img_h, (ymax - ymin) / img_h)\n",
    "                yolo_lines.append(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\")\n",
    "            \n",
    "            with open(dir_paths[f'{split}_labels'] / f\"{stem}.txt\", 'w') as f:\n",
    "                f.write(\"\\n\".join(yolo_lines))\n",
    "    \n",
    "    with open(output_dir / 'data.yaml', 'w') as f:\n",
    "        yaml.dump({ 'train': str(dir_paths['train_images'].resolve()), 'val': str(dir_paths['val_images'].resolve()),\n",
    "                     'nc': 1, 'names': ['chromosome']}, f, default_flow_style=False)\n",
    "    print(f\"✅ YOLO dataset created at '{output_dir.resolve()}'\")\n",
    "\n",
    "class YOLOIdentifier:\n",
    "    \"\"\"Wrapper for the YOLOv8 chromosome detection model.\"\"\"\n",
    "    def __init__(self, model_name: str = 'yolov8n.pt'):\n",
    "        self.model = YOLO(model_name)\n",
    "    def train(self, data_config_path: str, epochs: int, img_size: int, project: str, name: str):\n",
    "        self.model.train(data=data_config_path, epochs=epochs, imgsz=img_size,\n",
    "                         project=project, name=name, exist_ok=True)\n",
    "    def load_weights(self, weights_path: str):\n",
    "        self.model = YOLO(weights_path)\n",
    "    def predict_boxes(self, image: np.ndarray, confidence_threshold: float):\n",
    "        results = self.model.predict(image, conf=confidence_threshold, verbose=False)\n",
    "        if not results: return np.array([]), np.array([])\n",
    "        return results[0].boxes.xyxy.cpu().numpy(), results[0].boxes.conf.cpu().numpy()\n",
    "\n",
    "# --- Main execution for Identifier Training ---\n",
    "if Config.TRAIN_IDENTIFIER:\n",
    "    data_root = Path(Config.DATA_ROOT)\n",
    "    single_chr_images = data_root / 'single_chromosomes_object' / 'images'\n",
    "    single_chr_annotations = data_root / 'single_chromosomes_object' / 'annotations'\n",
    "\n",
    "    # Visualize Ground Truth Data Before Training\n",
    "    visualize_ground_truth_examples(single_chr_images, single_chr_annotations, num_examples=4)\n",
    "    \n",
    "    # Prepare data and train\n",
    "    prepare_yolo_data(single_chr_images, single_chr_annotations,\n",
    "                      Path(Config.YOLO_DATA_DIR), Config.VALIDATION_SPLIT)\n",
    "    \n",
    "    print(\"\\n🚀 STARTING YOLO IDENTIFIER TRAINING...\")\n",
    "    identifier_trainer = YOLOIdentifier(model_name=Config.YOLO_MODEL_NAME)\n",
    "    identifier_trainer.train(data_config_path=Config.YOLO_DATA_CONFIG, epochs=Config.IDENTIFIER_EPOCHS,\n",
    "                             img_size=Config.IDENTIFIER_IMG_SIZE, project=Config.YOLO_PROJECT_NAME,\n",
    "                             name=Config.YOLO_RUN_NAME)\n",
    "    print(\"\\n✅ YOLO Identifier training complete.\")\n",
    "else:\n",
    "    print(\"☑️ Skipping Identifier training as per Config.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 🏷️ CELL 4: STEP 2 - TRAIN THE TENSORFLOW CLASSIFIER\n",
    "# ==============================================================================\n",
    "\n",
    "class ChromosomeClassifierDataGenerator(keras.utils.Sequence):\n",
    "    \"\"\"Memory-efficient data generator for the classifier.\"\"\"\n",
    "    def __init__(self, images_dir: Path, annotations_dir: Path, batch_size: int, img_size: int,\n",
    "                 class_map: Dict, is_validation: bool = False, augment: bool = False,\n",
    "                 validation_split: float = 0.2, data_list=None, indexes=None):\n",
    "        self.images_dir, self.annotations_dir = images_dir, annotations_dir\n",
    "        self.batch_size, self.img_size = batch_size, img_size\n",
    "        self.class_map, self.is_validation = class_map, is_validation\n",
    "        if data_list is None:\n",
    "            self.chromosome_data = self._build_dataset_index()\n",
    "            all_indexes = np.arange(len(self.chromosome_data))\n",
    "            np.random.seed(42); np.random.shuffle(all_indexes)\n",
    "            val_size = int(len(self.chromosome_data) * validation_split)\n",
    "            self.val_indexes = all_indexes[:val_size]\n",
    "            self.train_indexes = all_indexes[val_size:]\n",
    "            self.indexes = self.val_indexes if is_validation else self.train_indexes\n",
    "        else:\n",
    "            self.chromosome_data, self.indexes = data_list, indexes\n",
    "        self.aug = A.Compose([A.HorizontalFlip(), A.VerticalFlip(), A.Rotate(limit=20),\n",
    "                              A.RandomBrightnessContrast(p=0.3), A.GaussNoise(p=0.2)]) if augment else None\n",
    "    def _build_dataset_index(self):\n",
    "        print(\"\\nBuilding classifier dataset index...\"); index = []\n",
    "        image_files = sorted([p.stem for p in self.images_dir.glob('*.jpg')] + [p.stem for p in self.images_dir.glob('*.png')])\n",
    "        for name in image_files:\n",
    "            xml_path = self.annotations_dir / f\"{name}.xml\"\n",
    "            if not xml_path.exists(): continue\n",
    "            for obj in AnnotationParser.parse_xml(xml_path)['objects']:\n",
    "                if obj['name'] in self.class_map:\n",
    "                    index.append({'img_name': name, 'bbox': obj['bbox'], 'label': self.class_map[obj['name']]})\n",
    "        print(f\"✅ Found {len(index)} chromosome samples.\")\n",
    "        return index\n",
    "    def get_validation_generator(self):\n",
    "        return ChromosomeClassifierDataGenerator(self.images_dir, self.annotations_dir, self.batch_size,\n",
    "                                             self.img_size, self.class_map, is_validation=True,\n",
    "                                             data_list=self.chromosome_data, indexes=self.val_indexes)\n",
    "    def __len__(self): return int(np.ceil(len(self.indexes) / self.batch_size))\n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        batch_images, batch_labels = [], []\n",
    "        for idx in batch_indexes:\n",
    "            item = self.chromosome_data[idx]\n",
    "            path = (self.images_dir / f\"{item['img_name']}.jpg\") or (self.images_dir / f\"{item['img_name']}.png\")\n",
    "            image = cv2.imread(str(path))\n",
    "            if image is None: continue\n",
    "            xmin, ymin, xmax, ymax = item['bbox']\n",
    "            crop = image[ymin:ymax, xmin:xmax]\n",
    "            if crop.size == 0: continue\n",
    "            if self.aug: crop = self.aug(image=crop)['image']\n",
    "            crop = cv2.resize(crop, (self.img_size, self.img_size))\n",
    "            norm_crop = (crop.astype(np.float32) / 255.0 - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
    "            batch_images.append(norm_crop); batch_labels.append(item['label'])\n",
    "        return np.array(batch_images), np.array(batch_labels)\n",
    "\n",
    "class ChromosomeClassifier:\n",
    "    def __init__(self, num_classes: int, img_size: int):\n",
    "        self.num_classes, self.img_size = num_classes, img_size\n",
    "        self.model = None\n",
    "    def _build_model(self):\n",
    "        base = ResNet50(include_top=False, weights='imagenet', input_shape=(self.img_size, self.img_size, 3))\n",
    "        for layer in base.layers[:140]: layer.trainable = False\n",
    "        x = layers.GlobalAveragePooling2D()(base.output)\n",
    "        x = layers.Dense(512, activation='relu')(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "        return models.Model(inputs=base.input, outputs=outputs)\n",
    "    def compile_model(self, lr: float, strategy: tf.distribute.Strategy):\n",
    "        with strategy.scope():\n",
    "            self.model = self._build_model()\n",
    "            self.model.compile(optimizer=optimizers.Adam(lr), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    def train(self, train_gen, val_gen, epochs: int, save_path: str):\n",
    "        callbacks = [ keras.callbacks.ReduceLROnPlateau(patience=3, min_lr=1e-7),\n",
    "                      keras.callbacks.ModelCheckpoint(save_path, save_best_only=True),\n",
    "                      keras.callbacks.EarlyStopping(patience=7, restore_best_weights=True) ]\n",
    "        return self.model.fit(train_gen, validation_data=val_gen, epochs=epochs, callbacks=callbacks)\n",
    "\n",
    "# --- Main execution for Classifier Training ---\n",
    "if Config.TRAIN_CLASSIFIER:\n",
    "    STRATEGY, DEVICE_TYPE = setup_device_strategy()\n",
    "    data_root = Path(Config.DATA_ROOT)\n",
    "    multi_chr_images = data_root / '24_chromosomes_object' / 'images'\n",
    "    multi_chr_annotations = data_root / '24_chromosomes_object' / 'annotations'\n",
    "    \n",
    "    cls_batch = 64 * STRATEGY.num_replicas_in_sync\n",
    "    train_gen = ChromosomeClassifierDataGenerator(\n",
    "        multi_chr_images, multi_chr_annotations, cls_batch, Config.CLASSIFIER_IMG_SIZE,\n",
    "        Config.CLASS_MAP, augment=True, validation_split=Config.VALIDATION_SPLIT)\n",
    "    val_gen = train_gen.get_validation_generator()\n",
    "    \n",
    "    print(\"\\n🚀 STARTING CLASSIFIER TRAINING...\")\n",
    "    classifier = ChromosomeClassifier(num_classes=Config.NUM_CLASSES, img_size=Config.CLASSIFIER_IMG_SIZE)\n",
    "    classifier.compile_model(lr=0.001, strategy=STRATEGY)\n",
    "    history = classifier.train(train_gen, val_gen, Config.CLASSIFIER_EPOCHS, Config.CLASSIFIER_MODEL_PATH)\n",
    "    \n",
    "    output_dir = Path(Config.OUTPUT_DIR); output_dir.mkdir(exist_ok=True)\n",
    "    plot_training_history(history, output_dir / 'classifier_training_history.png')\n",
    "    print(f\"\\n✅ Classifier training complete. Model saved to {Config.CLASSIFIER_MODEL_PATH}\")\n",
    "else:\n",
    "    print(\"☑️ Skipping Classifier training as per Config.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 🔎 CELL 5: PREDICTION PIPELINE & VISUALIZATION\n",
    "# ==============================================================================\n",
    "\n",
    "class KaryotypePipeline:\n",
    "    def __init__(self, identifier, classifier):\n",
    "        self.identifier = identifier\n",
    "        self.classifier = classifier\n",
    "        self.classifier_img_size = classifier.input_shape[1]\n",
    "\n",
    "    def process_image(self, image_path: Path, detection_threshold: float):\n",
    "        image = cv2.imread(str(image_path))\n",
    "        if image is None: return []\n",
    "        \n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        boxes, scores = self.identifier.predict_boxes(image_rgb, detection_threshold)\n",
    "        if len(boxes) == 0: return []\n",
    "        \n",
    "        predictions = []\n",
    "        for i, box in enumerate(boxes):\n",
    "            xmin, ymin, xmax, ymax = map(int, box)\n",
    "            if xmax <= xmin or ymax <= ymin: continue\n",
    "            crop = image_rgb[ymin:ymax, xmin:xmax]\n",
    "            if crop.size == 0: continue\n",
    "            \n",
    "            resized_crop = cv2.resize(crop, (self.classifier_img_size, self.classifier_img_size))\n",
    "            norm_crop = (resized_crop.astype(np.float32) / 255.0 - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
    "            norm_crop = np.expand_dims(norm_crop, axis=0)\n",
    "            \n",
    "            probs = self.classifier.predict(norm_crop, verbose=0)[0]\n",
    "            class_id = np.argmax(probs)\n",
    "            \n",
    "            predictions.append({\n",
    "                'bbox': box.tolist(), 'class_id': int(class_id),\n",
    "                'class_name': Config.CLASS_NAMES[class_id],\n",
    "                'combined_score': float(scores[i]) * probs[class_id],\n",
    "                'probs': probs \n",
    "            })\n",
    "        return predictions\n",
    "\n",
    "# --- Main execution for Prediction ---\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n📊 RUNNING PREDICTION ON TEST SET\\n\" + \"=\"*60)\n",
    "\n",
    "# 1. Load trained models\n",
    "print(\"Loading trained models...\")\n",
    "identifier = YOLOIdentifier()\n",
    "identifier.load_weights(Config.YOLO_MODEL_PATH)\n",
    "classifier_model = keras.models.load_model(Config.CLASSIFIER_MODEL_PATH)\n",
    "print(\"✅ Models loaded successfully.\")\n",
    "\n",
    "# 2. Setup pipeline and paths\n",
    "pipeline = KaryotypePipeline(identifier, classifier_model)\n",
    "data_root = Path(Config.DATA_ROOT)\n",
    "test_file = data_root / 'test.txt'\n",
    "multi_chr_images = data_root / '24_chromosomes_object' / 'images'\n",
    "multi_chr_annotations = data_root / '24_chromosomes_object' / 'annotations'\n",
    "output_dir = Path(Config.OUTPUT_DIR); output_dir.mkdir(exist_ok=True)\n",
    "vis_dir = output_dir / 'visualizations'; vis_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 3. Process test set and store results for evaluation\n",
    "with open(test_file, 'r') as f:\n",
    "    test_list = [line.strip() for line in f.readlines()]\n",
    "\n",
    "all_gt_labels = []\n",
    "all_pred_labels = []\n",
    "all_pred_scores = [] # For ROC/PRC\n",
    "\n",
    "for i, img_name in enumerate(test_list):\n",
    "    print(f\"Processing image {i+1}/{len(test_list)}: {img_name}\", end='\\r')\n",
    "    img_path = (multi_chr_images / f\"{img_name}.jpg\") or (multi_chr_images / f\"{img_name}.png\")\n",
    "    xml_path = multi_chr_annotations / f\"{img_name}.xml\"\n",
    "    if not img_path.exists() or not xml_path.exists(): continue\n",
    "    \n",
    "    predictions = pipeline.process_image(img_path, Config.DETECTION_THRESHOLD)\n",
    "    gt_annotations = AnnotationParser.parse_xml(xml_path)\n",
    "    gt_boxes = [obj for obj in gt_annotations['objects'] if obj['name'] in Config.CLASS_MAP]\n",
    "\n",
    "    # Match predictions to ground truth\n",
    "    for gt_box_info in gt_boxes:\n",
    "        gt_bbox = gt_box_info['bbox']\n",
    "        gt_label = Config.CLASS_MAP[gt_box_info['name']]\n",
    "        best_iou, best_pred = -1, None\n",
    "\n",
    "        for pred in predictions:\n",
    "            iou = compute_iou(gt_bbox, pred['bbox'])\n",
    "            if iou > best_iou:\n",
    "                best_iou, best_pred = iou, pred\n",
    "        \n",
    "        all_gt_labels.append(gt_label)\n",
    "        if best_iou >= Config.EVAL_IOU_THRESHOLD:\n",
    "            all_pred_labels.append(best_pred['class_id'])\n",
    "            all_pred_scores.append(best_pred['probs'])\n",
    "        else:\n",
    "            all_pred_labels.append(-1) # Missed detection\n",
    "            all_pred_scores.append(np.zeros(Config.NUM_CLASSES))\n",
    "\n",
    "    # Visualize first few predictions\n",
    "    if i < Config.NUM_VISUALIZATION_IMAGES:\n",
    "        vis_save_path = vis_dir / f\"{img_name}_pred.png\"\n",
    "        visualize_predictions(img_path, gt_boxes, predictions, vis_save_path)\n",
    "\n",
    "print(f\"\\n✅ Prediction complete. Visualizations saved to '{vis_dir}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 📈 CELL 6: PERFORMANCE EVALUATION METRICS\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_roc_curves(y_true_bin, y_pred_scores, class_names, save_path):\n",
    "    \"\"\"Plots multiclass ROC curves and calculates AUC.\"\"\"\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "    n_classes = len(class_names)\n",
    "\n",
    "    # Compute ROC for each class\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_scores[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_pred_scores.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"], label=f'Micro-average ROC (AUC = {roc_auc[\"micro\"]:.2f})', color='deeppink', linestyle=':', linewidth=4)\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        if i < 5: # Plot first 5 classes for clarity\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'ROC curve of class {class_names[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
    "    plt.legend(loc=\"lower right\"); plt.grid(True)\n",
    "    plt.savefig(save_path); plt.show()\n",
    "    print(f\"✅ ROC curves saved to '{save_path}'\")\n",
    "\n",
    "def plot_prc_curves(y_true_bin, y_pred_scores, class_names, save_path):\n",
    "    \"\"\"Plots multiclass Precision-Recall curves and calculates Average Precision.\"\"\"\n",
    "    precision, recall, avg_precision = {}, {}, {}\n",
    "    n_classes = len(class_names)\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_true_bin[:, i], y_pred_scores[:, i])\n",
    "        avg_precision[i] = average_precision_score(y_true_bin[:, i], y_pred_scores[:, i])\n",
    "\n",
    "    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_true_bin.ravel(), y_pred_scores.ravel())\n",
    "    avg_precision[\"micro\"] = average_precision_score(y_true_bin, y_pred_scores, average=\"micro\")\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.plot(recall[\"micro\"], precision[\"micro\"], label=f'Micro-average PRC (AP = {avg_precision[\"micro\"]:.2f})', color='navy', linestyle=':', linewidth=4)\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "         if i < 5: # Plot first 5 classes for clarity\n",
    "            plt.plot(recall[i], precision[i], color=color, lw=2, label=f'PRC of class {class_names[i]} (AP = {avg_precision[i]:.2f})')\n",
    "\n",
    "    plt.xlabel('Recall'); plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curves (PRC)')\n",
    "    plt.legend(loc=\"best\"); plt.grid(True)\n",
    "    plt.savefig(save_path); plt.show()\n",
    "    print(f\"✅ PRC curves saved to '{save_path}'\")\n",
    "\n",
    "# --- Main execution for Evaluation ---\n",
    "print(\"\\n\" + \"=\"*60 + \"\\nEVALUATION RESULTS\\n\" + \"=\"*60)\n",
    "output_dir = Path(Config.OUTPUT_DIR)\n",
    "\n",
    "# Filter out missed detections for metrics\n",
    "y_true_eval = np.array([gt for i, gt in enumerate(all_gt_labels) if all_pred_labels[i] != -1])\n",
    "y_pred_eval = np.array([p for p in all_pred_labels if p != -1])\n",
    "y_scores_eval = np.array([s for i, s in enumerate(all_pred_scores) if all_pred_labels[i] != -1])\n",
    "\n",
    "if len(y_true_eval) > 0:\n",
    "    # 1. Classification Report\n",
    "    print(\"Classification Report (on matched boxes):\\n\")\n",
    "    report = classification_report(y_true_eval, y_pred_eval, target_names=Config.CLASS_NAMES, zero_division=0)\n",
    "    print(report)\n",
    "\n",
    "    # 2. Confusion Matrix\n",
    "    plot_confusion_matrix(y_true_eval, y_pred_eval, Config.CLASS_NAMES, output_dir / 'confusion_matrix.png')\n",
    "    \n",
    "    # Binarize labels for ROC/PRC\n",
    "    y_true_bin = label_binarize(y_true_eval, classes=range(Config.NUM_CLASSES))\n",
    "\n",
    "    # 3. ROC Curve\n",
    "    plot_roc_curves(y_true_bin, y_scores_eval, Config.CLASS_NAMES, output_dir / 'roc_curves.png')\n",
    "\n",
    "    # 4. PRC Curve\n",
    "    plot_prc_curves(y_true_bin, y_scores_eval, Config.CLASS_NAMES, output_dir / 'prc_curves.png')\n",
    "else:\n",
    "    print(\"No valid matched boxes found to generate an evaluation report.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8486113,
     "sourceId": 13375782,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "aiml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
