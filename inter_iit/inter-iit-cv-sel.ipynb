{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Configuration Guide\n",
    "\n",
    "### Configuration Flags (Set at the top of script)\n",
    "\n",
    "```python\n",
    "TRAIN_IDENTIFIER = True   # Train identifier? (False = load existing)\n",
    "TRAIN_CLASSIFIER = True   # Train classifier? (False = load existing)\n",
    "RUN_EVALUATION = True     # Run evaluation?\n",
    "\n",
    "RESUME_IDENTIFIER = False # Continue identifier training from saved weights?\n",
    "RESUME_CLASSIFIER = False # Continue classifier training from saved weights?\n",
    "\n",
    "IDENTIFIER_EPOCHS = 15    # How many epochs for identifier\n",
    "CLASSIFIER_EPOCHS = 50    # How many epochs for classifier\n",
    "```\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "- **`RESUME = True`** only works when **`TRAIN = True`**\n",
    "- Models save automatically: `identifier_model.h5` and `best_classifier.h5`\n",
    "- If model files missing and `TRAIN = False`, script will error\n",
    "- **Resume = Continue training** from saved weights (not start over)\n",
    "\n",
    "\n",
    "### Quick Decision Tree\n",
    "\n",
    "- **First time running?** → All `TRAIN = True`, All `RESUME = False`\n",
    "- **Kaggle timed out?** → All `TRAIN = True`, All `RESUME = True`\n",
    "- **Just want results?** → All `TRAIN = False`, `RUN_EVALUATION = True`\n",
    "- **Only classifier bad?** → `TRAIN_CLASSIFIER = True`, `TRAIN_IDENTIFIER = False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-14T17:43:02.278662Z",
     "iopub.status.busy": "2025-10-14T17:43:02.277956Z",
     "iopub.status.idle": "2025-10-14T17:43:02.291188Z",
     "shell.execute_reply": "2025-10-14T17:43:02.290347Z",
     "shell.execute_reply.started": "2025-10-14T17:43:02.278633Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# COMPLETE CHROMOSOME CLASSIFICATION PIPELINE - TPU v5e-8 OPTIMIZED\n",
    "# Features:\n",
    "# - TPU v5e-8 optimized batch sizes\n",
    "# - Robust error handling\n",
    "# - Resume training capability\n",
    "# - Complete end-to-end pipeline\n",
    "#\n",
    "# TPU v5e-8 Setup (Kaggle):\n",
    "# 1. Go to notebook settings (right sidebar)\n",
    "# 2. Set Accelerator to \"TPU v5e-8\"\n",
    "# 3. Run this script - TPU will be auto-detected\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, auc, confusion_matrix, classification_report,\n",
    "    roc_curve, precision_recall_fscore_support, accuracy_score, \n",
    "    balanced_accuracy_score\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict\n",
    "from itertools import cycle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Install albumentations if not available\n",
    "try:\n",
    "    import albumentations as A\n",
    "    print(\"✓ Albumentations loaded successfully\")\n",
    "except ImportError:\n",
    "    print(\"Installing albumentations...\")\n",
    "    os.system('pip install -q albumentations')\n",
    "    import albumentations as A\n",
    "    print(\"✓ Albumentations installed and loaded\")\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURATION - SET THESE BEFORE RUNNING\n",
    "# ==============================================================================\n",
    "\n",
    "# Training control flags\n",
    "TRAIN_IDENTIFIER = True  # Set to False to skip identifier training\n",
    "TRAIN_CLASSIFIER = True  # Set to False to skip classifier training\n",
    "RUN_EVALUATION = True    # Set to False to skip evaluation\n",
    "\n",
    "# Resume training flags\n",
    "RESUME_IDENTIFIER = False  # Set to True to resume from saved identifier weights\n",
    "RESUME_CLASSIFIER = False  # Set to True to resume from saved classifier weights\n",
    "\n",
    "# Model paths\n",
    "IDENTIFIER_MODEL_PATH = 'identifier_model.h5'\n",
    "CLASSIFIER_MODEL_PATH = 'best_classifier.h5'\n",
    "\n",
    "# Training epochs\n",
    "IDENTIFIER_EPOCHS = 15\n",
    "CLASSIFIER_EPOCHS = 50\n",
    "\n",
    "# Data paths (adjust if needed)\n",
    "DATA_ROOT = '/kaggle/input/2025-karyogram-cv-camp/2025_Karyogram_CV_Camp'\n",
    "\n",
    "# ==============================================================================\n",
    "# DEVICE SETUP - TPU v5e-8 OPTIMIZED\n",
    "# ==============================================================================\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def setup_device_strategy():\n",
    "    \"\"\"\n",
    "    Setup device strategy with fallback: TPU -> GPU -> CPU\n",
    "    \n",
    "    TPU v5e-8 Setup (Kaggle):\n",
    "    - 8 cores with improved performance over v3-8\n",
    "    - Better memory efficiency\n",
    "    - Recommended batch sizes per core:\n",
    "      * Identifier: 4-8 per core\n",
    "      * Classifier: 128-256 per core\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"DEVICE SETUP\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ==================== TPU DETECTION ====================\n",
    "    try:\n",
    "        print(\"Attempting TPU detection...\")\n",
    "        \n",
    "        # Step 1: Detect and init the TPU\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        \n",
    "        # Step 2: Initialize TPU system\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        \n",
    "        # Step 3: Instantiate distribution strategy\n",
    "        strategy = tf.distribute.TPUStrategy(tpu)\n",
    "        device_type = \"TPU\"\n",
    "        \n",
    "        print(f\"✓ TPU detected and initialized\")\n",
    "        print(f\"✓ TPU cores: {strategy.num_replicas_in_sync}\")\n",
    "        print(f\"✓ TPU type: v5e-8 (optimized)\")\n",
    "        print(f\"✓ Recommended batch sizes:\")\n",
    "        print(f\"  - Identifier: {4 * strategy.num_replicas_in_sync}-{8 * strategy.num_replicas_in_sync} total (4-8 per core)\")\n",
    "        print(f\"  - Classifier: {128 * strategy.num_replicas_in_sync}-{256 * strategy.num_replicas_in_sync} total (128-256 per core)\")\n",
    "        \n",
    "        return strategy, device_type\n",
    "        \n",
    "    except (ValueError, tf.errors.NotFoundError, Exception) as e:\n",
    "        print(f\"✗ No TPU detected: {type(e).__name__}\")\n",
    "    \n",
    "    # ==================== GPU DETECTION ====================\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            \n",
    "            print(f\"✓ GPU(s) detected: {len(gpus)} device(s)\")\n",
    "            for i, gpu in enumerate(gpus):\n",
    "                print(f\"  GPU {i}: {gpu.name}\")\n",
    "            \n",
    "            if len(gpus) > 1:\n",
    "                strategy = tf.distribute.MirroredStrategy()\n",
    "                device_type = \"Multi-GPU\"\n",
    "                print(f\"✓ Using MirroredStrategy with {strategy.num_replicas_in_sync} GPUs\")\n",
    "            else:\n",
    "                strategy = tf.distribute.get_strategy()\n",
    "                device_type = \"Single-GPU\"\n",
    "                print(\"✓ Using single GPU\")\n",
    "            \n",
    "            return strategy, device_type\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(f\"✗ GPU setup failed: {e}\")\n",
    "    else:\n",
    "        print(\"✗ No GPU detected\")\n",
    "    \n",
    "    # ==================== CPU FALLBACK ====================\n",
    "    print(\"✓ Falling back to CPU\")\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    device_type = \"CPU\"\n",
    "    print(\"⚠ Warning: Training on CPU will be significantly slower\")\n",
    "    print(\"  Consider enabling GPU or TPU in Kaggle notebook settings\")\n",
    "    \n",
    "    return strategy, device_type\n",
    "\n",
    "STRATEGY, DEVICE_TYPE = setup_device_strategy()\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"CONFIGURATION: {DEVICE_TYPE}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Devices: {STRATEGY.num_replicas_in_sync}\")\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# SHARED UTILITIES\n",
    "# ==============================================================================\n",
    "\n",
    "class AnnotationParser:\n",
    "    \"\"\"Parse XML annotations\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_xml(xml_path: str) -> Dict:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        annotations = {\n",
    "            'filename': root.find('filename').text if root.find('filename') is not None else '',\n",
    "            'size': {},\n",
    "            'objects': []\n",
    "        }\n",
    "\n",
    "        size = root.find('size')\n",
    "        if size is not None:\n",
    "            annotations['size'] = {\n",
    "                'width': int(size.find('width').text),\n",
    "                'height': int(size.find('height').text),\n",
    "                'depth': int(size.find('depth').text) if size.find('depth') is not None else 3\n",
    "            }\n",
    "\n",
    "        for obj in root.findall('object'):\n",
    "            name = obj.find('name').text\n",
    "            bndbox = obj.find('bndbox')\n",
    "\n",
    "            if bndbox is not None:\n",
    "                bbox = {\n",
    "                    'xmin': int(float(bndbox.find('xmin').text)),\n",
    "                    'ymin': int(float(bndbox.find('ymin').text)),\n",
    "                    'xmax': int(float(bndbox.find('xmax').text)),\n",
    "                    'ymax': int(float(bndbox.find('ymax').text))\n",
    "                }\n",
    "\n",
    "                annotations['objects'].append({\n",
    "                    'name': name,\n",
    "                    'bbox': bbox\n",
    "                })\n",
    "\n",
    "        return annotations\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# IDENTIFIER MODULE\n",
    "# ==============================================================================\n",
    "\n",
    "class ChromosomeIdentifierDataGenerator(keras.utils.Sequence):\n",
    "    \"\"\"Data generator for training chromosome identifier\"\"\"\n",
    "\n",
    "    def __init__(self, images_dir: str, annotations_dir: str,\n",
    "                 batch_size=2, img_size=800, shuffle=True):\n",
    "        self.images_dir = images_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.shuffle = shuffle\n",
    "        self.parser = AnnotationParser()\n",
    "\n",
    "        self.image_list = []\n",
    "        for filename in os.listdir(images_dir):\n",
    "            if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "                base_name = os.path.splitext(filename)[0]\n",
    "                xml_path = os.path.join(annotations_dir, f\"{base_name}.xml\")\n",
    "                if os.path.exists(xml_path):\n",
    "                    self.image_list.append(base_name)\n",
    "\n",
    "        if len(self.image_list) == 0:\n",
    "            raise ValueError(f\"No valid image-annotation pairs found in {images_dir}\")\n",
    "\n",
    "        print(f\"Found {len(self.image_list)} images for identifier\")\n",
    "\n",
    "        self.indexes = np.arange(len(self.image_list))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_list) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        batch_images = [self.image_list[k] for k in batch_indexes]\n",
    "        X, y = self._generate_data(batch_images)\n",
    "        \n",
    "        # Handle empty batches by trying next batch\n",
    "        if len(X) == 0 and index < len(self) - 1:\n",
    "            return self.__getitem__((index + 1) % len(self))\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def _generate_data(self, batch_images):\n",
    "        images = []\n",
    "        targets = []\n",
    "\n",
    "        for img_name in batch_images:\n",
    "            img_path = os.path.join(self.images_dir, f\"{img_name}.jpg\")\n",
    "            xml_path = os.path.join(self.annotations_dir, f\"{img_name}.xml\")\n",
    "\n",
    "            if not os.path.exists(img_path):\n",
    "                img_path = os.path.join(self.images_dir, f\"{img_name}.png\")\n",
    "\n",
    "            if not os.path.exists(img_path) or not os.path.exists(xml_path):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                image = cv2.imread(img_path)\n",
    "                if image is None:\n",
    "                    continue\n",
    "                    \n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                orig_h, orig_w = image.shape[:2]\n",
    "\n",
    "                image = cv2.resize(image, (self.img_size, self.img_size))\n",
    "                image = image.astype(np.float32) / 255.0\n",
    "\n",
    "                annotations = self.parser.parse_xml(xml_path)\n",
    "\n",
    "                boxes = []\n",
    "                for obj in annotations['objects']:\n",
    "                    bbox = obj['bbox']\n",
    "                    xmin = bbox['xmin'] / orig_w\n",
    "                    ymin = bbox['ymin'] / orig_h\n",
    "                    xmax = bbox['xmax'] / orig_w\n",
    "                    ymax = bbox['ymax'] / orig_h\n",
    "                    boxes.append([ymin, xmin, ymax, xmax])\n",
    "\n",
    "                images.append(image)\n",
    "                targets.append(np.array(boxes) if len(boxes) > 0 else np.zeros((0, 4)))\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error processing {img_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "        return images, targets\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "\n",
    "class ChromosomeIdentifier:\n",
    "    \"\"\"Chromosome detection model\"\"\"\n",
    "\n",
    "    def __init__(self, img_size=800, max_detections=50):\n",
    "        self.img_size = img_size\n",
    "        self.max_detections = max_detections\n",
    "        self.model = None\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"Build model\"\"\"\n",
    "        inputs = layers.Input(shape=(self.img_size, self.img_size, 3))\n",
    "\n",
    "        base_model = ResNet50(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_tensor=inputs,\n",
    "            pooling=None\n",
    "        )\n",
    "\n",
    "        for layer in base_model.layers[:100]:\n",
    "            layer.trainable = False\n",
    "\n",
    "        x = base_model.output\n",
    "        x = layers.Conv2D(512, 3, padding='same')(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Conv2D(512, 3, padding='same')(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Conv2D(256, 3, padding='same')(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dense(1024)(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "\n",
    "        bbox_output = layers.Dense(self.max_detections * 4, name='bbox')(x)\n",
    "        objectness_output = layers.Dense(self.max_detections)(x)\n",
    "        objectness_output = layers.Activation('sigmoid', name='objectness')(objectness_output)\n",
    "\n",
    "        model = models.Model(inputs=inputs, outputs=[bbox_output, objectness_output])\n",
    "        return model\n",
    "\n",
    "    def load_weights(self, path, strategy=None):\n",
    "        \"\"\"Load pretrained weights\"\"\"\n",
    "        if os.path.exists(path):\n",
    "            print(f\"✓ Loading identifier weights from {path}\")\n",
    "            if strategy is not None:\n",
    "                with strategy.scope():\n",
    "                    self.model = keras.models.load_model(path, compile=False)\n",
    "                    self.model.compile(\n",
    "                        optimizer=optimizers.Adam(learning_rate=0.0005),\n",
    "                        loss={'bbox': 'mse', 'objectness': 'binary_crossentropy'},\n",
    "                        loss_weights={'bbox': 1.0, 'objectness': 0.5}\n",
    "                    )\n",
    "            else:\n",
    "                self.model = keras.models.load_model(path, compile=False)\n",
    "                self.model.compile(\n",
    "                    optimizer=optimizers.Adam(learning_rate=0.0005),\n",
    "                    loss={'bbox': 'mse', 'objectness': 'binary_crossentropy'},\n",
    "                    loss_weights={'bbox': 1.0, 'objectness': 0.5}\n",
    "                )\n",
    "            print(\"✓ Identifier weights loaded successfully\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"✗ Weights file not found: {path}\")\n",
    "            return False\n",
    "\n",
    "    def train(self, train_gen, epochs=15, strategy=None, resume=False, weights_path=None):\n",
    "        \"\"\"Train identifier with resume capability\"\"\"\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"IDENTIFIER TRAINING\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Try to resume from weights if requested\n",
    "        if resume and weights_path and os.path.exists(weights_path):\n",
    "            loaded = self.load_weights(weights_path, strategy)\n",
    "            if loaded:\n",
    "                print(f\"✓ Resuming training from epoch 0 (with pretrained weights)\")\n",
    "        else:\n",
    "            # Build new model\n",
    "            if strategy is not None:\n",
    "                with strategy.scope():\n",
    "                    self.model = self._build_model()\n",
    "                    self.model.compile(\n",
    "                        optimizer=optimizers.Adam(learning_rate=0.0005),\n",
    "                        loss={'bbox': 'mse', 'objectness': 'binary_crossentropy'},\n",
    "                        loss_weights={'bbox': 1.0, 'objectness': 0.5}\n",
    "                    )\n",
    "            else:\n",
    "                self.model = self._build_model()\n",
    "                self.model.compile(\n",
    "                    optimizer=optimizers.Adam(learning_rate=0.0005),\n",
    "                    loss={'bbox': 'mse', 'objectness': 'binary_crossentropy'},\n",
    "                    loss_weights={'bbox': 1.0, 'objectness': 0.5}\n",
    "                )\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "            epoch_losses = []\n",
    "\n",
    "            for batch_idx in range(len(train_gen)):\n",
    "                try:\n",
    "                    images, targets = train_gen[batch_idx]\n",
    "\n",
    "                    if len(images) == 0:\n",
    "                        continue\n",
    "\n",
    "                    images = np.array(images, dtype=np.float32)\n",
    "                    batch_size = len(images)\n",
    "\n",
    "                    target_bbox_batch = np.zeros((batch_size, self.max_detections * 4), dtype=np.float32)\n",
    "                    obj_target_batch = np.zeros((batch_size, self.max_detections), dtype=np.float32)\n",
    "\n",
    "                    for i, target in enumerate(targets):\n",
    "                        if len(target) > 0:\n",
    "                            num_objs = min(len(target), self.max_detections)\n",
    "                            target_flat = target[:num_objs].flatten()\n",
    "                            target_bbox_batch[i, :len(target_flat)] = target_flat\n",
    "                            obj_target_batch[i, :num_objs] = 1.0\n",
    "\n",
    "                    loss = self.model.train_on_batch(\n",
    "                        images,\n",
    "                        {'bbox': target_bbox_batch, 'objectness': obj_target_batch}\n",
    "                    )\n",
    "\n",
    "                    epoch_losses.append(loss[0] if isinstance(loss, list) else loss)\n",
    "\n",
    "                    if (batch_idx + 1) % 50 == 0:\n",
    "                        print(f\"  Batch {batch_idx+1}/{len(train_gen)}, Loss: {np.mean(epoch_losses[-50:]):.4f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error in batch {batch_idx}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            if len(epoch_losses) > 0:\n",
    "                print(f\"Epoch {epoch+1} Loss: {np.mean(epoch_losses):.4f}\")\n",
    "            else:\n",
    "                print(f\"Warning: No valid batches in epoch {epoch+1}\")\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def predict_boxes(self, image, confidence_threshold=0.5):\n",
    "        \"\"\"Predict bounding boxes\"\"\"\n",
    "        orig_h, orig_w = image.shape[:2]\n",
    "        image_resized = cv2.resize(image, (self.img_size, self.img_size))\n",
    "        image_resized = image_resized.astype(np.float32) / 255.0\n",
    "        image_resized = np.expand_dims(image_resized, axis=0)\n",
    "\n",
    "        bbox_pred, obj_pred = self.model.predict(image_resized, verbose=0)\n",
    "\n",
    "        bbox_pred = bbox_pred.reshape(self.max_detections, 4)\n",
    "        obj_pred = obj_pred.reshape(self.max_detections)\n",
    "\n",
    "        keep = obj_pred > confidence_threshold\n",
    "        boxes = bbox_pred[keep]\n",
    "        scores = obj_pred[keep]\n",
    "\n",
    "        final_boxes = []\n",
    "        for box in boxes:\n",
    "            ymin, xmin, ymax, xmax = box\n",
    "            xmin = int(np.clip(xmin * orig_w, 0, orig_w))\n",
    "            ymin = int(np.clip(ymin * orig_h, 0, orig_h))\n",
    "            xmax = int(np.clip(xmax * orig_w, 0, orig_w))\n",
    "            ymax = int(np.clip(ymax * orig_h, 0, orig_h))\n",
    "\n",
    "            if xmax > xmin and ymax > ymin:\n",
    "                final_boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        return np.array(final_boxes), scores[:len(final_boxes)]\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# CLASSIFIER MODULE\n",
    "# ==============================================================================\n",
    "\n",
    "class ChromosomeClassifierDataGenerator(keras.utils.Sequence):\n",
    "    \"\"\"Data generator for classifier\"\"\"\n",
    "\n",
    "    CHROMOSOME_CLASSES = {\n",
    "        '1': 0, '2': 1, '3': 2, '4': 3, '5': 4, '6': 5,\n",
    "        '7': 6, '8': 7, '9': 8, '10': 9, '11': 10, '12': 11,\n",
    "        '13': 12, '14': 13, '15': 14, '16': 15, '17': 16, '18': 17,\n",
    "        '19': 18, '20': 19, '21': 20, '22': 21, 'X': 22, 'Y': 23\n",
    "    }\n",
    "\n",
    "    def __init__(self, images_dir: str, annotations_dir: str, is_validation=False,\n",
    "                 batch_size=32, img_size=224, augment=False, validation_split=0.2,\n",
    "                 data=None, indexes=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.augment = augment\n",
    "        self.validation_split = validation_split\n",
    "        self.is_validation = is_validation\n",
    "        self.parser = AnnotationParser()\n",
    "\n",
    "        if data is None or indexes is None:\n",
    "            self.chromosome_data = []\n",
    "            self._build_dataset()\n",
    "\n",
    "            if len(self.chromosome_data) == 0:\n",
    "                raise ValueError(\"No chromosome data found!\")\n",
    "\n",
    "            if not self.is_validation:\n",
    "                total_samples = len(self.chromosome_data)\n",
    "                val_size = int(total_samples * validation_split)\n",
    "\n",
    "                np.random.seed(42)\n",
    "                all_indexes = np.arange(total_samples)\n",
    "                np.random.shuffle(all_indexes)\n",
    "\n",
    "                self.indexes = all_indexes[val_size:]\n",
    "                self.val_indexes = all_indexes[:val_size]\n",
    "\n",
    "                print(f\"Train: {len(self.indexes)}, Val: {len(self.val_indexes)}\")\n",
    "            else:\n",
    "                self.chromosome_data = data\n",
    "                self.indexes = indexes\n",
    "                self.val_indexes = []\n",
    "        else:\n",
    "            self.chromosome_data = data\n",
    "            self.indexes = indexes\n",
    "            self.val_indexes = []\n",
    "\n",
    "        self._print_class_distribution()\n",
    "\n",
    "        if augment and not self.is_validation:\n",
    "            self.aug = A.Compose([\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.5),\n",
    "                A.Rotate(limit=180, p=0.7),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "                A.GaussNoise(var_limit=(10, 50), p=0.3),\n",
    "                A.GaussianBlur(blur_limit=(3, 5), p=0.2),\n",
    "                A.CLAHE(clip_limit=2.0, p=0.3),\n",
    "            ])\n",
    "        else:\n",
    "            self.aug = None\n",
    "\n",
    "    def _build_dataset(self):\n",
    "        \"\"\"Build dataset\"\"\"\n",
    "        print(\"\\nBuilding classifier dataset...\")\n",
    "\n",
    "        image_files = []\n",
    "        for filename in os.listdir(self.images_dir):\n",
    "            if filename.endswith(('.jpg', '.png')):\n",
    "                base_name = os.path.splitext(filename)[0]\n",
    "                xml_path = os.path.join(self.annotations_dir, f\"{base_name}.xml\")\n",
    "                if os.path.exists(xml_path):\n",
    "                    image_files.append(base_name)\n",
    "\n",
    "        print(f\"Processing {len(image_files)} images...\")\n",
    "\n",
    "        for idx, img_name in enumerate(image_files):\n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(f\"  Processed {idx+1}/{len(image_files)}...\")\n",
    "\n",
    "            img_path = os.path.join(self.images_dir, f\"{img_name}.jpg\")\n",
    "            xml_path = os.path.join(self.annotations_dir, f\"{img_name}.xml\")\n",
    "\n",
    "            if not os.path.exists(img_path):\n",
    "                img_path = os.path.join(self.images_dir, f\"{img_name}.png\")\n",
    "\n",
    "            if not os.path.exists(img_path) or not os.path.exists(xml_path):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                image = cv2.imread(img_path)\n",
    "                if image is None:\n",
    "                    continue\n",
    "\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                annotations = self.parser.parse_xml(xml_path)\n",
    "\n",
    "                for obj in annotations['objects']:\n",
    "                    bbox = obj['bbox']\n",
    "                    label = obj['name']\n",
    "\n",
    "                    if label in self.CHROMOSOME_CLASSES:\n",
    "                        chromosome_img = image[bbox['ymin']:bbox['ymax'],\n",
    "                                            bbox['xmin']:bbox['xmax']]\n",
    "\n",
    "                        if chromosome_img.size > 0:\n",
    "                            self.chromosome_data.append({\n",
    "                                'image': chromosome_img,\n",
    "                                'label': self.CHROMOSOME_CLASSES[label],\n",
    "                                'label_name': label\n",
    "                            })\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error processing {img_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"Dataset: {len(self.chromosome_data)} samples\")\n",
    "\n",
    "    def _print_class_distribution(self):\n",
    "        \"\"\"Print class distribution\"\"\"\n",
    "        if len(self.chromosome_data) == 0:\n",
    "            return\n",
    "\n",
    "        class_counts = {}\n",
    "        for data in self.chromosome_data:\n",
    "            label = data['label']\n",
    "            class_counts[label] = class_counts.get(label, 0) + 1\n",
    "\n",
    "        print(\"\\nClass Distribution:\")\n",
    "        for class_id in sorted(class_counts.keys()):\n",
    "            class_name = [k for k, v in self.CHROMOSOME_CLASSES.items() if v == class_id][0]\n",
    "            print(f\"  Class {class_name:>2}: {class_counts[class_id]:5d} samples\")\n",
    "\n",
    "    def get_validation_generator(self):\n",
    "        \"\"\"Create validation generator\"\"\"\n",
    "        val_gen = ChromosomeClassifierDataGenerator(\n",
    "            images_dir=self.images_dir,\n",
    "            annotations_dir=self.annotations_dir,\n",
    "            batch_size=self.batch_size,\n",
    "            img_size=self.img_size,\n",
    "            augment=False,\n",
    "            validation_split=0.0,\n",
    "            is_validation=True,\n",
    "            data=[self.chromosome_data[i] for i in self.val_indexes],\n",
    "            indexes=np.arange(len(self.val_indexes))\n",
    "        )\n",
    "        return val_gen\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.indexes) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "\n",
    "        for idx in batch_indexes:\n",
    "            try:\n",
    "                data = self.chromosome_data[idx]\n",
    "                image = data['image'].copy()\n",
    "                label = data['label']\n",
    "\n",
    "                if self.aug is not None:\n",
    "                    augmented = self.aug(image=image)\n",
    "                    image = augmented['image']\n",
    "\n",
    "                image = cv2.resize(image, (self.img_size, self.img_size))\n",
    "                image = image.astype(np.float32) / 255.0\n",
    "\n",
    "                mean = np.array([0.485, 0.456, 0.406])\n",
    "                std = np.array([0.229, 0.224, 0.225])\n",
    "                image = (image - mean) / std\n",
    "\n",
    "                batch_images.append(image)\n",
    "                batch_labels.append(label)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        # Handle empty batches by trying next batch\n",
    "        if len(batch_images) == 0 and index < len(self) - 1:\n",
    "            return self.__getitem__((index + 1) % len(self))\n",
    "        \n",
    "        # Last resort: return minimal valid batch\n",
    "        if len(batch_images) == 0:\n",
    "            batch_images = [np.zeros((self.img_size, self.img_size, 3), dtype=np.float32)]\n",
    "            batch_labels = [0]\n",
    "\n",
    "        return np.array(batch_images, dtype=np.float32), np.array(batch_labels, dtype=np.int32)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if not self.is_validation:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "\n",
    "class ChromosomeClassifier:\n",
    "    \"\"\"Chromosome classifier\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=24, img_size=224):\n",
    "        self.num_classes = num_classes\n",
    "        self.img_size = img_size\n",
    "        self.model = None\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"Build model\"\"\"\n",
    "        base_model = ResNet50(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(self.img_size, self.img_size, 3)\n",
    "        )\n",
    "\n",
    "        for layer in base_model.layers[:120]:\n",
    "            layer.trainable = False\n",
    "        for layer in base_model.layers[120:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "        x = base_model.output\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "        x = layers.Dense(1024)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "\n",
    "        x = layers.Dense(512)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        x = layers.Dropout(0.4)(x)\n",
    "\n",
    "        x = layers.Dense(256)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "\n",
    "        outputs = layers.Dense(self.num_classes)(x)\n",
    "        outputs = layers.Activation('softmax')(outputs)\n",
    "\n",
    "        model = models.Model(inputs=base_model.input, outputs=outputs)\n",
    "        return model\n",
    "\n",
    "    def load_weights(self, path, strategy=None):\n",
    "        \"\"\"Load pretrained weights\"\"\"\n",
    "        if os.path.exists(path):\n",
    "            print(f\"✓ Loading classifier weights from {path}\")\n",
    "            if strategy is not None:\n",
    "                with strategy.scope():\n",
    "                    self.model = keras.models.load_model(path, compile=False)\n",
    "                    self.model.compile(\n",
    "                        optimizer=optimizers.Adam(0.001),\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy']\n",
    "                    )\n",
    "            else:\n",
    "                self.model = keras.models.load_model(path, compile=False)\n",
    "                self.model.compile(\n",
    "                    optimizer=optimizers.Adam(0.001),\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy']\n",
    "                )\n",
    "            print(\"✓ Classifier weights loaded successfully\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"✗ Weights file not found: {path}\")\n",
    "            return False\n",
    "\n",
    "    def compile_model(self, lr=0.001, strategy=None, resume=False, weights_path=None):\n",
    "        \"\"\"Compile model with resume capability\"\"\"\n",
    "        \n",
    "        if resume and weights_path and os.path.exists(weights_path):\n",
    "            loaded = self.load_weights(weights_path, strategy)\n",
    "            if loaded:\n",
    "                print(\"✓ Resuming from checkpoint\")\n",
    "                return\n",
    "        \n",
    "        if strategy is not None:\n",
    "            with strategy.scope():\n",
    "                self.model = self._build_model()\n",
    "                self.model.compile(\n",
    "                    optimizer=optimizers.Adam(lr),\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy']\n",
    "                )\n",
    "        else:\n",
    "            self.model = self._build_model()\n",
    "            self.model.compile(\n",
    "                optimizer=optimizers.Adam(lr),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "    def train(self, train_gen, val_gen, epochs=50):\n",
    "        \"\"\"Train classifier\"\"\"\n",
    "        callbacks = [\n",
    "            keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_accuracy',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-7,\n",
    "                verbose=1\n",
    "            ),\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                'best_classifier.h5',\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_accuracy',\n",
    "                patience=10,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        history = self.model.fit(\n",
    "            train_gen,\n",
    "            validation_data=val_gen,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        return history\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# PIPELINE AND EVALUATION\n",
    "# ==============================================================================\n",
    "\n",
    "class KaryotypePipeline:\n",
    "    \"\"\"End-to-end pipeline\"\"\"\n",
    "\n",
    "    def __init__(self, identifier: ChromosomeIdentifier,\n",
    "                 classifier: ChromosomeClassifier):\n",
    "        self.identifier = identifier\n",
    "        self.classifier = classifier\n",
    "        self.img_size = 224\n",
    "\n",
    "    def process_image(self, image_path: str, confidence_threshold=0.5):\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"Warning: Could not read image {image_path}\")\n",
    "            return []\n",
    "            \n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        boxes, scores = self.identifier.predict_boxes(image_rgb, confidence_threshold=confidence_threshold)\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            return []\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        for i, box in enumerate(boxes):\n",
    "            xmin, ymin, xmax, ymax = map(int, box)\n",
    "\n",
    "            chromosome_img = image_rgb[ymin:ymax, xmin:xmax]\n",
    "\n",
    "            if chromosome_img.size == 0:\n",
    "                continue\n",
    "\n",
    "            chromosome_img = cv2.resize(chromosome_img, (self.img_size, self.img_size))\n",
    "            chromosome_img = chromosome_img.astype(np.float32) / 255.0\n",
    "\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            chromosome_img = (chromosome_img - mean) / std\n",
    "\n",
    "            chromosome_img = np.expand_dims(chromosome_img, axis=0)\n",
    "\n",
    "            probs = self.classifier.model.predict(chromosome_img, verbose=0)[0]\n",
    "            class_id = np.argmax(probs)\n",
    "            confidence = probs[class_id]\n",
    "\n",
    "            predictions.append({\n",
    "                'bbox': box,\n",
    "                'class_id': int(class_id),\n",
    "                'class_name': self._get_class_name(class_id),\n",
    "                'confidence': float(confidence),\n",
    "                'score': float(scores[i]) if i < len(scores) else 1.0\n",
    "            })\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_class_name(class_id: int) -> str:\n",
    "        if class_id < 22:\n",
    "            return str(class_id + 1)\n",
    "        elif class_id == 22:\n",
    "            return 'X'\n",
    "        else:\n",
    "            return 'Y'\n",
    "\n",
    "\n",
    "def evaluate_pipeline_simple(pipeline, images_dir, annotations_dir, test_list, output_dir='results'):\n",
    "    \"\"\"Simple evaluation for auPRC\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    parser = AnnotationParser()\n",
    "\n",
    "    all_true_labels = []\n",
    "    all_pred_probs = []\n",
    "\n",
    "    print(\"Evaluating pipeline...\")\n",
    "    for idx, img_name in enumerate(test_list):\n",
    "        img_name = img_name.strip()\n",
    "        img_path = os.path.join(images_dir, f\"{img_name}.jpg\")\n",
    "        xml_path = os.path.join(annotations_dir, f\"{img_name}.xml\")\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            img_path = os.path.join(images_dir, f\"{img_name}.png\")\n",
    "\n",
    "        if not os.path.exists(img_path) or not os.path.exists(xml_path):\n",
    "            continue\n",
    "\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"  Processed {idx+1}/{len(test_list)}...\")\n",
    "\n",
    "        try:\n",
    "            annotations = parser.parse_xml(xml_path)\n",
    "            gt_chromosomes = [obj['name'] for obj in annotations['objects']]\n",
    "\n",
    "            predictions = pipeline.process_image(img_path)\n",
    "\n",
    "            for gt_chr in gt_chromosomes:\n",
    "                if gt_chr in ChromosomeClassifierDataGenerator.CHROMOSOME_CLASSES:\n",
    "                    gt_class = ChromosomeClassifierDataGenerator.CHROMOSOME_CLASSES[gt_chr]\n",
    "\n",
    "                    true_labels = np.zeros(24)\n",
    "                    true_labels[gt_class] = 1\n",
    "\n",
    "                    pred_probs = np.zeros(24)\n",
    "                    for pred in predictions:\n",
    "                        pred_probs[pred['class_id']] = max(\n",
    "                            pred_probs[pred['class_id']],\n",
    "                            pred['confidence']\n",
    "                        )\n",
    "\n",
    "                    all_true_labels.append(true_labels)\n",
    "                    all_pred_probs.append(pred_probs)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error processing {img_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if len(all_true_labels) == 0:\n",
    "        print(\"Warning: No valid samples found for evaluation!\")\n",
    "        return 0.0, []\n",
    "\n",
    "    all_true_labels = np.array(all_true_labels)\n",
    "    all_pred_probs = np.array(all_pred_probs)\n",
    "\n",
    "    auprc_scores = []\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    CLASS_NAMES = [str(i+1) for i in range(22)] + ['X', 'Y']\n",
    "\n",
    "    for i in range(24):\n",
    "        if np.sum(all_true_labels[:, i]) > 0:\n",
    "            precision, recall, _ = precision_recall_curve(\n",
    "                all_true_labels[:, i],\n",
    "                all_pred_probs[:, i]\n",
    "            )\n",
    "            auprc = auc(recall, precision)\n",
    "            auprc_scores.append(auprc)\n",
    "\n",
    "            plt.plot(recall, precision,\n",
    "                    label=f'Class {CLASS_NAMES[i]} (auPRC={auprc:.3f})')\n",
    "\n",
    "    plt.xlabel('Recall', fontsize=12)\n",
    "    plt.ylabel('Precision', fontsize=12)\n",
    "    plt.title('Precision-Recall Curves', fontsize=14)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'auprc_curves.png'),\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    mean_auprc = np.mean(auprc_scores) if len(auprc_scores) > 0 else 0.0\n",
    "    print(f\"\\nMean auPRC: {mean_auprc:.4f}\")\n",
    "\n",
    "    with open(os.path.join(output_dir, 'auprc_scores.txt'), 'w') as f:\n",
    "        f.write(f\"Mean auPRC: {mean_auprc:.4f}\\n\\n\")\n",
    "        f.write(\"Per-class auPRC scores:\\n\")\n",
    "        class_idx = 0\n",
    "        for i in range(24):\n",
    "            if np.sum(all_true_labels[:, i]) > 0:\n",
    "                f.write(f\"Class {CLASS_NAMES[i]}: {auprc_scores[class_idx]:.4f}\\n\")\n",
    "                class_idx += 1\n",
    "\n",
    "    return mean_auprc, auprc_scores\n",
    "\n",
    "\n",
    "def evaluate_comprehensive(pipeline, images_dir, annotations_dir, test_list, output_dir='evaluation_results'):\n",
    "    \"\"\"Comprehensive evaluation with confusion matrix\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    parser = AnnotationParser()\n",
    "\n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "    all_pred_probs = []\n",
    "\n",
    "    CLASS_NAMES = [str(i+1) for i in range(22)] + ['X', 'Y']\n",
    "\n",
    "    print(\"Computing comprehensive metrics...\")\n",
    "    for idx, img_name in enumerate(test_list):\n",
    "        img_name = img_name.strip()\n",
    "        img_path = os.path.join(images_dir, f\"{img_name}.jpg\")\n",
    "        xml_path = os.path.join(annotations_dir, f\"{img_name}.xml\")\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            img_path = os.path.join(images_dir, f\"{img_name}.png\")\n",
    "\n",
    "        if not os.path.exists(img_path) or not os.path.exists(xml_path):\n",
    "            continue\n",
    "\n",
    "        if (idx + 1) % 20 == 0:\n",
    "            print(f\"  Processed {idx+1}/{len(test_list)}...\")\n",
    "\n",
    "        try:\n",
    "            annotations = parser.parse_xml(xml_path)\n",
    "            predictions = pipeline.process_image(img_path, confidence_threshold=0.5)\n",
    "\n",
    "            for obj in annotations['objects']:\n",
    "                gt_chr = obj['name']\n",
    "                if gt_chr not in ChromosomeClassifierDataGenerator.CHROMOSOME_CLASSES:\n",
    "                    continue\n",
    "\n",
    "                gt_class = ChromosomeClassifierDataGenerator.CHROMOSOME_CLASSES[gt_chr]\n",
    "\n",
    "                if predictions:\n",
    "                    best_pred = max(predictions, key=lambda x: x['confidence'])\n",
    "                    pred_class = best_pred['class_id']\n",
    "\n",
    "                    pred_probs = np.zeros(24)\n",
    "                    pred_probs[pred_class] = best_pred['confidence']\n",
    "                else:\n",
    "                    pred_class = 0\n",
    "                    pred_probs = np.zeros(24)\n",
    "                    pred_probs[0] = 1.0\n",
    "\n",
    "                all_true_labels.append(gt_class)\n",
    "                all_pred_labels.append(pred_class)\n",
    "                all_pred_probs.append(pred_probs)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error processing {img_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if len(all_true_labels) == 0:\n",
    "        print(\"Warning: No valid samples found for comprehensive evaluation!\")\n",
    "        return {'Overall Accuracy': 0.0, 'Classes Present': 0}\n",
    "\n",
    "    all_true_labels = np.array(all_true_labels)\n",
    "    all_pred_labels = np.array(all_pred_labels)\n",
    "    all_pred_probs = np.array(all_pred_probs)\n",
    "\n",
    "    print(f\"\\nTotal samples: {len(all_true_labels)}\")\n",
    "\n",
    "    # Get unique classes\n",
    "    unique_classes = np.unique(np.concatenate([all_true_labels, all_pred_labels]))\n",
    "    print(f\"Unique classes: {len(unique_classes)}\")\n",
    "\n",
    "    present_class_names = [CLASS_NAMES[i] for i in unique_classes]\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_true_labels, all_pred_labels, labels=unique_classes)\n",
    "\n",
    "    plt.figure(figsize=(max(12, len(unique_classes)), max(10, len(unique_classes)-2)))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=present_class_names, yticklabels=present_class_names,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.xlabel('Predicted Class', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Class', fontsize=14, fontweight='bold')\n",
    "    plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'),\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Classification Report\n",
    "    report = classification_report(all_true_labels, all_pred_labels,\n",
    "                                   labels=unique_classes,\n",
    "                                   target_names=present_class_names,\n",
    "                                   output_dict=True, zero_division=0)\n",
    "\n",
    "    with open(os.path.join(output_dir, 'classification_report.txt'), 'w') as f:\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"CLASSIFICATION REPORT\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        f.write(classification_report(all_true_labels, all_pred_labels,\n",
    "                                     labels=unique_classes,\n",
    "                                     target_names=present_class_names,\n",
    "                                     zero_division=0))\n",
    "\n",
    "    # Metrics\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        all_true_labels, all_pred_labels, labels=unique_classes, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    overall_acc = accuracy_score(all_true_labels, all_pred_labels)\n",
    "\n",
    "    summary = {\n",
    "        'Overall Accuracy': overall_acc,\n",
    "        'Classes Present': len(unique_classes)\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(output_dir, 'summary.txt'), 'w') as f:\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"SUMMARY\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        f.write(f\"Overall Accuracy: {overall_acc:.4f}\\n\")\n",
    "        f.write(f\"Classes Present: {len(unique_classes)}\\n\")\n",
    "\n",
    "    print(f\"✓ Results saved to '{output_dir}/'\")\n",
    "    return summary\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CHROMOSOME CLASSIFICATION PIPELINE - TPU v5e-8\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Train Identifier: {TRAIN_IDENTIFIER}\")\n",
    "    print(f\"Train Classifier: {TRAIN_CLASSIFIER}\")\n",
    "    print(f\"Run Evaluation: {RUN_EVALUATION}\")\n",
    "    print(f\"Resume Identifier: {RESUME_IDENTIFIER}\")\n",
    "    print(f\"Resume Classifier: {RESUME_CLASSIFIER}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    # Setup paths\n",
    "    SINGLE_CHR_IMAGES = os.path.join(DATA_ROOT, 'single_chromosomes_object', 'images')\n",
    "    SINGLE_CHR_ANNOTATIONS = os.path.join(DATA_ROOT, 'single_chromosomes_object', 'annotations')\n",
    "    MULTI_CHR_IMAGES = os.path.join(DATA_ROOT, '24_chromosomes_object', 'images')\n",
    "    MULTI_CHR_ANNOTATIONS = os.path.join(DATA_ROOT, '24_chromosomes_object', 'annotations')\n",
    "\n",
    "    # TPU v5e-8 Optimized Batch Sizes\n",
    "    if DEVICE_TYPE == \"TPU\":\n",
    "        # TPU v5e-8: 8 cores\n",
    "        # Identifier: 4 per core = 32 total (conservative for 800x800 images)\n",
    "        # Classifier: 128 per core = 1024 total (optimal for 224x224 images)\n",
    "        IDENTIFIER_BATCH = 4 * STRATEGY.num_replicas_in_sync\n",
    "        CLASSIFIER_BATCH = 128 * STRATEGY.num_replicas_in_sync\n",
    "        print(f\"\\n✓ TPU v5e-8 Batch Sizes:\")\n",
    "        print(f\"  Identifier: {IDENTIFIER_BATCH} (4 per core)\")\n",
    "        print(f\"  Classifier: {CLASSIFIER_BATCH} (128 per core)\")\n",
    "    elif DEVICE_TYPE in [\"Single-GPU\", \"Multi-GPU\"]:\n",
    "        IDENTIFIER_BATCH = 2 * STRATEGY.num_replicas_in_sync\n",
    "        CLASSIFIER_BATCH = 32 * STRATEGY.num_replicas_in_sync\n",
    "        print(f\"\\n✓ GPU Batch Sizes:\")\n",
    "        print(f\"  Identifier: {IDENTIFIER_BATCH}\")\n",
    "        print(f\"  Classifier: {CLASSIFIER_BATCH}\")\n",
    "    else:\n",
    "        IDENTIFIER_BATCH = 1\n",
    "        CLASSIFIER_BATCH = 8\n",
    "        print(f\"\\n✓ CPU Batch Sizes:\")\n",
    "        print(f\"  Identifier: {IDENTIFIER_BATCH}\")\n",
    "        print(f\"  Classifier: {CLASSIFIER_BATCH}\")\n",
    "\n",
    "    # ==============================================================================\n",
    "    # STEP 1: TRAIN OR LOAD IDENTIFIER\n",
    "    # ==============================================================================\n",
    "\n",
    "    identifier = ChromosomeIdentifier(img_size=800, max_detections=50)\n",
    "\n",
    "    if TRAIN_IDENTIFIER:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 1: IDENTIFIER TRAINING\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        try:\n",
    "            train_identifier_gen = ChromosomeIdentifierDataGenerator(\n",
    "                SINGLE_CHR_IMAGES, SINGLE_CHR_ANNOTATIONS,\n",
    "                batch_size=IDENTIFIER_BATCH, img_size=800, shuffle=True\n",
    "            )\n",
    "\n",
    "            identifier.train(\n",
    "                train_identifier_gen,\n",
    "                epochs=IDENTIFIER_EPOCHS,\n",
    "                strategy=STRATEGY,\n",
    "                resume=RESUME_IDENTIFIER,\n",
    "                weights_path=IDENTIFIER_MODEL_PATH\n",
    "            )\n",
    "\n",
    "            identifier.model.save(IDENTIFIER_MODEL_PATH)\n",
    "            print(f\"\\n✓ Identifier saved to '{IDENTIFIER_MODEL_PATH}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error during identifier training: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 1: LOADING IDENTIFIER\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if os.path.exists(IDENTIFIER_MODEL_PATH):\n",
    "            identifier.load_weights(IDENTIFIER_MODEL_PATH, STRATEGY)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Model file not found: {IDENTIFIER_MODEL_PATH}. \"\n",
    "                                   \"Please set TRAIN_IDENTIFIER=True or provide valid model path\")\n",
    "\n",
    "    # ==============================================================================\n",
    "    # STEP 2: TRAIN OR LOAD CLASSIFIER\n",
    "    # ==============================================================================\n",
    "\n",
    "    classifier = ChromosomeClassifier(num_classes=24)\n",
    "\n",
    "    if TRAIN_CLASSIFIER:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 2: CLASSIFIER TRAINING\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        try:\n",
    "            train_classifier_gen = ChromosomeClassifierDataGenerator(\n",
    "                MULTI_CHR_IMAGES, MULTI_CHR_ANNOTATIONS,\n",
    "                batch_size=CLASSIFIER_BATCH, img_size=224, augment=True, validation_split=0.2\n",
    "            )\n",
    "\n",
    "            val_classifier_gen = train_classifier_gen.get_validation_generator()\n",
    "\n",
    "            classifier.compile_model(\n",
    "                lr=0.001,\n",
    "                strategy=STRATEGY,\n",
    "                resume=RESUME_CLASSIFIER,\n",
    "                weights_path=CLASSIFIER_MODEL_PATH\n",
    "            )\n",
    "\n",
    "            print(\"\\nStarting classifier training...\")\n",
    "            history = classifier.train(train_classifier_gen, val_classifier_gen, epochs=CLASSIFIER_EPOCHS)\n",
    "\n",
    "            print(f\"\\n✓ Classifier saved to '{CLASSIFIER_MODEL_PATH}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error during classifier training: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 2: LOADING CLASSIFIER\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if os.path.exists(CLASSIFIER_MODEL_PATH):\n",
    "            classifier.load_weights(CLASSIFIER_MODEL_PATH, STRATEGY)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Model file not found: {CLASSIFIER_MODEL_PATH}. \"\n",
    "                                   \"Please set TRAIN_CLASSIFIER=True or provide valid model path\")\n",
    "\n",
    "    # ==============================================================================\n",
    "    # STEP 3: EVALUATION\n",
    "    # ==============================================================================\n",
    "\n",
    "    if RUN_EVALUATION:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 3: EVALUATION\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        try:\n",
    "            # Load test list\n",
    "            test_file_path = os.path.join(DATA_ROOT, 'test.txt')\n",
    "            if not os.path.exists(test_file_path):\n",
    "                print(f\"Warning: test.txt not found at {test_file_path}\")\n",
    "                print(\"Skipping evaluation...\")\n",
    "            else:\n",
    "                with open(test_file_path, 'r') as f:\n",
    "                    test_list = f.readlines()\n",
    "\n",
    "                print(f\"Test samples: {len(test_list)}\")\n",
    "\n",
    "                # Create pipeline\n",
    "                pipeline = KaryotypePipeline(identifier, classifier)\n",
    "\n",
    "                # Simple evaluation (auPRC)\n",
    "                print(\"\\nRunning auPRC evaluation...\")\n",
    "                mean_auprc, class_auprcs = evaluate_pipeline_simple(\n",
    "                    pipeline, MULTI_CHR_IMAGES, MULTI_CHR_ANNOTATIONS,\n",
    "                    test_list, output_dir='results'\n",
    "                )\n",
    "\n",
    "                # Comprehensive evaluation\n",
    "                print(\"\\nRunning comprehensive evaluation...\")\n",
    "                summary = evaluate_comprehensive(\n",
    "                    pipeline, MULTI_CHR_IMAGES, MULTI_CHR_ANNOTATIONS,\n",
    "                    test_list, output_dir='evaluation_results'\n",
    "                )\n",
    "\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"EVALUATION COMPLETE!\")\n",
    "                print(\"=\"*60)\n",
    "                print(f\"Mean auPRC: {mean_auprc:.4f}\")\n",
    "                print(f\"Overall Accuracy: {summary['Overall Accuracy']:.4f}\")\n",
    "                print(f\"Classes Detected: {summary['Classes Present']}\")\n",
    "                print(\"\\nResults saved to:\")\n",
    "                print(\"  - results/auprc_curves.png\")\n",
    "                print(\"  - results/auprc_scores.txt\")\n",
    "                print(\"  - evaluation_results/confusion_matrix.png\")\n",
    "                print(\"  - evaluation_results/classification_report.txt\")\n",
    "                print(\"=\"*60)\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error during evaluation: {e}\")\n",
    "            raise\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PIPELINE EXECUTION COMPLETE!\")\n",
    "    print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8486113,
     "sourceId": 13375782,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
